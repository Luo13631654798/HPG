nohup: ignoring input
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 20:38:38
run_iTransformer.py --history 24 --model iTransformer --dataset physionet --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=128, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2141348, pred_window=24, ndim=41, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
True
Couldn't import umap
PID, device: 2141348 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=128, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7168001
n_train_batches: 225
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py", line 406, in <module>
    train_res["loss"].backward()
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 20:47:06
run_iTransformer.py --history 36 --model iTransformer --dataset physionet --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=175, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=2146657, pred_window=12, ndim=41, patch_layer=2, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 11644
Train - Loss (one batch): 0.06178
Val - Loss, MSE, RMSE, MAE, MAPE: 0.05632, 0.05632, 0.23733, 0.17677, 1194.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 39.44s
- Epoch 001, ExpID 11644
Train - Loss (one batch): 0.06608
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06615, 0.06615, 0.25720, 0.19722, 1968.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.89s
- Epoch 002, ExpID 11644
Train - Loss (one batch): 0.07109
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06439, 0.06439, 0.25376, 0.19452, 1836.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.04s
- Epoch 003, ExpID 11644
Train - Loss (one batch): 0.06078
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06404, 0.06404, 0.25307, 0.19750, 1897.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.50s
- Epoch 004, ExpID 11644
Train - Loss (one batch): 0.06460
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06567, 0.06567, 0.25627, 0.20304, 2087.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.86s
- Epoch 005, ExpID 11644
Train - Loss (one batch): 0.06280
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06474, 0.06474, 0.25444, 0.19383, 1694.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.79s
- Epoch 006, ExpID 11644
Train - Loss (one batch): 0.06958
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06514, 0.06514, 0.25523, 0.20051, 2028.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.85s
- Epoch 007, ExpID 11644
Train - Loss (one batch): 0.06723
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06495, 0.06495, 0.25486, 0.19568, 1832.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.87s
- Epoch 008, ExpID 11644
Train - Loss (one batch): 0.06212
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06510, 0.06510, 0.25515, 0.19420, 1798.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.85s
- Epoch 009, ExpID 11644
Train - Loss (one batch): 0.06473
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06521, 0.06521, 0.25536, 0.19827, 1966.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.55s
- Epoch 010, ExpID 11644
Train - Loss (one batch): 0.07069
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06622, 0.06622, 0.25733, 0.20315, 2083.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.95s
- Epoch 011, ExpID 11644
Train - Loss (one batch): 0.06676
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06543, 0.06543, 0.25579, 0.19894, 2006.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.93s
- Epoch 012, ExpID 11644
Train - Loss (one batch): 0.07220
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06533, 0.06533, 0.25561, 0.19828, 1955.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.66s
- Epoch 013, ExpID 11644
Train - Loss (one batch): 0.06813
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06528, 0.06528, 0.25549, 0.19533, 1843.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.90s
- Epoch 014, ExpID 11644
Train - Loss (one batch): 0.06235
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06525, 0.06525, 0.25545, 0.19651, 1844.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.04s
- Epoch 015, ExpID 11644
Train - Loss (one batch): 0.06898
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06747, 0.06747, 0.25974, 0.20453, 2321.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.98s
- Epoch 016, ExpID 11644
Train - Loss (one batch): 0.06699
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06750, 0.06750, 0.25980, 0.20468, 2326.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.00s
- Epoch 017, ExpID 11644
Train - Loss (one batch): 0.06884
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06718, 0.06718, 0.25918, 0.20286, 2255.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.72s
- Epoch 018, ExpID 11644
Train - Loss (one batch): 0.07004
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25889, 0.20157, 2200.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.29s
- Epoch 019, ExpID 11644
Train - Loss (one batch): 0.07290
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06714, 0.06714, 0.25911, 0.20259, 2244.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.57s
- Epoch 020, ExpID 11644
Train - Loss (one batch): 0.07155
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06709, 0.06709, 0.25901, 0.20216, 2226.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.63s
- Epoch 021, ExpID 11644
Train - Loss (one batch): 0.07877
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06700, 0.06700, 0.25884, 0.19892, 2064.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.31s
- Epoch 022, ExpID 11644
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06711, 0.06711, 0.25905, 0.19819, 2017.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.03s
- Epoch 023, ExpID 11644
Train - Loss (one batch): 0.07419
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06718, 0.06718, 0.25919, 0.20285, 2255.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.17s
- Epoch 024, ExpID 11644
Train - Loss (one batch): 0.06505
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.19979, 2114.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.84s
- Epoch 025, ExpID 11644
Train - Loss (one batch): 0.06837
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26051, 0.20629, 2384.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.63s
- Epoch 026, ExpID 11644
Train - Loss (one batch): 0.06748
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06728, 0.06728, 0.25939, 0.20353, 2282.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.22s
- Epoch 027, ExpID 11644
Train - Loss (one batch): 0.06469
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25891, 0.20172, 2207.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.58s
- Epoch 028, ExpID 11644
Train - Loss (one batch): 0.07215
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06721, 0.06721, 0.25926, 0.20312, 2266.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.31s
- Epoch 029, ExpID 11644
Train - Loss (one batch): 0.06355
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.19933, 2087.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.58s
- Epoch 030, ExpID 11644
Train - Loss (one batch): 0.06764
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.19879, 2056.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.78s
- Epoch 031, ExpID 11644
Train - Loss (one batch): 0.06574
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25877, 0.20074, 2162.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.94s
- Epoch 032, ExpID 11644
Train - Loss (one batch): 0.06870
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25877, 0.19946, 2095.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.13s
- Epoch 033, ExpID 11644
Train - Loss (one batch): 0.07272
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.19879, 2056.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.33s
- Epoch 034, ExpID 11644
Train - Loss (one batch): 0.06998
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25889, 0.20157, 2200.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.03s
- Epoch 035, ExpID 11644
Train - Loss (one batch): 0.06546
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19961, 2103.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.10s
- Epoch 036, ExpID 11644
Train - Loss (one batch): 0.06977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20036, 2143.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.09s
- Epoch 037, ExpID 11644
Train - Loss (one batch): 0.07140
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06758, 0.06758, 0.25997, 0.20507, 2340.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.50s
- Epoch 038, ExpID 11644
Train - Loss (one batch): 0.06590
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06703, 0.06703, 0.25890, 0.20163, 2202.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.57s
- Epoch 039, ExpID 11644
Train - Loss (one batch): 0.07359
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.19911, 2075.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.76s
- Epoch 040, ExpID 11644
Train - Loss (one batch): 0.06896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20108, 2178.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.45s
- Epoch 041, ExpID 11644
Train - Loss (one batch): 0.06883
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20065, 2157.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.09s
- Epoch 042, ExpID 11644
Train - Loss (one batch): 0.06573
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19986, 2118.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.39s
- Epoch 043, ExpID 11644
Train - Loss (one batch): 0.06978
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06711, 0.06711, 0.25905, 0.20238, 2235.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.83s
- Epoch 044, ExpID 11644
Train - Loss (one batch): 0.07117
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20017, 2133.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.01s
- Epoch 045, ExpID 11644
Train - Loss (one batch): 0.06499
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20022, 2136.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.20s
- Epoch 046, ExpID 11644
Train - Loss (one batch): 0.07159
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25883, 0.20138, 2196.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.53s
- Epoch 047, ExpID 11644
Train - Loss (one batch): 0.06931
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20017, 2133.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.86s
- Epoch 048, ExpID 11644
Train - Loss (one batch): 0.07037
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25885, 0.20139, 2192.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.49s
- Epoch 049, ExpID 11644
Train - Loss (one batch): 0.06570
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25880, 0.20101, 2175.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.31s
- Epoch 050, ExpID 11644
Train - Loss (one batch): 0.06682
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20093, 2171.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.23s
- Epoch 051, ExpID 11644
Train - Loss (one batch): 0.07225
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19955, 2101.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.40s
- Epoch 052, ExpID 11644
Train - Loss (one batch): 0.07278
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19953, 2100.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.08s
- Epoch 053, ExpID 11644
Train - Loss (one batch): 0.06971
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25876, 0.20052, 2151.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.68s
- Epoch 054, ExpID 11644
Train - Loss (one batch): 0.06642
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.20149, 2196.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.29s
- Epoch 055, ExpID 11644
Train - Loss (one batch): 0.06958
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.19904, 2072.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.03s
- Epoch 056, ExpID 11644
Train - Loss (one batch): 0.07274
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25883, 0.20112, 2179.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.91s
- Epoch 057, ExpID 11644
Train - Loss (one batch): 0.07353
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25892, 0.19856, 2043.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.69s
- Epoch 058, ExpID 11644
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20095, 2172.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.66s
- Epoch 059, ExpID 11644
Train - Loss (one batch): 0.06584
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06712, 0.06712, 0.25908, 0.20252, 2242.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.17s
- Epoch 060, ExpID 11644
Train - Loss (one batch): 0.06903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19983, 2117.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.97s
- Epoch 061, ExpID 11644
Train - Loss (one batch): 0.06846
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19984, 2118.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.62s
- Epoch 062, ExpID 11644
Train - Loss (one batch): 0.06314
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20080, 2165.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.43s
- Epoch 063, ExpID 11644
Train - Loss (one batch): 0.06631
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20032, 2141.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.81s
- Epoch 064, ExpID 11644
Train - Loss (one batch): 0.06976
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25878, 0.19967, 2108.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.16s
- Epoch 065, ExpID 11644
Train - Loss (one batch): 0.07624
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20065, 2157.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.48s
- Epoch 066, ExpID 11644
Train - Loss (one batch): 0.06663
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25887, 0.20153, 2199.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.64s
- Epoch 067, ExpID 11644
Train - Loss (one batch): 0.07229
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20054, 2153.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.09s
- Epoch 068, ExpID 11644
Train - Loss (one batch): 0.06516
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.20004, 2128.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.11s
- Epoch 069, ExpID 11644
Train - Loss (one batch): 0.06525
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06703, 0.06703, 0.25889, 0.20153, 2198.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.83s
- Epoch 070, ExpID 11644
Train - Loss (one batch): 0.07212
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20116, 2182.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.83s
- Epoch 071, ExpID 11644
Train - Loss (one batch): 0.06630
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25886, 0.20143, 2194.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.45s
- Epoch 072, ExpID 11644
Train - Loss (one batch): 0.06857
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20050, 2152.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.37s
- Epoch 073, ExpID 11644
Train - Loss (one batch): 0.07441
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.19966, 2107.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.41s
- Epoch 074, ExpID 11644
Train - Loss (one batch): 0.06899
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20094, 2170.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.35s
- Epoch 075, ExpID 11644
Train - Loss (one batch): 0.06431
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20039, 2144.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.90s
- Epoch 076, ExpID 11644
Train - Loss (one batch): 0.06629
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25872, 0.20031, 2143.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.07s
- Epoch 077, ExpID 11644
Train - Loss (one batch): 0.07220
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06710, 0.06710, 0.25903, 0.20226, 2230.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.55s
- Epoch 078, ExpID 11644
Train - Loss (one batch): 0.06394
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25893, 0.20188, 2216.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.69s
- Epoch 079, ExpID 11644
Train - Loss (one batch): 0.07025
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06706, 0.06706, 0.25897, 0.20164, 2201.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.95s
- Epoch 080, ExpID 11644
Train - Loss (one batch): 0.07025
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25880, 0.20106, 2179.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.73s
- Epoch 081, ExpID 11644
Train - Loss (one batch): 0.06678
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06722, 0.06722, 0.25926, 0.20304, 2262.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.68s
- Epoch 082, ExpID 11644
Train - Loss (one batch): 0.06500
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25882, 0.19902, 2070.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.92s
- Epoch 083, ExpID 11644
Train - Loss (one batch): 0.06794
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25878, 0.19929, 2087.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.93s
- Epoch 084, ExpID 11644
Train - Loss (one batch): 0.06992
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20079, 2165.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.11s
- Epoch 085, ExpID 11644
Train - Loss (one batch): 0.07332
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25888, 0.20148, 2195.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.76s
- Epoch 086, ExpID 11644
Train - Loss (one batch): 0.06940
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25874, 0.20045, 2149.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.56s
- Epoch 087, ExpID 11644
Train - Loss (one batch): 0.06816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06710, 0.06710, 0.25903, 0.20229, 2233.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.26s
- Epoch 088, ExpID 11644
Train - Loss (one batch): 0.06790
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06693, 0.06693, 0.25871, 0.19969, 2111.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.31s
- Epoch 089, ExpID 11644
Train - Loss (one batch): 0.06924
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19981, 2116.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 27.83s
- Epoch 090, ExpID 11644
Train - Loss (one batch): 0.07167
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20049, 2150.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.08s
- Epoch 091, ExpID 11644
Train - Loss (one batch): 0.07236
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.20000, 2126.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.67s
- Epoch 092, ExpID 11644
Train - Loss (one batch): 0.07355
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.20123, 2182.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.59s
- Epoch 093, ExpID 11644
Train - Loss (one batch): 0.06952
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19956, 2102.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.64s
- Epoch 094, ExpID 11644
Train - Loss (one batch): 0.06954
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25885, 0.20144, 2195.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 31.62s
- Epoch 095, ExpID 11644
Train - Loss (one batch): 0.05417
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20051, 2151.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 30.78s
- Epoch 096, ExpID 11644
Train - Loss (one batch): 0.07295
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.19939, 2093.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 29.81s
- Epoch 097, ExpID 11644
Train - Loss (one batch): 0.06827
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25874, 0.19967, 2108.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.07s
- Epoch 098, ExpID 11644
Train - Loss (one batch): 0.07155
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19956, 2104.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 32.29s
- Epoch 099, ExpID 11644
Train - Loss (one batch): 0.06859
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20012, 2128.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 28.84s
True
Couldn't import umap
PID, device: 2146657 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=175, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7192065
n_train_batches: 225
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 21:46:56
run_iTransformer.py --history 12 --model iTransformer --dataset physionet --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=83, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2180281, pred_window=36, ndim=41, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
True
Couldn't import umap
PID, device: 2180281 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=83, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7144961
n_train_batches: 225
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py", line 406, in <module>
    train_res["loss"].backward()
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 21:52:45
run_iTransformer.py --history 24 --model iTransformer --dataset mimic --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=280, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2185121, pred_window=24, ndim=96, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 3720
Train - Loss (one batch): 0.07365
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26694, 0.21656, 852.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 150.99s
- Epoch 001, ExpID 3720
Train - Loss (one batch): 0.07887
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07329, 0.07329, 0.27072, 0.22848, 993.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 139.51s
- Epoch 002, ExpID 3720
Train - Loss (one batch): 0.06916
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07150, 0.07150, 0.26739, 0.21918, 882.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 141.91s
- Epoch 003, ExpID 3720
Train - Loss (one batch): 0.08596
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26641, 0.21781, 886.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 155.50s
- Epoch 004, ExpID 3720
Train - Loss (one batch): 0.08150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07133, 0.07133, 0.26708, 0.21706, 855.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.00s
- Epoch 005, ExpID 3720
Train - Loss (one batch): 0.07166
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07197, 0.07197, 0.26827, 0.22235, 916.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.66s
- Epoch 006, ExpID 3720
Train - Loss (one batch): 0.07363
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07270, 0.07270, 0.26964, 0.22601, 954.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.89s
- Epoch 007, ExpID 3720
Train - Loss (one batch): 0.07162
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07137, 0.07137, 0.26715, 0.21252, 798.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.19s
- Epoch 008, ExpID 3720
Train - Loss (one batch): 0.07471
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07512, 0.07512, 0.27408, 0.23413, 1033.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.41s
- Epoch 009, ExpID 3720
Train - Loss (one batch): 0.07832
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07154, 0.07154, 0.26747, 0.21919, 880.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.52s
- Epoch 010, ExpID 3720
Train - Loss (one batch): 0.07580
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07181, 0.07181, 0.26798, 0.22152, 907.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.41s
- Epoch 011, ExpID 3720
Train - Loss (one batch): 0.08061
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07143, 0.07143, 0.26727, 0.21856, 871.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.00s
- Epoch 012, ExpID 3720
Train - Loss (one batch): 0.07969
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07130, 0.07130, 0.26702, 0.21667, 850.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.95s
- Epoch 013, ExpID 3720
Train - Loss (one batch): 0.07767
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07130, 0.07130, 0.26702, 0.21794, 865.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.62s
- Epoch 014, ExpID 3720
Train - Loss (one batch): 0.06499
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07156, 0.07156, 0.26750, 0.21961, 883.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.08s
- Epoch 015, ExpID 3720
Train - Loss (one batch): 0.08354
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07153, 0.07153, 0.26745, 0.21933, 882.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.03s
- Epoch 016, ExpID 3720
Train - Loss (one batch): 0.06678
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07148, 0.07148, 0.26736, 0.21876, 875.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.84s
- Epoch 017, ExpID 3720
Train - Loss (one batch): 0.07432
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26693, 0.21602, 841.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.16s
- Epoch 018, ExpID 3720
Train - Loss (one batch): 0.07430
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07127, 0.07127, 0.26697, 0.21627, 836.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.12s
- Epoch 019, ExpID 3720
Train - Loss (one batch): 0.06997
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26673, 0.21500, 817.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 142.45s
- Epoch 020, ExpID 3720
Train - Loss (one batch): 0.07813
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07142, 0.07142, 0.26724, 0.22021, 876.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.70s
- Epoch 021, ExpID 3720
Train - Loss (one batch): 0.07206
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07121, 0.07121, 0.26686, 0.21781, 848.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.86s
- Epoch 022, ExpID 3720
Train - Loss (one batch): 0.05948
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07113, 0.07113, 0.26670, 0.21701, 838.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.53s
- Epoch 023, ExpID 3720
Train - Loss (one batch): 0.06662
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07168, 0.07168, 0.26774, 0.22227, 896.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.32s
- Epoch 024, ExpID 3720
Train - Loss (one batch): 0.08066
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26658, 0.21618, 826.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.96s
- Epoch 025, ExpID 3720
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07140, 0.07140, 0.26721, 0.22056, 877.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.69s
- Epoch 026, ExpID 3720
Train - Loss (one batch): 0.07347
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07127, 0.07127, 0.26696, 0.21627, 830.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.25s
- Epoch 027, ExpID 3720
Train - Loss (one batch): 0.06779
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26677, 0.21673, 834.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.36s
- Epoch 028, ExpID 3720
Train - Loss (one batch): 0.07835
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26682, 0.21898, 856.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.75s
- Epoch 029, ExpID 3720
Train - Loss (one batch): 0.07401
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07126, 0.07126, 0.26695, 0.21554, 822.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.85s
- Epoch 030, ExpID 3720
Train - Loss (one batch): 0.07344
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26667, 0.21716, 835.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.42s
- Epoch 031, ExpID 3720
Train - Loss (one batch): 0.08243
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07123, 0.07123, 0.26690, 0.21809, 849.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.57s
- Epoch 032, ExpID 3720
Train - Loss (one batch): 0.07055
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07129, 0.07129, 0.26700, 0.21992, 865.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.47s
- Epoch 033, ExpID 3720
Train - Loss (one batch): 0.06930
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07132, 0.07132, 0.26706, 0.21956, 865.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 138.36s
- Epoch 034, ExpID 3720
Train - Loss (one batch): 0.07552
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26690, 0.21594, 826.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.63s
- Epoch 035, ExpID 3720
Train - Loss (one batch): 0.07577
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26691, 0.21640, 831.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.97s
- Epoch 036, ExpID 3720
Train - Loss (one batch): 0.07510
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07132, 0.07132, 0.26705, 0.21950, 864.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.33s
- Epoch 037, ExpID 3720
Train - Loss (one batch): 0.07726
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26676, 0.21851, 850.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 139.18s
- Epoch 038, ExpID 3720
Train - Loss (one batch): 0.08804
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21573, 818.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.24s
- Epoch 039, ExpID 3720
Train - Loss (one batch): 0.07107
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07121, 0.07121, 0.26684, 0.21606, 826.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.37s
- Epoch 040, ExpID 3720
Train - Loss (one batch): 0.07238
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21629, 823.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 141.16s
- Epoch 041, ExpID 3720
Train - Loss (one batch): 0.07449
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26675, 0.21747, 840.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.46s
- Epoch 042, ExpID 3720
Train - Loss (one batch): 0.07013
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26683, 0.21762, 843.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 140.54s
- Epoch 043, ExpID 3720
Train - Loss (one batch): 0.07588
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26666, 0.21731, 837.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 146.08s
- Epoch 044, ExpID 3720
Train - Loss (one batch): 0.07114
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07112, 0.07112, 0.26667, 0.21582, 821.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 148.68s
- Epoch 045, ExpID 3720
Train - Loss (one batch): 0.06885
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07107, 0.07107, 0.26659, 0.21716, 834.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 147.90s
- Epoch 046, ExpID 3720
Train - Loss (one batch): 0.07298
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26683, 0.21692, 836.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 136.57s
- Epoch 047, ExpID 3720
Train - Loss (one batch): 0.06961
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26693, 0.21564, 823.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 136.47s
- Epoch 048, ExpID 3720
Train - Loss (one batch): 0.07161
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21569, 822.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 130.55s
- Epoch 049, ExpID 3720
Train - Loss (one batch): 0.08031
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07117, 0.07117, 0.26678, 0.21685, 834.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 136.51s
- Epoch 050, ExpID 3720
Train - Loss (one batch): 0.07419
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26673, 0.21770, 842.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 135.84s
- Epoch 051, ExpID 3720
Train - Loss (one batch): 0.06886
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26674, 0.21600, 824.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 136.50s
- Epoch 052, ExpID 3720
Train - Loss (one batch): 0.07202
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26675, 0.21582, 822.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 136.72s
- Epoch 053, ExpID 3720
Train - Loss (one batch): 0.07465
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07118, 0.07118, 0.26679, 0.21690, 835.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 118.99s
- Epoch 054, ExpID 3720
Train - Loss (one batch): 0.08199
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26690, 0.21904, 857.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.38s
- Epoch 055, ExpID 3720
Train - Loss (one batch): 0.07070
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21668, 833.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.14s
- Epoch 056, ExpID 3720
Train - Loss (one batch): 0.08121
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26673, 0.21614, 826.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 117.56s
- Epoch 057, ExpID 3720
Train - Loss (one batch): 0.08086
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26652, 0.21672, 827.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.14s
- Epoch 058, ExpID 3720
Train - Loss (one batch): 0.07485
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07108, 0.07108, 0.26661, 0.21588, 826.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.99s
- Epoch 059, ExpID 3720
Train - Loss (one batch): 0.07399
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26672, 0.21677, 832.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.59s
- Epoch 060, ExpID 3720
Train - Loss (one batch): 0.06996
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21683, 835.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.42s
- Epoch 061, ExpID 3720
Train - Loss (one batch): 0.10100
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07112, 0.07112, 0.26668, 0.21681, 834.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.42s
- Epoch 062, ExpID 3720
Train - Loss (one batch): 0.07875
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21815, 851.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.38s
- Epoch 063, ExpID 3720
Train - Loss (one batch): 0.07986
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26658, 0.21743, 845.18%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.10s
- Epoch 064, ExpID 3720
Train - Loss (one batch): 0.07656
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26651, 0.21579, 828.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.59s
- Epoch 065, ExpID 3720
Train - Loss (one batch): 0.06816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21634, 832.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.78s
- Epoch 066, ExpID 3720
Train - Loss (one batch): 0.07255
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07094, 0.07094, 0.26635, 0.21643, 831.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 130.15s
- Epoch 067, ExpID 3720
Train - Loss (one batch): 0.07563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07105, 0.07105, 0.26655, 0.21843, 853.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.22s
- Epoch 068, ExpID 3720
Train - Loss (one batch): 0.06944
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26648, 0.21728, 842.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.58s
- Epoch 069, ExpID 3720
Train - Loss (one batch): 0.07557
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26640, 0.21552, 821.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.92s
- Epoch 070, ExpID 3720
Train - Loss (one batch): 0.07989
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07099, 0.07099, 0.26644, 0.21746, 844.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.95s
- Epoch 071, ExpID 3720
Train - Loss (one batch): 0.06703
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26649, 0.21660, 836.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.37s
- Epoch 072, ExpID 3720
Train - Loss (one batch): 0.07638
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26676, 0.21926, 863.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.16s
- Epoch 073, ExpID 3720
Train - Loss (one batch): 0.07265
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07108, 0.07108, 0.26660, 0.21683, 837.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.12s
- Epoch 074, ExpID 3720
Train - Loss (one batch): 0.07512
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26652, 0.21839, 854.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.33s
- Epoch 075, ExpID 3720
Train - Loss (one batch): 0.07964
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26649, 0.21817, 850.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.17s
- Epoch 076, ExpID 3720
Train - Loss (one batch): 0.06050
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26649, 0.21647, 833.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.37s
- Epoch 077, ExpID 3720
Train - Loss (one batch): 0.07476
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07118, 0.07118, 0.26680, 0.21842, 858.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.82s
- Epoch 078, ExpID 3720
Train - Loss (one batch): 0.07897
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26648, 0.21707, 840.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.01s
- Epoch 079, ExpID 3720
Train - Loss (one batch): 0.07489
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07096, 0.07096, 0.26639, 0.21732, 840.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.18s
- Epoch 080, ExpID 3720
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26650, 0.21533, 821.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.86s
- Epoch 081, ExpID 3720
Train - Loss (one batch): 0.06949
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26649, 0.21864, 853.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.67s
- Epoch 082, ExpID 3720
Train - Loss (one batch): 0.07955
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21598, 831.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.26s
- Epoch 083, ExpID 3720
Train - Loss (one batch): 0.07740
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07098, 0.07098, 0.26641, 0.21623, 830.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.22s
- Epoch 084, ExpID 3720
Train - Loss (one batch): 0.08254
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21673, 835.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.74s
- Epoch 085, ExpID 3720
Train - Loss (one batch): 0.07830
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07090, 0.07090, 0.26628, 0.21671, 832.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 128.39s
- Epoch 086, ExpID 3720
Train - Loss (one batch): 0.06781
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21791, 846.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.45s
- Epoch 087, ExpID 3720
Train - Loss (one batch): 0.07804
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07093, 0.07093, 0.26633, 0.21604, 826.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 116.44s
- Epoch 088, ExpID 3720
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21656, 835.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 116.95s
- Epoch 089, ExpID 3720
Train - Loss (one batch): 0.07889
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26640, 0.21654, 834.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.67s
- Epoch 090, ExpID 3720
Train - Loss (one batch): 0.07345
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07094, 0.07094, 0.26635, 0.21686, 836.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 114.98s
- Epoch 091, ExpID 3720
Train - Loss (one batch): 0.06858
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21851, 851.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 118.12s
- Epoch 092, ExpID 3720
Train - Loss (one batch): 0.06851
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07092, 0.07092, 0.26631, 0.21656, 832.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 114.25s
- Epoch 093, ExpID 3720
Train - Loss (one batch): 0.06977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26667, 0.21729, 843.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 117.72s
- Epoch 094, ExpID 3720
Train - Loss (one batch): 0.06666
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07109, 0.07109, 0.26662, 0.21607, 832.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.69s
- Epoch 095, ExpID 3720
Train - Loss (one batch): 0.07040
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21835, 853.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 116.07s
- Epoch 096, ExpID 3720
Train - Loss (one batch): 0.07244
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21693, 838.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.28s
- Epoch 097, ExpID 3720
Train - Loss (one batch): 0.07578
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21729, 843.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.98s
- Epoch 098, ExpID 3720
Train - Loss (one batch): 0.07122
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21788, 847.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 117.07s
- Epoch 099, ExpID 3720
Train - Loss (one batch): 0.07353
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07098, 0.07098, 0.26641, 0.21736, 841.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.38s
True
Couldn't import umap
PID, device: 2185121 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=280, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7245825
n_train_batches: 440
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-20 01:31:10
run_iTransformer.py --history 36 --model iTransformer --dataset mimic --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=464, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=2310886, pred_window=12, ndim=96, patch_layer=2, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 2376
Train - Loss (one batch): 0.07799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06866, 0.06866, 0.26202, 0.20672, 696.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 72.71s
- Epoch 001, ExpID 2376
Train - Loss (one batch): 0.07370
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21372, 778.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.29s
- Epoch 002, ExpID 2376
Train - Loss (one batch): 0.06950
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06964, 0.06964, 0.26389, 0.20873, 726.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.92s
- Epoch 003, ExpID 2376
Train - Loss (one batch): 0.05623
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07006, 0.07006, 0.26468, 0.21473, 787.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.66s
- Epoch 004, ExpID 2376
Train - Loss (one batch): 0.06341
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.20987, 735.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.20s
- Epoch 005, ExpID 2376
Train - Loss (one batch): 0.07468
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06994, 0.06994, 0.26446, 0.21282, 765.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.13s
- Epoch 006, ExpID 2376
Train - Loss (one batch): 0.06581
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06997, 0.06997, 0.26451, 0.20701, 701.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.93s
- Epoch 007, ExpID 2376
Train - Loss (one batch): 0.05903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06984, 0.06984, 0.26427, 0.21263, 766.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.89s
- Epoch 008, ExpID 2376
Train - Loss (one batch): 0.06444
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.21172, 756.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.01s
- Epoch 009, ExpID 2376
Train - Loss (one batch): 0.06315
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07005, 0.07005, 0.26468, 0.21492, 790.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.74s
- Epoch 010, ExpID 2376
Train - Loss (one batch): 0.08195
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06999, 0.06999, 0.26456, 0.21424, 782.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.57s
- Epoch 011, ExpID 2376
Train - Loss (one batch): 0.05158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06978, 0.06978, 0.26416, 0.21083, 746.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.36s
- Epoch 012, ExpID 2376
Train - Loss (one batch): 0.06563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.20911, 727.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.17s
- Epoch 013, ExpID 2376
Train - Loss (one batch): 0.06887
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06979, 0.06979, 0.26418, 0.21009, 738.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.97s
- Epoch 014, ExpID 2376
Train - Loss (one batch): 0.07258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.21127, 751.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.92s
- Epoch 015, ExpID 2376
Train - Loss (one batch): 0.06479
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26422, 0.21066, 744.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.48s
- Epoch 016, ExpID 2376
Train - Loss (one batch): 0.07007
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06984, 0.06984, 0.26426, 0.21135, 751.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.42s
- Epoch 017, ExpID 2376
Train - Loss (one batch): 0.06943
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21134, 751.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.05s
- Epoch 018, ExpID 2376
Train - Loss (one batch): 0.06164
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21098, 746.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.51s
- Epoch 019, ExpID 2376
Train - Loss (one batch): 0.06647
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26437, 0.20884, 722.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.22s
- Epoch 020, ExpID 2376
Train - Loss (one batch): 0.05979
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06985, 0.06985, 0.26429, 0.21031, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.30s
- Epoch 021, ExpID 2376
Train - Loss (one batch): 0.07134
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06997, 0.06997, 0.26451, 0.21318, 770.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.17s
- Epoch 022, ExpID 2376
Train - Loss (one batch): 0.06677
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26434, 0.20932, 728.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.90s
- Epoch 023, ExpID 2376
Train - Loss (one batch): 0.07364
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21159, 753.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.30s
- Epoch 024, ExpID 2376
Train - Loss (one batch): 0.06939
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21008, 736.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.28s
- Epoch 025, ExpID 2376
Train - Loss (one batch): 0.06135
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06993, 0.06993, 0.26444, 0.21263, 764.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.88s
- Epoch 026, ExpID 2376
Train - Loss (one batch): 0.06466
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21154, 752.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.53s
- Epoch 027, ExpID 2376
Train - Loss (one batch): 0.07300
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21182, 756.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.87s
- Epoch 028, ExpID 2376
Train - Loss (one batch): 0.07631
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21045, 741.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.22s
- Epoch 029, ExpID 2376
Train - Loss (one batch): 0.07008
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.20950, 730.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.02s
- Epoch 030, ExpID 2376
Train - Loss (one batch): 0.05081
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.20924, 727.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.53s
- Epoch 031, ExpID 2376
Train - Loss (one batch): 0.05875
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21065, 743.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.52s
- Epoch 032, ExpID 2376
Train - Loss (one batch): 0.06896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.20963, 731.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 66.02s
- Epoch 033, ExpID 2376
Train - Loss (one batch): 0.06411
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21159, 753.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.71s
- Epoch 034, ExpID 2376
Train - Loss (one batch): 0.05163
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26437, 0.21190, 756.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.22s
- Epoch 035, ExpID 2376
Train - Loss (one batch): 0.06800
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21012, 737.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.39s
- Epoch 036, ExpID 2376
Train - Loss (one batch): 0.06504
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21078, 744.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.48s
- Epoch 037, ExpID 2376
Train - Loss (one batch): 0.06461
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21039, 740.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.05s
- Epoch 038, ExpID 2376
Train - Loss (one batch): 0.06054
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20996, 735.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.49s
- Epoch 039, ExpID 2376
Train - Loss (one batch): 0.06059
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06998, 0.06998, 0.26454, 0.21342, 772.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.01s
- Epoch 040, ExpID 2376
Train - Loss (one batch): 0.05553
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21136, 751.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.21s
- Epoch 041, ExpID 2376
Train - Loss (one batch): 0.07474
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21117, 749.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.85s
- Epoch 042, ExpID 2376
Train - Loss (one batch): 0.06510
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21100, 747.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.67s
- Epoch 043, ExpID 2376
Train - Loss (one batch): 0.06436
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06996, 0.06996, 0.26451, 0.20777, 709.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.06s
- Epoch 044, ExpID 2376
Train - Loss (one batch): 0.06120
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21004, 736.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.87s
- Epoch 045, ExpID 2376
Train - Loss (one batch): 0.06275
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21172, 754.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.28s
- Epoch 046, ExpID 2376
Train - Loss (one batch): 0.06500
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21170, 754.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.76s
- Epoch 047, ExpID 2376
Train - Loss (one batch): 0.06222
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20972, 732.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.92s
- Epoch 048, ExpID 2376
Train - Loss (one batch): 0.05829
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21157, 753.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.74s
- Epoch 049, ExpID 2376
Train - Loss (one batch): 0.07969
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06990, 0.06990, 0.26438, 0.21205, 758.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.49s
- Epoch 050, ExpID 2376
Train - Loss (one batch): 0.07150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21161, 753.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.42s
- Epoch 051, ExpID 2376
Train - Loss (one batch): 0.06871
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21095, 746.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.78s
- Epoch 052, ExpID 2376
Train - Loss (one batch): 0.07350
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06990, 0.06990, 0.26438, 0.21206, 758.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.34s
- Epoch 053, ExpID 2376
Train - Loss (one batch): 0.05681
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21173, 755.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.49s
- Epoch 054, ExpID 2376
Train - Loss (one batch): 0.06849
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21144, 751.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.26s
- Epoch 055, ExpID 2376
Train - Loss (one batch): 0.05925
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21147, 752.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.03s
- Epoch 056, ExpID 2376
Train - Loss (one batch): 0.06648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21108, 748.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.67s
- Epoch 057, ExpID 2376
Train - Loss (one batch): 0.06768
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21030, 739.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 66.17s
- Epoch 058, ExpID 2376
Train - Loss (one batch): 0.07644
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21026, 738.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.47s
- Epoch 059, ExpID 2376
Train - Loss (one batch): 0.06258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20984, 734.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.68s
- Epoch 060, ExpID 2376
Train - Loss (one batch): 0.06767
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21225, 760.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.48s
- Epoch 061, ExpID 2376
Train - Loss (one batch): 0.07193
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21012, 737.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.96s
- Epoch 062, ExpID 2376
Train - Loss (one batch): 0.06308
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21147, 752.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.12s
- Epoch 063, ExpID 2376
Train - Loss (one batch): 0.06509
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.35s
- Epoch 064, ExpID 2376
Train - Loss (one batch): 0.07536
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21071, 743.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.36s
- Epoch 065, ExpID 2376
Train - Loss (one batch): 0.07077
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21069, 743.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.82s
- Epoch 066, ExpID 2376
Train - Loss (one batch): 0.07006
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20977, 733.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.11s
- Epoch 067, ExpID 2376
Train - Loss (one batch): 0.08869
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.20945, 729.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.42s
- Epoch 068, ExpID 2376
Train - Loss (one batch): 0.07095
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21121, 749.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.33s
- Epoch 069, ExpID 2376
Train - Loss (one batch): 0.06695
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06998, 0.06998, 0.26453, 0.21332, 771.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.26s
- Epoch 070, ExpID 2376
Train - Loss (one batch): 0.06173
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21173, 755.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.70s
- Epoch 071, ExpID 2376
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21029, 739.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.05s
- Epoch 072, ExpID 2376
Train - Loss (one batch): 0.06192
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20969, 732.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.25s
- Epoch 073, ExpID 2376
Train - Loss (one batch): 0.07430
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21050, 741.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.01s
- Epoch 074, ExpID 2376
Train - Loss (one batch): 0.07602
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.84s
- Epoch 075, ExpID 2376
Train - Loss (one batch): 0.06670
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21153, 752.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.48s
- Epoch 076, ExpID 2376
Train - Loss (one batch): 0.06597
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.38s
- Epoch 077, ExpID 2376
Train - Loss (one batch): 0.06249
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21064, 743.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.74s
- Epoch 078, ExpID 2376
Train - Loss (one batch): 0.07370
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21223, 760.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.86s
- Epoch 079, ExpID 2376
Train - Loss (one batch): 0.06542
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21127, 750.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.80s
- Epoch 080, ExpID 2376
Train - Loss (one batch): 0.06014
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21070, 743.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.78s
- Epoch 081, ExpID 2376
Train - Loss (one batch): 0.06462
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21097, 746.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.84s
- Epoch 082, ExpID 2376
Train - Loss (one batch): 0.06923
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20993, 735.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.02s
- Epoch 083, ExpID 2376
Train - Loss (one batch): 0.06024
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21150, 752.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.31s
- Epoch 084, ExpID 2376
Train - Loss (one batch): 0.06473
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21079, 744.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.87s
- Epoch 085, ExpID 2376
Train - Loss (one batch): 0.06807
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21182, 756.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.28s
- Epoch 086, ExpID 2376
Train - Loss (one batch): 0.06324
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21136, 751.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.30s
- Epoch 087, ExpID 2376
Train - Loss (one batch): 0.06317
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06992, 0.06992, 0.26443, 0.21250, 763.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.68s
- Epoch 088, ExpID 2376
Train - Loss (one batch): 0.06389
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21172, 754.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.33s
- Epoch 089, ExpID 2376
Train - Loss (one batch): 0.07835
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21121, 749.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.41s
- Epoch 090, ExpID 2376
Train - Loss (one batch): 0.06422
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21049, 741.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.43s
- Epoch 091, ExpID 2376
Train - Loss (one batch): 0.07513
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20968, 732.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.93s
- Epoch 092, ExpID 2376
Train - Loss (one batch): 0.06535
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21063, 743.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 66.31s
- Epoch 093, ExpID 2376
Train - Loss (one batch): 0.06454
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21222, 760.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.83s
- Epoch 094, ExpID 2376
Train - Loss (one batch): 0.05238
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21119, 749.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.03s
- Epoch 095, ExpID 2376
Train - Loss (one batch): 0.06385
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06993, 0.06993, 0.26445, 0.21270, 765.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.58s
- Epoch 096, ExpID 2376
Train - Loss (one batch): 0.05735
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21035, 739.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.45s
- Epoch 097, ExpID 2376
Train - Loss (one batch): 0.05959
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21115, 748.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.96s
- Epoch 098, ExpID 2376
Train - Loss (one batch): 0.06648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21155, 753.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.47s
- Epoch 099, ExpID 2376
Train - Loss (one batch): 0.06690
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21112, 748.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.81s
True
Couldn't import umap
PID, device: 2310886 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=464, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7340033
n_train_batches: 440
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-20 03:22:47
run_iTransformer.py --history 12 --model iTransformer --dataset mimic --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=133, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2373153, pred_window=36, ndim=96, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 22244
Train - Loss (one batch): 0.06495
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26651, 0.21105, 813.18%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 178.85s
- Epoch 001, ExpID 22244
Train - Loss (one batch): 0.06884
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07253, 0.07253, 0.26931, 0.22236, 955.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.33s
- Epoch 002, ExpID 22244
Train - Loss (one batch): 0.06704
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07226, 0.07226, 0.26882, 0.21947, 922.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.02s
- Epoch 003, ExpID 22244
Train - Loss (one batch): 0.07194
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07213, 0.07213, 0.26858, 0.21958, 923.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.67s
- Epoch 004, ExpID 22244
Train - Loss (one batch): 0.07580
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07229, 0.07229, 0.26888, 0.22094, 932.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 156.97s
- Epoch 005, ExpID 22244
Train - Loss (one batch): 0.07726
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07203, 0.07203, 0.26839, 0.21554, 857.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.09s
- Epoch 006, ExpID 22244
Train - Loss (one batch): 0.07064
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07208, 0.07208, 0.26848, 0.21951, 908.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.37s
- Epoch 007, ExpID 22244
Train - Loss (one batch): 0.07242
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07204, 0.07204, 0.26841, 0.21827, 896.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.58s
- Epoch 008, ExpID 22244
Train - Loss (one batch): 0.07432
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07210, 0.07210, 0.26851, 0.21856, 895.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.72s
- Epoch 009, ExpID 22244
Train - Loss (one batch): 0.06681
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07220, 0.07220, 0.26870, 0.22065, 927.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.01s
- Epoch 010, ExpID 22244
Train - Loss (one batch): 0.07218
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07239, 0.07239, 0.26905, 0.22246, 947.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.63s
- Epoch 011, ExpID 22244
Train - Loss (one batch): 0.06702
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07200, 0.07200, 0.26833, 0.21824, 891.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.17s
- Epoch 012, ExpID 22244
Train - Loss (one batch): 0.06826
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07222, 0.07222, 0.26873, 0.21932, 916.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.75s
- Epoch 013, ExpID 22244
Train - Loss (one batch): 0.07064
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07209, 0.07209, 0.26850, 0.21430, 835.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.97s
- Epoch 014, ExpID 22244
Train - Loss (one batch): 0.07232
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07204, 0.07204, 0.26840, 0.21581, 859.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.60s
- Epoch 015, ExpID 22244
Train - Loss (one batch): 0.07648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07197, 0.07197, 0.26827, 0.21722, 880.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 157.93s
- Epoch 016, ExpID 22244
Train - Loss (one batch): 0.08501
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07194, 0.07194, 0.26822, 0.21827, 883.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.33s
- Epoch 017, ExpID 22244
Train - Loss (one batch): 0.06939
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07221, 0.07221, 0.26871, 0.21879, 910.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.04s
- Epoch 018, ExpID 22244
Train - Loss (one batch): 0.07540
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07189, 0.07189, 0.26812, 0.21914, 898.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 156.88s
- Epoch 019, ExpID 22244
Train - Loss (one batch): 0.07396
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07191, 0.07191, 0.26816, 0.21928, 903.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.40s
- Epoch 020, ExpID 22244
Train - Loss (one batch): 0.06606
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07183, 0.07183, 0.26801, 0.21836, 887.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.86s
- Epoch 021, ExpID 22244
Train - Loss (one batch): 0.07300
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07202, 0.07202, 0.26837, 0.21667, 872.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.05s
- Epoch 022, ExpID 22244
Train - Loss (one batch): 0.06097
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07198, 0.07198, 0.26830, 0.21569, 855.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.61s
- Epoch 023, ExpID 22244
Train - Loss (one batch): 0.07039
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07202, 0.07202, 0.26837, 0.21977, 905.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.98s
- Epoch 024, ExpID 22244
Train - Loss (one batch): 0.07421
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07178, 0.07178, 0.26792, 0.21809, 881.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.60s
- Epoch 025, ExpID 22244
Train - Loss (one batch): 0.07533
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07182, 0.07182, 0.26800, 0.21653, 870.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.31s
- Epoch 026, ExpID 22244
Train - Loss (one batch): 0.08312
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07184, 0.07184, 0.26803, 0.21624, 855.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.08s
- Epoch 027, ExpID 22244
Train - Loss (one batch): 0.07130
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07187, 0.07187, 0.26809, 0.21914, 903.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.83s
- Epoch 028, ExpID 22244
Train - Loss (one batch): 0.07493
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07186, 0.07186, 0.26808, 0.21925, 901.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.10s
- Epoch 029, ExpID 22244
Train - Loss (one batch): 0.07353
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07204, 0.07204, 0.26841, 0.21690, 870.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.97s
- Epoch 030, ExpID 22244
Train - Loss (one batch): 0.07199
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07180, 0.07180, 0.26796, 0.21818, 890.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.61s
- Epoch 031, ExpID 22244
Train - Loss (one batch): 0.06651
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07180, 0.07180, 0.26795, 0.21931, 904.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.86s
- Epoch 032, ExpID 22244
Train - Loss (one batch): 0.06514
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07178, 0.07178, 0.26792, 0.21679, 871.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.70s
- Epoch 033, ExpID 22244
Train - Loss (one batch): 0.07114
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07186, 0.07186, 0.26807, 0.21695, 875.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.66s
- Epoch 034, ExpID 22244
Train - Loss (one batch): 0.07302
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07185, 0.07185, 0.26804, 0.22038, 914.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.32s
- Epoch 035, ExpID 22244
Train - Loss (one batch): 0.07421
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21791, 876.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.64s
- Epoch 036, ExpID 22244
Train - Loss (one batch): 0.07787
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07189, 0.07189, 0.26813, 0.21975, 905.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.42s
- Epoch 037, ExpID 22244
Train - Loss (one batch): 0.06949
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07175, 0.07175, 0.26787, 0.21923, 900.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.82s
- Epoch 038, ExpID 22244
Train - Loss (one batch): 0.08452
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07214, 0.07214, 0.26858, 0.21534, 855.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.29s
- Epoch 039, ExpID 22244
Train - Loss (one batch): 0.06715
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07200, 0.07200, 0.26832, 0.22059, 913.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.59s
- Epoch 040, ExpID 22244
Train - Loss (one batch): 0.07220
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07177, 0.07177, 0.26790, 0.21731, 876.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.48s
- Epoch 041, ExpID 22244
Train - Loss (one batch): 0.07408
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07183, 0.07183, 0.26800, 0.21914, 896.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 161.08s
- Epoch 042, ExpID 22244
Train - Loss (one batch): 0.07416
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07182, 0.07182, 0.26799, 0.21947, 901.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.77s
- Epoch 043, ExpID 22244
Train - Loss (one batch): 0.06634
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07187, 0.07187, 0.26809, 0.21804, 886.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.33s
- Epoch 044, ExpID 22244
Train - Loss (one batch): 0.07995
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07186, 0.07186, 0.26806, 0.21809, 891.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.26s
- Epoch 045, ExpID 22244
Train - Loss (one batch): 0.06779
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21706, 866.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.26s
- Epoch 046, ExpID 22244
Train - Loss (one batch): 0.06909
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07188, 0.07188, 0.26811, 0.21633, 864.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.05s
- Epoch 047, ExpID 22244
Train - Loss (one batch): 0.07562
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07173, 0.07173, 0.26782, 0.21721, 871.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.30s
- Epoch 048, ExpID 22244
Train - Loss (one batch): 0.06512
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07188, 0.07188, 0.26810, 0.21876, 893.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.98s
- Epoch 049, ExpID 22244
Train - Loss (one batch): 0.07073
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07214, 0.07214, 0.26860, 0.21871, 905.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.12s
- Epoch 050, ExpID 22244
Train - Loss (one batch): 0.07230
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07191, 0.07191, 0.26816, 0.21931, 901.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 161.34s
- Epoch 051, ExpID 22244
Train - Loss (one batch): 0.06896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07174, 0.07174, 0.26785, 0.21814, 880.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.18s
- Epoch 052, ExpID 22244
Train - Loss (one batch): 0.07258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07176, 0.07176, 0.26788, 0.21615, 859.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.49s
- Epoch 053, ExpID 22244
Train - Loss (one batch): 0.06891
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07172, 0.07172, 0.26781, 0.21832, 882.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.88s
- Epoch 054, ExpID 22244
Train - Loss (one batch): 0.07492
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07176, 0.07176, 0.26787, 0.21906, 893.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.04s
- Epoch 055, ExpID 22244
Train - Loss (one batch): 0.07914
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07179, 0.07179, 0.26794, 0.21755, 878.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 157.62s
- Epoch 056, ExpID 22244
Train - Loss (one batch): 0.07242
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07161, 0.07161, 0.26761, 0.21820, 882.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.28s
- Epoch 057, ExpID 22244
Train - Loss (one batch): 0.07299
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07196, 0.07196, 0.26826, 0.21775, 882.61%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.81s
- Epoch 058, ExpID 22244
Train - Loss (one batch): 0.06909
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07165, 0.07165, 0.26768, 0.21736, 874.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.78s
- Epoch 059, ExpID 22244
Train - Loss (one batch): 0.07114
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07168, 0.07168, 0.26773, 0.21885, 887.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.67s
- Epoch 060, ExpID 22244
Train - Loss (one batch): 0.07805
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07183, 0.07183, 0.26801, 0.22023, 906.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.46s
- Epoch 061, ExpID 22244
Train - Loss (one batch): 0.07095
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07161, 0.07161, 0.26761, 0.21737, 872.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.05s
- Epoch 062, ExpID 22244
Train - Loss (one batch): 0.06991
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07188, 0.07188, 0.26810, 0.21996, 903.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.06s
- Epoch 063, ExpID 22244
Train - Loss (one batch): 0.06869
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07165, 0.07165, 0.26768, 0.21747, 874.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.55s
- Epoch 064, ExpID 22244
Train - Loss (one batch): 0.07082
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07175, 0.07175, 0.26786, 0.21619, 859.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.82s
- Epoch 065, ExpID 22244
Train - Loss (one batch): 0.07960
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07175, 0.07175, 0.26787, 0.21714, 873.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.22s
- Epoch 066, ExpID 22244
Train - Loss (one batch): 0.06960
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21821, 887.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.33s
- Epoch 067, ExpID 22244
Train - Loss (one batch): 0.07726
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07175, 0.07175, 0.26785, 0.21721, 871.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.64s
- Epoch 068, ExpID 22244
Train - Loss (one batch): 0.07264
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07168, 0.07168, 0.26774, 0.21777, 879.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.07s
- Epoch 069, ExpID 22244
Train - Loss (one batch): 0.07323
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07173, 0.07173, 0.26783, 0.21779, 880.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.52s
- Epoch 070, ExpID 22244
Train - Loss (one batch): 0.07985
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07169, 0.07169, 0.26775, 0.21760, 876.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.99s
- Epoch 071, ExpID 22244
Train - Loss (one batch): 0.06565
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07200, 0.07200, 0.26832, 0.21729, 880.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 161.25s
- Epoch 072, ExpID 22244
Train - Loss (one batch): 0.06875
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07174, 0.07174, 0.26784, 0.21913, 892.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.82s
- Epoch 073, ExpID 22244
Train - Loss (one batch): 0.07126
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07163, 0.07163, 0.26765, 0.21816, 879.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.18s
- Epoch 074, ExpID 22244
Train - Loss (one batch): 0.08190
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07182, 0.07182, 0.26800, 0.22067, 909.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 161.73s
- Epoch 075, ExpID 22244
Train - Loss (one batch): 0.07293
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07171, 0.07171, 0.26778, 0.21876, 890.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.62s
- Epoch 076, ExpID 22244
Train - Loss (one batch): 0.07923
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07194, 0.07194, 0.26822, 0.22023, 905.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.11s
- Epoch 077, ExpID 22244
Train - Loss (one batch): 0.06913
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07172, 0.07172, 0.26780, 0.21763, 875.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.91s
- Epoch 078, ExpID 22244
Train - Loss (one batch): 0.07423
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07157, 0.07157, 0.26753, 0.21801, 880.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.63s
- Epoch 079, ExpID 22244
Train - Loss (one batch): 0.06755
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07164, 0.07164, 0.26765, 0.21827, 883.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.59s
- Epoch 080, ExpID 22244
Train - Loss (one batch): 0.07495
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07165, 0.07165, 0.26768, 0.21877, 887.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.99s
- Epoch 081, ExpID 22244
Train - Loss (one batch): 0.07062
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07188, 0.07188, 0.26810, 0.21784, 883.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.27s
- Epoch 082, ExpID 22244
Train - Loss (one batch): 0.06474
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21848, 887.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.42s
- Epoch 083, ExpID 22244
Train - Loss (one batch): 0.06644
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07184, 0.07184, 0.26803, 0.22004, 907.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.75s
- Epoch 084, ExpID 22244
Train - Loss (one batch): 0.07381
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07164, 0.07164, 0.26765, 0.21743, 874.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.12s
- Epoch 085, ExpID 22244
Train - Loss (one batch): 0.06837
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07158, 0.07158, 0.26754, 0.21776, 876.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.04s
- Epoch 086, ExpID 22244
Train - Loss (one batch): 0.07196
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07157, 0.07157, 0.26753, 0.21745, 869.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.09s
- Epoch 087, ExpID 22244
Train - Loss (one batch): 0.06639
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07159, 0.07159, 0.26757, 0.21809, 879.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.23s
- Epoch 088, ExpID 22244
Train - Loss (one batch): 0.07379
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07158, 0.07158, 0.26755, 0.21690, 867.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 161.20s
- Epoch 089, ExpID 22244
Train - Loss (one batch): 0.07599
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07162, 0.07162, 0.26762, 0.21761, 875.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.13s
- Epoch 090, ExpID 22244
Train - Loss (one batch): 0.07194
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07169, 0.07169, 0.26775, 0.21863, 890.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.39s
- Epoch 091, ExpID 22244
Train - Loss (one batch): 0.07592
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07165, 0.07165, 0.26767, 0.21805, 880.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.40s
- Epoch 092, ExpID 22244
Train - Loss (one batch): 0.07437
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07169, 0.07169, 0.26775, 0.21949, 897.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 157.88s
- Epoch 093, ExpID 22244
Train - Loss (one batch): 0.06807
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07160, 0.07160, 0.26758, 0.21742, 874.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 159.71s
- Epoch 094, ExpID 22244
Train - Loss (one batch): 0.07475
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07157, 0.07157, 0.26752, 0.21749, 872.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 158.36s
- Epoch 095, ExpID 22244
Train - Loss (one batch): 0.06745
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07153, 0.07153, 0.26745, 0.21741, 870.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.56s
- Epoch 096, ExpID 22244
Train - Loss (one batch): 0.07040
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21883, 890.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.19s
- Epoch 097, ExpID 22244
Train - Loss (one batch): 0.06239
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07167, 0.07167, 0.26771, 0.21872, 889.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.87s
- Epoch 098, ExpID 22244
Train - Loss (one batch): 0.07099
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07157, 0.07157, 0.26752, 0.21705, 868.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 162.48s
- Epoch 099, ExpID 22244
Train - Loss (one batch): 0.07441
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07171, 0.07171, 0.26778, 0.21853, 884.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07519, 0.07519, 0.27421, 0.21246, 764.78%
Time spent: 160.76s
True
Couldn't import umap
PID, device: 2373153 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=133, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7170561
n_train_batches: 440
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-20 07:49:13
run_iTransformer.py --history 24 --model iTransformer --dataset ushcn --gpu 2
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='ushcn', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='2', seq_len=205, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2524315, n_months=48, pred_window=1, ndim=5, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 49655
Train - Loss (one batch): 0.83211
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14820, 1.14820, 1.07154, 0.52918, -98.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 16.59s
- Epoch 001, ExpID 49655
Train - Loss (one batch): 0.55090
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16086, 1.16086, 1.07743, 0.54474, -116.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 14.86s
- Epoch 002, ExpID 49655
Train - Loss (one batch): 0.48451
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15851, 1.15851, 1.07634, 0.52615, -97.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 15.25s
- Epoch 003, ExpID 49655
Train - Loss (one batch): 2.06049
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16868, 1.16868, 1.08105, 0.54518, -101.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 14.44s
- Epoch 004, ExpID 49655
Train - Loss (one batch): 0.52008
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14504, 1.14504, 1.07006, 0.52319, -97.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.75642, 0.75642, 0.86973, 0.49328, -98.53%
Time spent: 16.92s
- Epoch 005, ExpID 49655
Train - Loss (one batch): 0.56293
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14254, 1.14254, 1.06890, 0.51815, -91.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.77s
- Epoch 006, ExpID 49655
Train - Loss (one batch): 0.40378
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16982, 1.16982, 1.08158, 0.50860, -61.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.74s
- Epoch 007, ExpID 49655
Train - Loss (one batch): 0.31913
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16987, 1.16987, 1.08160, 0.51330, -68.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.39s
- Epoch 008, ExpID 49655
Train - Loss (one batch): 0.53086
Val - Loss, MSE, RMSE, MAE, MAPE: 1.18134, 1.18134, 1.08689, 0.51994, -63.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.53s
- Epoch 009, ExpID 49655
Train - Loss (one batch): 1.22436
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17922, 1.17922, 1.08592, 0.53023, -73.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.60s
- Epoch 010, ExpID 49655
Train - Loss (one batch): 0.66053
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20998, 1.20998, 1.09999, 0.54426, -58.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.46s
- Epoch 011, ExpID 49655
Train - Loss (one batch): 0.58991
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20787, 1.20787, 1.09903, 0.51965, -36.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.44s
- Epoch 012, ExpID 49655
Train - Loss (one batch): 0.59762
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20677, 1.20677, 1.09853, 0.54416, -62.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.66s
- Epoch 013, ExpID 49655
Train - Loss (one batch): 0.69837
Val - Loss, MSE, RMSE, MAE, MAPE: 1.21114, 1.21114, 1.10052, 0.52662, -41.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.34s
- Epoch 014, ExpID 49655
Train - Loss (one batch): 1.76396
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20537, 1.20537, 1.09789, 0.55906, -81.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.24s
- Epoch 015, ExpID 49655
Train - Loss (one batch): 0.46792
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20246, 1.20246, 1.09657, 0.55506, -82.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 016, ExpID 49655
Train - Loss (one batch): 1.11103
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17864, 1.17864, 1.08565, 0.53979, -77.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 017, ExpID 49655
Train - Loss (one batch): 1.53562
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15223, 1.15223, 1.07342, 0.51908, -86.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 018, ExpID 49655
Train - Loss (one batch): 0.42909
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16337, 1.16337, 1.07860, 0.53218, -95.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.52s
- Epoch 019, ExpID 49655
Train - Loss (one batch): 0.39305
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15561, 1.15561, 1.07499, 0.53179, -96.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.93s
- Epoch 020, ExpID 49655
Train - Loss (one batch): 1.37649
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15371, 1.15371, 1.07411, 0.51712, -84.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.93s
- Epoch 021, ExpID 49655
Train - Loss (one batch): 0.58898
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15810, 1.15810, 1.07615, 0.51669, -81.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.96s
- Epoch 022, ExpID 49655
Train - Loss (one batch): 1.25931
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15278, 1.15278, 1.07367, 0.52859, -97.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.94s
- Epoch 023, ExpID 49655
Train - Loss (one batch): 0.41614
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15941, 1.15941, 1.07676, 0.53215, -93.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.92s
- Epoch 024, ExpID 49655
Train - Loss (one batch): 0.55478
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15909, 1.15909, 1.07661, 0.53205, -95.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.94s
- Epoch 025, ExpID 49655
Train - Loss (one batch): 0.94325
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15222, 1.15222, 1.07342, 0.51493, -83.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.94s
- Epoch 026, ExpID 49655
Train - Loss (one batch): 0.88726
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15395, 1.15395, 1.07422, 0.52660, -91.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.93s
- Epoch 027, ExpID 49655
Train - Loss (one batch): 0.61737
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15420, 1.15420, 1.07434, 0.51693, -82.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.90s
- Epoch 028, ExpID 49655
Train - Loss (one batch): 0.36144
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15327, 1.15327, 1.07390, 0.51765, -86.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.99s
- Epoch 029, ExpID 49655
Train - Loss (one batch): 0.47134
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15433, 1.15433, 1.07440, 0.52649, -92.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.88s
- Epoch 030, ExpID 49655
Train - Loss (one batch): 2.25681
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15513, 1.15513, 1.07477, 0.52216, -89.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.87s
- Epoch 031, ExpID 49655
Train - Loss (one batch): 0.65219
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15533, 1.15533, 1.07486, 0.52841, -96.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.40s
- Epoch 032, ExpID 49655
Train - Loss (one batch): 1.24601
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15210, 1.15210, 1.07336, 0.51934, -86.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.64s
- Epoch 033, ExpID 49655
Train - Loss (one batch): 1.01426
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15147, 1.15147, 1.07307, 0.51948, -86.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.92s
- Epoch 034, ExpID 49655
Train - Loss (one batch): 0.47623
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15154, 1.15154, 1.07310, 0.52374, -92.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.88s
- Epoch 035, ExpID 49655
Train - Loss (one batch): 0.43605
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15445, 1.15445, 1.07445, 0.51351, -78.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.55s
- Epoch 036, ExpID 49655
Train - Loss (one batch): 1.14697
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16063, 1.16063, 1.07733, 0.53801, -103.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.94s
- Epoch 037, ExpID 49655
Train - Loss (one batch): 0.77069
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15349, 1.15349, 1.07401, 0.52692, -93.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.92s
- Epoch 038, ExpID 49655
Train - Loss (one batch): 0.60469
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15486, 1.15486, 1.07465, 0.53026, -97.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.91s
- Epoch 039, ExpID 49655
Train - Loss (one batch): 0.55241
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15483, 1.15483, 1.07463, 0.52374, -89.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.68s
- Epoch 040, ExpID 49655
Train - Loss (one batch): 0.37808
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15375, 1.15375, 1.07413, 0.52652, -93.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.89s
- Epoch 041, ExpID 49655
Train - Loss (one batch): 0.36023
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15212, 1.15212, 1.07337, 0.51675, -85.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.46s
- Epoch 042, ExpID 49655
Train - Loss (one batch): 0.42918
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15246, 1.15246, 1.07353, 0.51818, -84.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.34s
- Epoch 043, ExpID 49655
Train - Loss (one batch): 0.64691
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15359, 1.15359, 1.07405, 0.51395, -81.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.91s
- Epoch 044, ExpID 49655
Train - Loss (one batch): 0.40142
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15263, 1.15263, 1.07360, 0.51302, -81.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.80s
- Epoch 045, ExpID 49655
Train - Loss (one batch): 0.60986
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15607, 1.15607, 1.07521, 0.52999, -96.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.91s
- Epoch 046, ExpID 49655
Train - Loss (one batch): 1.54490
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15184, 1.15184, 1.07324, 0.51924, -87.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.91s
- Epoch 047, ExpID 49655
Train - Loss (one batch): 0.33049
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17411, 1.17411, 1.08356, 0.55035, -108.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.89s
- Epoch 048, ExpID 49655
Train - Loss (one batch): 1.10149
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15209, 1.15209, 1.07335, 0.52456, -93.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 17.88s
- Epoch 049, ExpID 49655
Train - Loss (one batch): 0.42769
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15306, 1.15306, 1.07381, 0.52082, -89.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.72s
- Epoch 050, ExpID 49655
Train - Loss (one batch): 0.82024
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15133, 1.15133, 1.07300, 0.51736, -84.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.91s
- Epoch 051, ExpID 49655
Train - Loss (one batch): 0.34664
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16286, 1.16286, 1.07836, 0.54298, -107.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 052, ExpID 49655
Train - Loss (one batch): 0.56038
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15342, 1.15342, 1.07397, 0.52530, -93.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
- Epoch 053, ExpID 49655
Train - Loss (one batch): 0.70661
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16605, 1.16605, 1.07984, 0.52575, -87.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 054, ExpID 49655
Train - Loss (one batch): 0.47455
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15187, 1.15187, 1.07325, 0.52116, -89.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 055, ExpID 49655
Train - Loss (one batch): 2.15308
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15338, 1.15338, 1.07396, 0.52759, -93.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 056, ExpID 49655
Train - Loss (one batch): 0.65054
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15323, 1.15323, 1.07388, 0.52331, -90.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.93s
- Epoch 057, ExpID 49655
Train - Loss (one batch): 0.50470
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15736, 1.15736, 1.07581, 0.53014, -95.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 058, ExpID 49655
Train - Loss (one batch): 2.06229
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15495, 1.15495, 1.07468, 0.53265, -99.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
- Epoch 059, ExpID 49655
Train - Loss (one batch): 1.74771
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15389, 1.15389, 1.07419, 0.52923, -97.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.91s
- Epoch 060, ExpID 49655
Train - Loss (one batch): 0.99950
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15363, 1.15363, 1.07407, 0.52721, -95.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 061, ExpID 49655
Train - Loss (one batch): 0.94571
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15223, 1.15223, 1.07342, 0.52514, -93.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 062, ExpID 49655
Train - Loss (one batch): 0.82096
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15161, 1.15161, 1.07313, 0.51887, -88.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.91s
- Epoch 063, ExpID 49655
Train - Loss (one batch): 0.68655
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15441, 1.15441, 1.07443, 0.52920, -97.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 064, ExpID 49655
Train - Loss (one batch): 0.64846
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15650, 1.15650, 1.07541, 0.52563, -94.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.45s
- Epoch 065, ExpID 49655
Train - Loss (one batch): 0.55562
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15483, 1.15483, 1.07463, 0.53014, -96.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
- Epoch 066, ExpID 49655
Train - Loss (one batch): 0.79318
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15236, 1.15236, 1.07348, 0.51719, -85.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 067, ExpID 49655
Train - Loss (one batch): 0.50406
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15109, 1.15109, 1.07289, 0.52104, -89.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 068, ExpID 49655
Train - Loss (one batch): 0.68175
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15233, 1.15233, 1.07346, 0.51472, -83.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 069, ExpID 49655
Train - Loss (one batch): 0.95467
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15162, 1.15162, 1.07314, 0.52248, -90.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 070, ExpID 49655
Train - Loss (one batch): 0.37607
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15232, 1.15232, 1.07346, 0.51567, -84.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 071, ExpID 49655
Train - Loss (one batch): 0.42334
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15542, 1.15542, 1.07490, 0.52225, -90.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.02s
- Epoch 072, ExpID 49655
Train - Loss (one batch): 0.55590
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15316, 1.15316, 1.07385, 0.51252, -79.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 073, ExpID 49655
Train - Loss (one batch): 0.72268
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15172, 1.15172, 1.07318, 0.52163, -90.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 074, ExpID 49655
Train - Loss (one batch): 0.51229
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15151, 1.15151, 1.07309, 0.51920, -86.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 075, ExpID 49655
Train - Loss (one batch): 0.66453
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15287, 1.15287, 1.07372, 0.51404, -82.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 076, ExpID 49655
Train - Loss (one batch): 0.57236
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15077, 1.15077, 1.07274, 0.51549, -83.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 077, ExpID 49655
Train - Loss (one batch): 0.52558
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15821, 1.15821, 1.07620, 0.52257, -89.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 078, ExpID 49655
Train - Loss (one batch): 0.41785
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15581, 1.15581, 1.07509, 0.51915, -85.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 079, ExpID 49655
Train - Loss (one batch): 0.40002
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15460, 1.15460, 1.07453, 0.53159, -97.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.99s
- Epoch 080, ExpID 49655
Train - Loss (one batch): 0.32391
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15089, 1.15089, 1.07279, 0.51697, -85.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 081, ExpID 49655
Train - Loss (one batch): 0.56063
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15632, 1.15632, 1.07532, 0.52322, -90.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 082, ExpID 49655
Train - Loss (one batch): 1.62874
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15113, 1.15113, 1.07291, 0.52152, -89.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 083, ExpID 49655
Train - Loss (one batch): 0.44640
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15439, 1.15439, 1.07442, 0.52573, -92.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 084, ExpID 49655
Train - Loss (one batch): 1.87010
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15281, 1.15281, 1.07369, 0.51975, -88.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 085, ExpID 49655
Train - Loss (one batch): 1.74642
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15377, 1.15377, 1.07414, 0.51806, -85.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.99s
- Epoch 086, ExpID 49655
Train - Loss (one batch): 1.34664
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15207, 1.15207, 1.07334, 0.51506, -83.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 14.33s
- Epoch 087, ExpID 49655
Train - Loss (one batch): 0.37898
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15112, 1.15112, 1.07290, 0.51548, -85.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.97s
- Epoch 088, ExpID 49655
Train - Loss (one batch): 0.38694
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15173, 1.15173, 1.07319, 0.51724, -86.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.96s
- Epoch 089, ExpID 49655
Train - Loss (one batch): 1.30190
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15290, 1.15290, 1.07373, 0.52437, -93.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 090, ExpID 49655
Train - Loss (one batch): 0.49529
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15119, 1.15119, 1.07294, 0.52046, -89.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.94s
- Epoch 091, ExpID 49655
Train - Loss (one batch): 0.68505
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15312, 1.15312, 1.07384, 0.51565, -84.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.95s
- Epoch 092, ExpID 49655
Train - Loss (one batch): 0.52596
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15320, 1.15320, 1.07387, 0.51655, -83.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.98s
- Epoch 093, ExpID 49655
Train - Loss (one batch): 1.31327
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15201, 1.15201, 1.07332, 0.51862, -85.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 15.88s
- Epoch 094, ExpID 49655
Train - Loss (one batch): 1.48162
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15281, 1.15281, 1.07369, 0.52727, -93.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 13.36s
- Epoch 095, ExpID 49655
Train - Loss (one batch): 1.03094
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15692, 1.15692, 1.07560, 0.53546, -102.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 13.47s
- Epoch 096, ExpID 49655
Train - Loss (one batch): 0.27677
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15148, 1.15148, 1.07307, 0.52400, -91.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.91s
- Epoch 097, ExpID 49655
Train - Loss (one batch): 1.37166
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15311, 1.15311, 1.07383, 0.52808, -96.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
- Epoch 098, ExpID 49655
Train - Loss (one batch): 0.46769
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15275, 1.15275, 1.07366, 0.51409, -81.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
- Epoch 099, ExpID 49655
Train - Loss (one batch): 0.47322
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15290, 1.15290, 1.07373, 0.50902, -76.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 16.92s
True
Couldn't import umap
PID, device: 2524315 cuda:0
Total records: 1114
Dataset n_samples: 1114 668 223 223
Test record ids (first 20): [886, 101, 1120, 730, 291, 875, 958, 260, 128, 1043, 620, 88, 1005, 872, 617, 538, 508, 44, 273, 817]
Test record ids (last 20): [982, 275, 991, 997, 66, 801, 67, 942, 12, 361, 555, 710, 333, 1017, 851, 184, 882, 509, 726, 587]
data_max: tensor([37.2551, 36.6691, 38.2520,  3.0401,  2.9104], device='cuda:0')
data_min: tensor([-0.3057, -0.1280, -0.1738, -3.8821, -4.1854], device='cuda:0')
time_max: tensor(48., device='cuda:0')
Dataset n_samples after time split: 26736 16032 5352 5352
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=205, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7207425
n_train_batches: 501
