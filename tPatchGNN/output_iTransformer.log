nohup: ignoring input
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 15:08:35
run_iTransformer.py --history 24 --model iTransformer --dataset physionet
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=128, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=1942242, pred_window=24, ndim=41, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 75256
Train - Loss (one batch): 0.05889
Val - Loss, MSE, RMSE, MAE, MAPE: 0.05891, 0.05891, 0.24272, 0.17080, 776.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05753, 0.05753, 0.23985, 0.16830, 802.99%
Time spent: 15.30s
- Epoch 001, ExpID 75256
Train - Loss (one batch): 0.04930
Val - Loss, MSE, RMSE, MAE, MAPE: 0.05469, 0.05469, 0.23385, 0.17467, 1034.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 14.61s
- Epoch 002, ExpID 75256
Train - Loss (one batch): 0.06449
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06567, 0.06567, 0.25625, 0.19720, 1604.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.99s
- Epoch 003, ExpID 75256
Train - Loss (one batch): 0.06750
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06549, 0.06549, 0.25591, 0.19254, 1645.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.96s
- Epoch 004, ExpID 75256
Train - Loss (one batch): 0.06648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06594, 0.06594, 0.25678, 0.20056, 1789.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.82s
- Epoch 005, ExpID 75256
Train - Loss (one batch): 0.06880
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06571, 0.06571, 0.25633, 0.19374, 1623.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.70s
- Epoch 006, ExpID 75256
Train - Loss (one batch): 0.07031
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06534, 0.06534, 0.25561, 0.19483, 1746.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.75s
- Epoch 007, ExpID 75256
Train - Loss (one batch): 0.06909
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06758, 0.06758, 0.25996, 0.19695, 1747.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.01s
- Epoch 008, ExpID 75256
Train - Loss (one batch): 0.06600
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06654, 0.06654, 0.25796, 0.19718, 1744.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.91s
- Epoch 009, ExpID 75256
Train - Loss (one batch): 0.06595
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06677, 0.06677, 0.25839, 0.19821, 1845.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.92s
- Epoch 010, ExpID 75256
Train - Loss (one batch): 0.06434
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06630, 0.06630, 0.25748, 0.19714, 1781.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.95s
- Epoch 011, ExpID 75256
Train - Loss (one batch): 0.06706
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06810, 0.06810, 0.26095, 0.20130, 1997.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.12s
- Epoch 012, ExpID 75256
Train - Loss (one batch): 0.06955
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06795, 0.06795, 0.26067, 0.20153, 2017.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.88s
- Epoch 013, ExpID 75256
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06791, 0.06791, 0.26060, 0.20198, 2043.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.87s
- Epoch 014, ExpID 75256
Train - Loss (one batch): 0.06750
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06796, 0.06796, 0.26069, 0.20304, 2093.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.87s
- Epoch 015, ExpID 75256
Train - Loss (one batch): 0.06659
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06664, 0.06664, 0.25815, 0.19788, 1826.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.02s
- Epoch 016, ExpID 75256
Train - Loss (one batch): 0.06832
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26062, 0.20253, 2069.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.26s
- Epoch 017, ExpID 75256
Train - Loss (one batch): 0.06533
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06783, 0.06783, 0.26044, 0.20292, 2083.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.22s
- Epoch 018, ExpID 75256
Train - Loss (one batch): 0.06572
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06783, 0.06783, 0.26045, 0.19877, 1832.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.09s
- Epoch 019, ExpID 75256
Train - Loss (one batch): 0.06614
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06802, 0.06802, 0.26080, 0.20209, 2045.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.99s
- Epoch 020, ExpID 75256
Train - Loss (one batch): 0.06642
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26061, 0.20310, 2104.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.01s
- Epoch 021, ExpID 75256
Train - Loss (one batch): 0.06484
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26061, 0.20233, 2060.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.99s
- Epoch 022, ExpID 75256
Train - Loss (one batch): 0.06988
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06804, 0.06804, 0.26084, 0.20189, 2025.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.01s
- Epoch 023, ExpID 75256
Train - Loss (one batch): 0.07003
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06822, 0.06822, 0.26120, 0.20060, 1947.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.05s
- Epoch 024, ExpID 75256
Train - Loss (one batch): 0.07234
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06796, 0.06796, 0.26070, 0.20317, 2098.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.97s
- Epoch 025, ExpID 75256
Train - Loss (one batch): 0.06567
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06803, 0.06803, 0.26083, 0.20364, 2115.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.08s
- Epoch 026, ExpID 75256
Train - Loss (one batch): 0.06979
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06796, 0.06796, 0.26068, 0.20277, 2080.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.80s
- Epoch 027, ExpID 75256
Train - Loss (one batch): 0.06808
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26062, 0.20183, 2036.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.98s
- Epoch 028, ExpID 75256
Train - Loss (one batch): 0.06918
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06794, 0.06794, 0.26066, 0.20273, 2077.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.83s
- Epoch 029, ExpID 75256
Train - Loss (one batch): 0.06411
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06819, 0.06819, 0.26113, 0.19969, 1903.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.81s
- Epoch 030, ExpID 75256
Train - Loss (one batch): 0.07028
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06793, 0.06793, 0.26064, 0.20165, 2026.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.71s
- Epoch 031, ExpID 75256
Train - Loss (one batch): 0.06664
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06819, 0.06819, 0.26113, 0.20511, 2181.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.88s
- Epoch 032, ExpID 75256
Train - Loss (one batch): 0.06833
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06793, 0.06793, 0.26063, 0.20165, 2025.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.91s
- Epoch 033, ExpID 75256
Train - Loss (one batch): 0.06997
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06797, 0.06797, 0.26072, 0.20103, 1990.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.79s
- Epoch 034, ExpID 75256
Train - Loss (one batch): 0.06703
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06796, 0.06796, 0.26069, 0.20137, 2009.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.82s
- Epoch 035, ExpID 75256
Train - Loss (one batch): 0.06579
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06799, 0.06799, 0.26075, 0.20223, 2047.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.80s
- Epoch 036, ExpID 75256
Train - Loss (one batch): 0.06728
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06791, 0.06791, 0.26059, 0.20074, 1978.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.98s
- Epoch 037, ExpID 75256
Train - Loss (one batch): 0.07054
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26061, 0.20232, 2060.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.68s
- Epoch 038, ExpID 75256
Train - Loss (one batch): 0.06898
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06794, 0.06794, 0.26065, 0.20142, 2014.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.12s
- Epoch 039, ExpID 75256
Train - Loss (one batch): 0.06881
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06793, 0.06793, 0.26063, 0.20169, 2028.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.07s
- Epoch 040, ExpID 75256
Train - Loss (one batch): 0.06739
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06805, 0.06805, 0.26086, 0.20044, 1957.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.97s
- Epoch 041, ExpID 75256
Train - Loss (one batch): 0.07152
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06793, 0.06793, 0.26064, 0.20135, 2011.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.01s
- Epoch 042, ExpID 75256
Train - Loss (one batch): 0.06904
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26062, 0.20122, 2005.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.20s
- Epoch 043, ExpID 75256
Train - Loss (one batch): 0.06836
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06800, 0.06800, 0.26077, 0.20067, 1972.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.71s
- Epoch 044, ExpID 75256
Train - Loss (one batch): 0.06728
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26056, 0.20178, 2035.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.78s
- Epoch 045, ExpID 75256
Train - Loss (one batch): 0.06799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06795, 0.06795, 0.26066, 0.20245, 2065.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.93s
- Epoch 046, ExpID 75256
Train - Loss (one batch): 0.07278
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06790, 0.06790, 0.26057, 0.20106, 1999.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.03s
- Epoch 047, ExpID 75256
Train - Loss (one batch): 0.06941
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06797, 0.06797, 0.26071, 0.20074, 1977.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.78s
- Epoch 048, ExpID 75256
Train - Loss (one batch): 0.06872
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20154, 2026.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.11s
- Epoch 049, ExpID 75256
Train - Loss (one batch): 0.07162
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06797, 0.06797, 0.26071, 0.20086, 1984.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.04s
- Epoch 050, ExpID 75256
Train - Loss (one batch): 0.06723
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26055, 0.20210, 2052.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.13s
- Epoch 051, ExpID 75256
Train - Loss (one batch): 0.06246
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20239, 2068.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.20s
- Epoch 052, ExpID 75256
Train - Loss (one batch): 0.06781
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26055, 0.20127, 2010.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.18s
- Epoch 053, ExpID 75256
Train - Loss (one batch): 0.06977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26050, 0.20255, 2076.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.23s
- Epoch 054, ExpID 75256
Train - Loss (one batch): 0.07042
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26047, 0.20200, 2049.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.05s
- Epoch 055, ExpID 75256
Train - Loss (one batch): 0.07145
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26051, 0.20190, 2042.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.98s
- Epoch 056, ExpID 75256
Train - Loss (one batch): 0.06885
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06788, 0.06788, 0.26053, 0.20105, 1999.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.03s
- Epoch 057, ExpID 75256
Train - Loss (one batch): 0.06937
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06788, 0.06788, 0.26054, 0.20127, 2010.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.02s
- Epoch 058, ExpID 75256
Train - Loss (one batch): 0.06671
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26045, 0.20173, 2036.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.97s
- Epoch 059, ExpID 75256
Train - Loss (one batch): 0.06706
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26051, 0.20222, 2058.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.03s
- Epoch 060, ExpID 75256
Train - Loss (one batch): 0.06590
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06783, 0.06783, 0.26045, 0.20179, 2039.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.64s
- Epoch 061, ExpID 75256
Train - Loss (one batch): 0.06578
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26046, 0.20159, 2029.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.01s
- Epoch 062, ExpID 75256
Train - Loss (one batch): 0.07015
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26049, 0.20173, 2036.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.05s
- Epoch 063, ExpID 75256
Train - Loss (one batch): 0.06771
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26047, 0.20212, 2055.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.04s
- Epoch 064, ExpID 75256
Train - Loss (one batch): 0.06799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06800, 0.06800, 0.26076, 0.20271, 2073.61%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.02s
- Epoch 065, ExpID 75256
Train - Loss (one batch): 0.06816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26047, 0.20147, 2022.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.95s
- Epoch 066, ExpID 75256
Train - Loss (one batch): 0.06536
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20155, 2026.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.92s
- Epoch 067, ExpID 75256
Train - Loss (one batch): 0.06638
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26046, 0.20142, 2020.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.92s
- Epoch 068, ExpID 75256
Train - Loss (one batch): 0.06294
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26050, 0.20277, 2086.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.79s
- Epoch 069, ExpID 75256
Train - Loss (one batch): 0.07180
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26047, 0.20188, 2043.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.72s
- Epoch 070, ExpID 75256
Train - Loss (one batch): 0.06976
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26050, 0.20120, 2008.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.62s
- Epoch 071, ExpID 75256
Train - Loss (one batch): 0.06890
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26057, 0.20234, 2062.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.77s
- Epoch 072, ExpID 75256
Train - Loss (one batch): 0.06735
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26061, 0.20069, 1979.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.59s
- Epoch 073, ExpID 75256
Train - Loss (one batch): 0.06779
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26049, 0.20140, 2018.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.52s
- Epoch 074, ExpID 75256
Train - Loss (one batch): 0.06496
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06790, 0.06790, 0.26058, 0.20227, 2058.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.52s
- Epoch 075, ExpID 75256
Train - Loss (one batch): 0.06878
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20257, 2077.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.60s
- Epoch 076, ExpID 75256
Train - Loss (one batch): 0.06727
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06787, 0.06787, 0.26052, 0.20124, 2010.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.63s
- Epoch 077, ExpID 75256
Train - Loss (one batch): 0.06616
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26050, 0.20171, 2034.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.60s
- Epoch 078, ExpID 75256
Train - Loss (one batch): 0.06530
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26046, 0.20182, 2041.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.77s
- Epoch 079, ExpID 75256
Train - Loss (one batch): 0.06752
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06788, 0.06788, 0.26054, 0.20099, 1997.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.68s
- Epoch 080, ExpID 75256
Train - Loss (one batch): 0.07066
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20135, 2017.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.72s
- Epoch 081, ExpID 75256
Train - Loss (one batch): 0.07103
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06791, 0.06791, 0.26059, 0.20096, 1993.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.94s
- Epoch 082, ExpID 75256
Train - Loss (one batch): 0.07280
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06787, 0.06787, 0.26053, 0.20142, 2019.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.88s
- Epoch 083, ExpID 75256
Train - Loss (one batch): 0.07142
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26048, 0.20158, 2029.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.16s
- Epoch 084, ExpID 75256
Train - Loss (one batch): 0.06822
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06790, 0.06790, 0.26057, 0.20113, 2002.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.25s
- Epoch 085, ExpID 75256
Train - Loss (one batch): 0.06854
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26056, 0.20141, 2017.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.82s
- Epoch 086, ExpID 75256
Train - Loss (one batch): 0.07342
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06790, 0.06790, 0.26057, 0.20180, 2036.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.74s
- Epoch 087, ExpID 75256
Train - Loss (one batch): 0.07154
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06787, 0.06787, 0.26052, 0.20169, 2032.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.95s
- Epoch 088, ExpID 75256
Train - Loss (one batch): 0.06998
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26049, 0.20123, 2010.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.98s
- Epoch 089, ExpID 75256
Train - Loss (one batch): 0.06881
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06788, 0.06788, 0.26054, 0.20113, 2003.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.00s
- Epoch 090, ExpID 75256
Train - Loss (one batch): 0.06461
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06785, 0.06785, 0.26047, 0.20149, 2023.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.99s
- Epoch 091, ExpID 75256
Train - Loss (one batch): 0.06838
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26051, 0.20249, 2072.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.07s
- Epoch 092, ExpID 75256
Train - Loss (one batch): 0.07208
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06787, 0.06787, 0.26052, 0.20121, 2008.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.03s
- Epoch 093, ExpID 75256
Train - Loss (one batch): 0.07158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26049, 0.20141, 2018.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.36s
- Epoch 094, ExpID 75256
Train - Loss (one batch): 0.06536
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06784, 0.06784, 0.26046, 0.20221, 2059.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 15.04s
- Epoch 095, ExpID 75256
Train - Loss (one batch): 0.06749
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26062, 0.20172, 2030.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 15.14s
- Epoch 096, ExpID 75256
Train - Loss (one batch): 0.06596
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06788, 0.06788, 0.26053, 0.20178, 2036.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 15.41s
- Epoch 097, ExpID 75256
Train - Loss (one batch): 0.06677
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06792, 0.06792, 0.26062, 0.20094, 1992.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 15.06s
- Epoch 098, ExpID 75256
Train - Loss (one batch): 0.06978
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26057, 0.20122, 2008.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 13.46s
- Epoch 099, ExpID 75256
Train - Loss (one batch): 0.07442
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06789, 0.06789, 0.26056, 0.20135, 2014.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.05373, 0.05373, 0.23179, 0.17283, 1085.06%
Time spent: 12.76s
True
Couldn't import umap
PID, device: 1942242 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=128, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7168001
n_train_batches: 225
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 15:31:09
run_iTransformer.py --history 36 --model iTransformer --dataset physionet
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=175, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=1955723, pred_window=12, ndim=41, patch_layer=2, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 6130
Train - Loss (one batch): 0.06178
Val - Loss, MSE, RMSE, MAE, MAPE: 0.05632, 0.05632, 0.23733, 0.17677, 1194.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 11.81s
- Epoch 001, ExpID 6130
Train - Loss (one batch): 0.06608
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06615, 0.06615, 0.25720, 0.19722, 1968.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.74s
- Epoch 002, ExpID 6130
Train - Loss (one batch): 0.07109
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06439, 0.06439, 0.25376, 0.19452, 1836.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.06s
- Epoch 003, ExpID 6130
Train - Loss (one batch): 0.06078
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06404, 0.06404, 0.25307, 0.19750, 1897.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.20s
- Epoch 004, ExpID 6130
Train - Loss (one batch): 0.06460
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06567, 0.06567, 0.25627, 0.20304, 2087.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.38s
- Epoch 005, ExpID 6130
Train - Loss (one batch): 0.06280
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06474, 0.06474, 0.25444, 0.19383, 1694.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.12s
- Epoch 006, ExpID 6130
Train - Loss (one batch): 0.06958
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06514, 0.06514, 0.25523, 0.20051, 2028.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.28s
- Epoch 007, ExpID 6130
Train - Loss (one batch): 0.06723
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06495, 0.06495, 0.25486, 0.19568, 1832.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.71s
- Epoch 008, ExpID 6130
Train - Loss (one batch): 0.06212
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06510, 0.06510, 0.25515, 0.19420, 1798.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.58s
- Epoch 009, ExpID 6130
Train - Loss (one batch): 0.06473
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06521, 0.06521, 0.25536, 0.19827, 1966.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.92s
- Epoch 010, ExpID 6130
Train - Loss (one batch): 0.07069
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06622, 0.06622, 0.25733, 0.20315, 2083.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.51s
- Epoch 011, ExpID 6130
Train - Loss (one batch): 0.06676
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06543, 0.06543, 0.25579, 0.19894, 2006.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.68s
- Epoch 012, ExpID 6130
Train - Loss (one batch): 0.07220
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06533, 0.06533, 0.25561, 0.19828, 1955.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.75s
- Epoch 013, ExpID 6130
Train - Loss (one batch): 0.06813
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06528, 0.06528, 0.25549, 0.19533, 1843.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.81s
- Epoch 014, ExpID 6130
Train - Loss (one batch): 0.06235
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06525, 0.06525, 0.25545, 0.19651, 1844.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.70s
- Epoch 015, ExpID 6130
Train - Loss (one batch): 0.06898
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06747, 0.06747, 0.25974, 0.20453, 2321.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.56s
- Epoch 016, ExpID 6130
Train - Loss (one batch): 0.06699
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06750, 0.06750, 0.25980, 0.20468, 2326.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.63s
- Epoch 017, ExpID 6130
Train - Loss (one batch): 0.06884
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06718, 0.06718, 0.25918, 0.20286, 2255.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.54s
- Epoch 018, ExpID 6130
Train - Loss (one batch): 0.07004
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25889, 0.20157, 2200.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.33s
- Epoch 019, ExpID 6130
Train - Loss (one batch): 0.07290
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06714, 0.06714, 0.25911, 0.20259, 2244.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.41s
- Epoch 020, ExpID 6130
Train - Loss (one batch): 0.07155
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06709, 0.06709, 0.25901, 0.20216, 2226.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.49s
- Epoch 021, ExpID 6130
Train - Loss (one batch): 0.07877
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06700, 0.06700, 0.25884, 0.19892, 2064.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.75s
- Epoch 022, ExpID 6130
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06711, 0.06711, 0.25905, 0.19819, 2017.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.23s
- Epoch 023, ExpID 6130
Train - Loss (one batch): 0.07419
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06718, 0.06718, 0.25919, 0.20285, 2255.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.25s
- Epoch 024, ExpID 6130
Train - Loss (one batch): 0.06505
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.19979, 2114.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.52s
- Epoch 025, ExpID 6130
Train - Loss (one batch): 0.06837
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06786, 0.06786, 0.26051, 0.20629, 2384.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.54s
- Epoch 026, ExpID 6130
Train - Loss (one batch): 0.06748
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06728, 0.06728, 0.25939, 0.20353, 2282.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.87s
- Epoch 027, ExpID 6130
Train - Loss (one batch): 0.06469
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25891, 0.20172, 2207.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.52s
- Epoch 028, ExpID 6130
Train - Loss (one batch): 0.07215
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06721, 0.06721, 0.25926, 0.20312, 2266.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.59s
- Epoch 029, ExpID 6130
Train - Loss (one batch): 0.06355
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.19933, 2087.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.41s
- Epoch 030, ExpID 6130
Train - Loss (one batch): 0.06764
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.19879, 2056.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.96s
- Epoch 031, ExpID 6130
Train - Loss (one batch): 0.06574
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25877, 0.20074, 2162.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.39s
- Epoch 032, ExpID 6130
Train - Loss (one batch): 0.06870
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25877, 0.19946, 2095.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.56s
- Epoch 033, ExpID 6130
Train - Loss (one batch): 0.07272
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.19879, 2056.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.26s
- Epoch 034, ExpID 6130
Train - Loss (one batch): 0.06998
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25889, 0.20157, 2200.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.84s
- Epoch 035, ExpID 6130
Train - Loss (one batch): 0.06546
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19961, 2103.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.82s
- Epoch 036, ExpID 6130
Train - Loss (one batch): 0.06977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20036, 2143.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.74s
- Epoch 037, ExpID 6130
Train - Loss (one batch): 0.07140
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06758, 0.06758, 0.25997, 0.20507, 2340.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.73s
- Epoch 038, ExpID 6130
Train - Loss (one batch): 0.06590
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06703, 0.06703, 0.25890, 0.20163, 2202.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.77s
- Epoch 039, ExpID 6130
Train - Loss (one batch): 0.07359
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.19911, 2075.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.73s
- Epoch 040, ExpID 6130
Train - Loss (one batch): 0.06896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20108, 2178.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.66s
- Epoch 041, ExpID 6130
Train - Loss (one batch): 0.06883
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20065, 2157.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.91s
- Epoch 042, ExpID 6130
Train - Loss (one batch): 0.06573
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19986, 2118.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.48s
- Epoch 043, ExpID 6130
Train - Loss (one batch): 0.06978
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06711, 0.06711, 0.25905, 0.20238, 2235.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.95s
- Epoch 044, ExpID 6130
Train - Loss (one batch): 0.07117
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20017, 2133.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.50s
- Epoch 045, ExpID 6130
Train - Loss (one batch): 0.06499
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20022, 2136.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.98s
- Epoch 046, ExpID 6130
Train - Loss (one batch): 0.07159
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25883, 0.20138, 2196.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.72s
- Epoch 047, ExpID 6130
Train - Loss (one batch): 0.06931
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20017, 2133.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.88s
- Epoch 048, ExpID 6130
Train - Loss (one batch): 0.07037
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25885, 0.20139, 2192.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.82s
- Epoch 049, ExpID 6130
Train - Loss (one batch): 0.06570
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25880, 0.20101, 2175.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.81s
- Epoch 050, ExpID 6130
Train - Loss (one batch): 0.06682
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20093, 2171.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.29s
- Epoch 051, ExpID 6130
Train - Loss (one batch): 0.07225
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19955, 2101.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.78s
- Epoch 052, ExpID 6130
Train - Loss (one batch): 0.07278
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19953, 2100.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.95s
- Epoch 053, ExpID 6130
Train - Loss (one batch): 0.06971
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25876, 0.20052, 2151.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.90s
- Epoch 054, ExpID 6130
Train - Loss (one batch): 0.06642
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.20149, 2196.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.66s
- Epoch 055, ExpID 6130
Train - Loss (one batch): 0.06958
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.19904, 2072.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.42s
- Epoch 056, ExpID 6130
Train - Loss (one batch): 0.07274
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25883, 0.20112, 2179.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.05s
- Epoch 057, ExpID 6130
Train - Loss (one batch): 0.07353
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25892, 0.19856, 2043.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.52s
- Epoch 058, ExpID 6130
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20095, 2172.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.59s
- Epoch 059, ExpID 6130
Train - Loss (one batch): 0.06584
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06712, 0.06712, 0.25908, 0.20252, 2242.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.71s
- Epoch 060, ExpID 6130
Train - Loss (one batch): 0.06903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19983, 2117.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.85s
- Epoch 061, ExpID 6130
Train - Loss (one batch): 0.06846
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19984, 2118.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.67s
- Epoch 062, ExpID 6130
Train - Loss (one batch): 0.06314
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20080, 2165.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.91s
- Epoch 063, ExpID 6130
Train - Loss (one batch): 0.06631
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20032, 2141.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.87s
- Epoch 064, ExpID 6130
Train - Loss (one batch): 0.06976
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25878, 0.19967, 2108.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.14s
- Epoch 065, ExpID 6130
Train - Loss (one batch): 0.07624
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20065, 2157.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.78s
- Epoch 066, ExpID 6130
Train - Loss (one batch): 0.06663
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25887, 0.20153, 2199.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.80s
- Epoch 067, ExpID 6130
Train - Loss (one batch): 0.07229
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20054, 2153.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.79s
- Epoch 068, ExpID 6130
Train - Loss (one batch): 0.06516
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.20004, 2128.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.88s
- Epoch 069, ExpID 6130
Train - Loss (one batch): 0.06525
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06703, 0.06703, 0.25889, 0.20153, 2198.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.39s
- Epoch 070, ExpID 6130
Train - Loss (one batch): 0.07212
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20116, 2182.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.45s
- Epoch 071, ExpID 6130
Train - Loss (one batch): 0.06630
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25886, 0.20143, 2194.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.92s
- Epoch 072, ExpID 6130
Train - Loss (one batch): 0.06857
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25874, 0.20050, 2152.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.50s
- Epoch 073, ExpID 6130
Train - Loss (one batch): 0.07441
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.19966, 2107.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.50s
- Epoch 074, ExpID 6130
Train - Loss (one batch): 0.06899
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25881, 0.20094, 2170.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.50s
- Epoch 075, ExpID 6130
Train - Loss (one batch): 0.06431
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20039, 2144.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.09s
- Epoch 076, ExpID 6130
Train - Loss (one batch): 0.06629
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25872, 0.20031, 2143.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.66s
- Epoch 077, ExpID 6130
Train - Loss (one batch): 0.07220
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06710, 0.06710, 0.25903, 0.20226, 2230.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 10.18s
- Epoch 078, ExpID 6130
Train - Loss (one batch): 0.06394
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06704, 0.06704, 0.25893, 0.20188, 2216.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.75s
- Epoch 079, ExpID 6130
Train - Loss (one batch): 0.07025
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06706, 0.06706, 0.25897, 0.20164, 2201.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.69s
- Epoch 080, ExpID 6130
Train - Loss (one batch): 0.07025
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06698, 0.06698, 0.25880, 0.20106, 2179.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.64s
- Epoch 081, ExpID 6130
Train - Loss (one batch): 0.06678
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06722, 0.06722, 0.25926, 0.20304, 2262.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.71s
- Epoch 082, ExpID 6130
Train - Loss (one batch): 0.06500
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06699, 0.06699, 0.25882, 0.19902, 2070.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.55s
- Epoch 083, ExpID 6130
Train - Loss (one batch): 0.06794
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25878, 0.19929, 2087.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.76s
- Epoch 084, ExpID 6130
Train - Loss (one batch): 0.06992
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.20079, 2165.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.67s
- Epoch 085, ExpID 6130
Train - Loss (one batch): 0.07332
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06702, 0.06702, 0.25888, 0.20148, 2195.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.81s
- Epoch 086, ExpID 6130
Train - Loss (one batch): 0.06940
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25874, 0.20045, 2149.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.86s
- Epoch 087, ExpID 6130
Train - Loss (one batch): 0.06816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06710, 0.06710, 0.25903, 0.20229, 2233.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.50s
- Epoch 088, ExpID 6130
Train - Loss (one batch): 0.06790
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06693, 0.06693, 0.25871, 0.19969, 2111.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.98s
- Epoch 089, ExpID 6130
Train - Loss (one batch): 0.06924
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19981, 2116.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.67s
- Epoch 090, ExpID 6130
Train - Loss (one batch): 0.07167
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20049, 2150.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.79s
- Epoch 091, ExpID 6130
Train - Loss (one batch): 0.07236
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.20000, 2126.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.81s
- Epoch 092, ExpID 6130
Train - Loss (one batch): 0.07355
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25887, 0.20123, 2182.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.62s
- Epoch 093, ExpID 6130
Train - Loss (one batch): 0.06952
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.19956, 2102.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.84s
- Epoch 094, ExpID 6130
Train - Loss (one batch): 0.06954
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06701, 0.06701, 0.25885, 0.20144, 2195.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.54s
- Epoch 095, ExpID 6130
Train - Loss (one batch): 0.05417
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06695, 0.06695, 0.25875, 0.20051, 2151.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.28s
- Epoch 096, ExpID 6130
Train - Loss (one batch): 0.07295
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06696, 0.06696, 0.25876, 0.19939, 2093.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.86s
- Epoch 097, ExpID 6130
Train - Loss (one batch): 0.06827
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25874, 0.19967, 2108.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.36s
- Epoch 098, ExpID 6130
Train - Loss (one batch): 0.07155
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06694, 0.06694, 0.25873, 0.19956, 2104.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.21s
- Epoch 099, ExpID 6130
Train - Loss (one batch): 0.06859
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06697, 0.06697, 0.25879, 0.20012, 2128.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05572, 0.05572, 0.23605, 0.17490, 1140.19%
Time spent: 9.19s
True
Couldn't import umap
PID, device: 1955723 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=175, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7192065
n_train_batches: 225
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 15:48:06
run_iTransformer.py --history 12 --model iTransformer --dataset physionet
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=83, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=1965886, pred_window=36, ndim=41, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 81097
Train - Loss (one batch): 0.05369
Val - Loss, MSE, RMSE, MAE, MAPE: 0.05880, 0.05880, 0.24249, 0.17889, 1149.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 20.01s
- Epoch 001, ExpID 81097
Train - Loss (one batch): 0.06510
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06257, 0.06257, 0.25014, 0.19167, 1451.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.39s
- Epoch 002, ExpID 81097
Train - Loss (one batch): 0.06894
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06673, 0.06673, 0.25832, 0.20174, 1979.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.25s
- Epoch 003, ExpID 81097
Train - Loss (one batch): 0.06858
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06620, 0.06620, 0.25730, 0.19676, 1836.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.17s
- Epoch 004, ExpID 81097
Train - Loss (one batch): 0.06491
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06634, 0.06634, 0.25756, 0.19671, 1812.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.41s
- Epoch 005, ExpID 81097
Train - Loss (one batch): 0.06192
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06611, 0.06611, 0.25712, 0.20081, 1979.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.33s
- Epoch 006, ExpID 81097
Train - Loss (one batch): 0.06888
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06689, 0.06689, 0.25863, 0.20563, 2046.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.40s
- Epoch 007, ExpID 81097
Train - Loss (one batch): 0.06477
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06674, 0.06674, 0.25833, 0.20485, 2025.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.20s
- Epoch 008, ExpID 81097
Train - Loss (one batch): 0.06086
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06598, 0.06598, 0.25686, 0.19837, 1820.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.42s
- Epoch 009, ExpID 81097
Train - Loss (one batch): 0.06606
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06613, 0.06613, 0.25716, 0.19912, 1896.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.62s
- Epoch 010, ExpID 81097
Train - Loss (one batch): 0.06846
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06602, 0.06602, 0.25695, 0.19788, 1843.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 17.26s
- Epoch 011, ExpID 81097
Train - Loss (one batch): 0.06633
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06765, 0.06765, 0.26010, 0.20083, 2070.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.27s
- Epoch 012, ExpID 81097
Train - Loss (one batch): 0.06593
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06623, 0.06623, 0.25736, 0.19980, 1918.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.52s
- Epoch 013, ExpID 81097
Train - Loss (one batch): 0.06520
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06620, 0.06620, 0.25729, 0.19669, 1774.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.18s
- Epoch 014, ExpID 81097
Train - Loss (one batch): 0.06535
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06631, 0.06631, 0.25751, 0.20054, 1909.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.39s
- Epoch 015, ExpID 81097
Train - Loss (one batch): 0.06998
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06623, 0.06623, 0.25734, 0.19721, 1759.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.53s
- Epoch 016, ExpID 81097
Train - Loss (one batch): 0.06611
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06620, 0.06620, 0.25730, 0.19887, 1901.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.65s
- Epoch 017, ExpID 81097
Train - Loss (one batch): 0.06587
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06746, 0.06746, 0.25973, 0.20542, 2212.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.37s
- Epoch 018, ExpID 81097
Train - Loss (one batch): 0.06538
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06647, 0.06647, 0.25782, 0.19707, 1757.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.40s
- Epoch 019, ExpID 81097
Train - Loss (one batch): 0.07158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06778, 0.06778, 0.26034, 0.20095, 2073.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.44s
- Epoch 020, ExpID 81097
Train - Loss (one batch): 0.06475
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06649, 0.06649, 0.25785, 0.19680, 1750.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.34s
- Epoch 021, ExpID 81097
Train - Loss (one batch): 0.06933
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06660, 0.06660, 0.25807, 0.19607, 1735.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.38s
- Epoch 022, ExpID 81097
Train - Loss (one batch): 0.06701
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06650, 0.06650, 0.25787, 0.19739, 1836.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 17.13s
- Epoch 023, ExpID 81097
Train - Loss (one batch): 0.06851
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06665, 0.06665, 0.25817, 0.19679, 1805.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.44s
- Epoch 024, ExpID 81097
Train - Loss (one batch): 0.06257
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06654, 0.06654, 0.25796, 0.19659, 1754.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.38s
- Epoch 025, ExpID 81097
Train - Loss (one batch): 0.06649
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06773, 0.06773, 0.26025, 0.20159, 2111.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 026, ExpID 81097
Train - Loss (one batch): 0.06714
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26024, 0.20182, 2124.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.31s
- Epoch 027, ExpID 81097
Train - Loss (one batch): 0.07061
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06777, 0.06777, 0.26033, 0.20192, 2125.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.16s
- Epoch 028, ExpID 81097
Train - Loss (one batch): 0.06665
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20179, 2123.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.24s
- Epoch 029, ExpID 81097
Train - Loss (one batch): 0.06924
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20217, 2143.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.45s
- Epoch 030, ExpID 81097
Train - Loss (one batch): 0.06603
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20141, 2102.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.74s
- Epoch 031, ExpID 81097
Train - Loss (one batch): 0.06763
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06774, 0.06774, 0.26026, 0.20084, 2072.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.41s
- Epoch 032, ExpID 81097
Train - Loss (one batch): 0.06634
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26020, 0.20168, 2116.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.53s
- Epoch 033, ExpID 81097
Train - Loss (one batch): 0.06930
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26023, 0.20111, 2084.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.47s
- Epoch 034, ExpID 81097
Train - Loss (one batch): 0.06684
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20188, 2126.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 035, ExpID 81097
Train - Loss (one batch): 0.06826
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06773, 0.06773, 0.26024, 0.20080, 2069.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.23s
- Epoch 036, ExpID 81097
Train - Loss (one batch): 0.06901
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06776, 0.06776, 0.26031, 0.20187, 2120.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.10s
- Epoch 037, ExpID 81097
Train - Loss (one batch): 0.06870
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06774, 0.06774, 0.26026, 0.20139, 2097.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.42s
- Epoch 038, ExpID 81097
Train - Loss (one batch): 0.06693
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26023, 0.20242, 2155.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.46s
- Epoch 039, ExpID 81097
Train - Loss (one batch): 0.06316
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06780, 0.06780, 0.26038, 0.20058, 2050.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.48s
- Epoch 040, ExpID 81097
Train - Loss (one batch): 0.06729
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20140, 2101.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.25s
- Epoch 041, ExpID 81097
Train - Loss (one batch): 0.06679
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06779, 0.06779, 0.26037, 0.20171, 2112.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.49s
- Epoch 042, ExpID 81097
Train - Loss (one batch): 0.06551
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06775, 0.06775, 0.26029, 0.20079, 2063.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.42s
- Epoch 043, ExpID 81097
Train - Loss (one batch): 0.06258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26017, 0.20139, 2102.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.28s
- Epoch 044, ExpID 81097
Train - Loss (one batch): 0.06898
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06780, 0.06780, 0.26039, 0.20024, 2031.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.23s
- Epoch 045, ExpID 81097
Train - Loss (one batch): 0.06603
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06774, 0.06774, 0.26027, 0.20070, 2061.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.27s
- Epoch 046, ExpID 81097
Train - Loss (one batch): 0.06575
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06775, 0.06775, 0.26029, 0.20158, 2109.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.39s
- Epoch 047, ExpID 81097
Train - Loss (one batch): 0.06912
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06779, 0.06779, 0.26037, 0.20131, 2091.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.39s
- Epoch 048, ExpID 81097
Train - Loss (one batch): 0.06642
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06774, 0.06774, 0.26027, 0.20080, 2067.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.20s
- Epoch 049, ExpID 81097
Train - Loss (one batch): 0.07221
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26016, 0.20184, 2126.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.27s
- Epoch 050, ExpID 81097
Train - Loss (one batch): 0.06637
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20206, 2139.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.18s
- Epoch 051, ExpID 81097
Train - Loss (one batch): 0.06893
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20217, 2143.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.12s
- Epoch 052, ExpID 81097
Train - Loss (one batch): 0.06488
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20106, 2085.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.33s
- Epoch 053, ExpID 81097
Train - Loss (one batch): 0.06606
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20105, 2084.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.30s
- Epoch 054, ExpID 81097
Train - Loss (one batch): 0.06859
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26015, 0.20167, 2119.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.53s
- Epoch 055, ExpID 81097
Train - Loss (one batch): 0.06818
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06783, 0.06783, 0.26043, 0.20203, 2120.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.51s
- Epoch 056, ExpID 81097
Train - Loss (one batch): 0.07067
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26022, 0.20159, 2112.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.45s
- Epoch 057, ExpID 81097
Train - Loss (one batch): 0.06402
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26020, 0.20180, 2125.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.70s
- Epoch 058, ExpID 81097
Train - Loss (one batch): 0.06626
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26017, 0.20178, 2122.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.57s
- Epoch 059, ExpID 81097
Train - Loss (one batch): 0.07008
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26023, 0.20134, 2101.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.44s
- Epoch 060, ExpID 81097
Train - Loss (one batch): 0.06842
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26020, 0.20232, 2150.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 061, ExpID 81097
Train - Loss (one batch): 0.06644
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26023, 0.20240, 2152.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.29s
- Epoch 062, ExpID 81097
Train - Loss (one batch): 0.06679
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06772, 0.06772, 0.26022, 0.20126, 2092.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.44s
- Epoch 063, ExpID 81097
Train - Loss (one batch): 0.06409
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20158, 2110.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.38s
- Epoch 064, ExpID 81097
Train - Loss (one batch): 0.06271
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26022, 0.20153, 2108.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.30s
- Epoch 065, ExpID 81097
Train - Loss (one batch): 0.06555
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20128, 2097.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.37s
- Epoch 066, ExpID 81097
Train - Loss (one batch): 0.06616
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26017, 0.20195, 2131.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.41s
- Epoch 067, ExpID 81097
Train - Loss (one batch): 0.06928
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20224, 2145.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.38s
- Epoch 068, ExpID 81097
Train - Loss (one batch): 0.06787
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26017, 0.20150, 2107.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.34s
- Epoch 069, ExpID 81097
Train - Loss (one batch): 0.06501
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26017, 0.20142, 2101.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.27s
- Epoch 070, ExpID 81097
Train - Loss (one batch): 0.07026
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20092, 2076.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.30s
- Epoch 071, ExpID 81097
Train - Loss (one batch): 0.06770
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20186, 2124.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 072, ExpID 81097
Train - Loss (one batch): 0.06699
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26020, 0.20113, 2086.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.30s
- Epoch 073, ExpID 81097
Train - Loss (one batch): 0.06547
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20134, 2096.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.80s
- Epoch 074, ExpID 81097
Train - Loss (one batch): 0.06877
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26016, 0.20162, 2114.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.41s
- Epoch 075, ExpID 81097
Train - Loss (one batch): 0.06551
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26015, 0.20152, 2109.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.37s
- Epoch 076, ExpID 81097
Train - Loss (one batch): 0.06646
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26020, 0.20140, 2101.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.48s
- Epoch 077, ExpID 81097
Train - Loss (one batch): 0.06631
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20188, 2127.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.45s
- Epoch 078, ExpID 81097
Train - Loss (one batch): 0.06935
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26016, 0.20169, 2116.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.53s
- Epoch 079, ExpID 81097
Train - Loss (one batch): 0.06993
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26020, 0.20158, 2111.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.44s
- Epoch 080, ExpID 81097
Train - Loss (one batch): 0.06756
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06773, 0.06773, 0.26024, 0.20139, 2094.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.40s
- Epoch 081, ExpID 81097
Train - Loss (one batch): 0.06450
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26016, 0.20204, 2136.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.53s
- Epoch 082, ExpID 81097
Train - Loss (one batch): 0.06878
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26020, 0.20102, 2080.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.39s
- Epoch 083, ExpID 81097
Train - Loss (one batch): 0.06803
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26016, 0.20164, 2114.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.47s
- Epoch 084, ExpID 81097
Train - Loss (one batch): 0.06603
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06773, 0.06773, 0.26025, 0.20131, 2092.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.20s
- Epoch 085, ExpID 81097
Train - Loss (one batch): 0.06756
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20197, 2133.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.18s
- Epoch 086, ExpID 81097
Train - Loss (one batch): 0.06679
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20096, 2078.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.24s
- Epoch 087, ExpID 81097
Train - Loss (one batch): 0.06705
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26016, 0.20162, 2113.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.25s
- Epoch 088, ExpID 81097
Train - Loss (one batch): 0.06672
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20107, 2084.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.35s
- Epoch 089, ExpID 81097
Train - Loss (one batch): 0.06552
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20131, 2096.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 090, ExpID 81097
Train - Loss (one batch): 0.06542
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26020, 0.20177, 2119.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.17s
- Epoch 091, ExpID 81097
Train - Loss (one batch): 0.07017
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06768, 0.06768, 0.26016, 0.20171, 2120.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.17s
- Epoch 092, ExpID 81097
Train - Loss (one batch): 0.06967
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20157, 2111.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.30s
- Epoch 093, ExpID 81097
Train - Loss (one batch): 0.06257
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06770, 0.06770, 0.26019, 0.20191, 2128.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 17.21s
- Epoch 094, ExpID 81097
Train - Loss (one batch): 0.06799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06773, 0.06773, 0.26025, 0.20094, 2074.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.46s
- Epoch 095, ExpID 81097
Train - Loss (one batch): 0.06895
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26022, 0.20187, 2121.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.51s
- Epoch 096, ExpID 81097
Train - Loss (one batch): 0.06647
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20150, 2105.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 097, ExpID 81097
Train - Loss (one batch): 0.06895
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06771, 0.06771, 0.26021, 0.20113, 2085.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.37s
- Epoch 098, ExpID 81097
Train - Loss (one batch): 0.06929
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06776, 0.06776, 0.26031, 0.20071, 2061.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.36s
- Epoch 099, ExpID 81097
Train - Loss (one batch): 0.06478
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06769, 0.06769, 0.26018, 0.20143, 2104.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.05840, 0.05840, 0.24167, 0.17840, 1180.27%
Time spent: 16.31s
True
Couldn't import umap
PID, device: 1965886 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=83, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7144961
n_train_batches: 225
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 16:18:18
run_iTransformer.py --history 24 --model iTransformer --dataset mimic
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=280, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=1982474, pred_window=24, ndim=96, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 73081
Train - Loss (one batch): 0.07365
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26694, 0.21656, 852.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 131.20s
- Epoch 001, ExpID 73081
Train - Loss (one batch): 0.07887
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07329, 0.07329, 0.27072, 0.22848, 993.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 115.44s
- Epoch 002, ExpID 73081
Train - Loss (one batch): 0.06916
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07150, 0.07150, 0.26739, 0.21918, 882.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07571, 0.07571, 0.27516, 0.21899, 790.50%
Time spent: 117.01s
- Epoch 003, ExpID 73081
Train - Loss (one batch): 0.08596
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26641, 0.21781, 886.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 128.35s
- Epoch 004, ExpID 73081
Train - Loss (one batch): 0.08150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07133, 0.07133, 0.26708, 0.21706, 855.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.80s
- Epoch 005, ExpID 73081
Train - Loss (one batch): 0.07166
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07197, 0.07197, 0.26827, 0.22235, 916.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 114.95s
- Epoch 006, ExpID 73081
Train - Loss (one batch): 0.07363
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07270, 0.07270, 0.26964, 0.22601, 954.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.64s
- Epoch 007, ExpID 73081
Train - Loss (one batch): 0.07162
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07137, 0.07137, 0.26715, 0.21252, 798.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.14s
- Epoch 008, ExpID 73081
Train - Loss (one batch): 0.07471
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07512, 0.07512, 0.27408, 0.23413, 1033.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.36s
- Epoch 009, ExpID 73081
Train - Loss (one batch): 0.07832
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07154, 0.07154, 0.26747, 0.21919, 880.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.55s
- Epoch 010, ExpID 73081
Train - Loss (one batch): 0.07580
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07181, 0.07181, 0.26798, 0.22152, 907.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.71s
- Epoch 011, ExpID 73081
Train - Loss (one batch): 0.08061
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07143, 0.07143, 0.26727, 0.21856, 871.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.07s
- Epoch 012, ExpID 73081
Train - Loss (one batch): 0.07969
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07130, 0.07130, 0.26702, 0.21667, 850.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.38s
- Epoch 013, ExpID 73081
Train - Loss (one batch): 0.07767
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07130, 0.07130, 0.26702, 0.21794, 865.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.85s
- Epoch 014, ExpID 73081
Train - Loss (one batch): 0.06499
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07156, 0.07156, 0.26750, 0.21961, 883.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.86s
- Epoch 015, ExpID 73081
Train - Loss (one batch): 0.08354
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07153, 0.07153, 0.26745, 0.21933, 882.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 118.28s
- Epoch 016, ExpID 73081
Train - Loss (one batch): 0.06678
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07148, 0.07148, 0.26736, 0.21876, 875.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.71s
- Epoch 017, ExpID 73081
Train - Loss (one batch): 0.07432
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26693, 0.21602, 841.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.14s
- Epoch 018, ExpID 73081
Train - Loss (one batch): 0.07430
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07127, 0.07127, 0.26697, 0.21627, 836.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 119.68s
- Epoch 019, ExpID 73081
Train - Loss (one batch): 0.06997
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26673, 0.21500, 817.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 117.14s
- Epoch 020, ExpID 73081
Train - Loss (one batch): 0.07813
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07142, 0.07142, 0.26724, 0.22021, 876.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.88s
- Epoch 021, ExpID 73081
Train - Loss (one batch): 0.07206
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07121, 0.07121, 0.26686, 0.21781, 848.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.66s
- Epoch 022, ExpID 73081
Train - Loss (one batch): 0.05948
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07113, 0.07113, 0.26670, 0.21701, 838.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.60s
- Epoch 023, ExpID 73081
Train - Loss (one batch): 0.06662
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07168, 0.07168, 0.26774, 0.22227, 896.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.21s
- Epoch 024, ExpID 73081
Train - Loss (one batch): 0.08066
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26658, 0.21618, 826.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 127.12s
- Epoch 025, ExpID 73081
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07140, 0.07140, 0.26721, 0.22056, 877.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 120.94s
- Epoch 026, ExpID 73081
Train - Loss (one batch): 0.07347
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07127, 0.07127, 0.26696, 0.21627, 830.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.29s
- Epoch 027, ExpID 73081
Train - Loss (one batch): 0.06779
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26677, 0.21673, 834.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.41s
- Epoch 028, ExpID 73081
Train - Loss (one batch): 0.07835
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26682, 0.21898, 856.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 117.12s
- Epoch 029, ExpID 73081
Train - Loss (one batch): 0.07401
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07126, 0.07126, 0.26695, 0.21554, 822.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.57s
- Epoch 030, ExpID 73081
Train - Loss (one batch): 0.07344
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26667, 0.21716, 835.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.86s
- Epoch 031, ExpID 73081
Train - Loss (one batch): 0.08243
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07123, 0.07123, 0.26690, 0.21809, 849.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.32s
- Epoch 032, ExpID 73081
Train - Loss (one batch): 0.07055
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07129, 0.07129, 0.26700, 0.21992, 865.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.14s
- Epoch 033, ExpID 73081
Train - Loss (one batch): 0.06930
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07132, 0.07132, 0.26706, 0.21956, 865.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 114.88s
- Epoch 034, ExpID 73081
Train - Loss (one batch): 0.07552
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26690, 0.21594, 826.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.76s
- Epoch 035, ExpID 73081
Train - Loss (one batch): 0.07577
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26691, 0.21640, 831.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.33s
- Epoch 036, ExpID 73081
Train - Loss (one batch): 0.07510
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07132, 0.07132, 0.26705, 0.21950, 864.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.36s
- Epoch 037, ExpID 73081
Train - Loss (one batch): 0.07726
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26676, 0.21851, 850.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.62s
- Epoch 038, ExpID 73081
Train - Loss (one batch): 0.08804
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21573, 818.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.61s
- Epoch 039, ExpID 73081
Train - Loss (one batch): 0.07107
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07121, 0.07121, 0.26684, 0.21606, 826.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.48s
- Epoch 040, ExpID 73081
Train - Loss (one batch): 0.07238
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21629, 823.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.70s
- Epoch 041, ExpID 73081
Train - Loss (one batch): 0.07449
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26675, 0.21747, 840.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.59s
- Epoch 042, ExpID 73081
Train - Loss (one batch): 0.07013
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26683, 0.21762, 843.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.46s
- Epoch 043, ExpID 73081
Train - Loss (one batch): 0.07588
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26666, 0.21731, 837.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.82s
- Epoch 044, ExpID 73081
Train - Loss (one batch): 0.07114
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07112, 0.07112, 0.26667, 0.21582, 821.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 122.10s
- Epoch 045, ExpID 73081
Train - Loss (one batch): 0.06885
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07107, 0.07107, 0.26659, 0.21716, 834.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 123.88s
- Epoch 046, ExpID 73081
Train - Loss (one batch): 0.07298
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07120, 0.07120, 0.26683, 0.21692, 836.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 114.91s
- Epoch 047, ExpID 73081
Train - Loss (one batch): 0.06961
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07125, 0.07125, 0.26693, 0.21564, 823.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 117.11s
- Epoch 048, ExpID 73081
Train - Loss (one batch): 0.07161
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21569, 822.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.86s
- Epoch 049, ExpID 73081
Train - Loss (one batch): 0.08031
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07117, 0.07117, 0.26678, 0.21685, 834.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.62s
- Epoch 050, ExpID 73081
Train - Loss (one batch): 0.07419
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26673, 0.21770, 842.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.19s
- Epoch 051, ExpID 73081
Train - Loss (one batch): 0.06886
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26674, 0.21600, 824.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.94s
- Epoch 052, ExpID 73081
Train - Loss (one batch): 0.07202
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07115, 0.07115, 0.26675, 0.21582, 822.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.70s
- Epoch 053, ExpID 73081
Train - Loss (one batch): 0.07465
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07118, 0.07118, 0.26679, 0.21690, 835.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.69s
- Epoch 054, ExpID 73081
Train - Loss (one batch): 0.08199
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07124, 0.07124, 0.26690, 0.21904, 857.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.46s
- Epoch 055, ExpID 73081
Train - Loss (one batch): 0.07070
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21668, 833.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.49s
- Epoch 056, ExpID 73081
Train - Loss (one batch): 0.08121
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26673, 0.21614, 826.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.50s
- Epoch 057, ExpID 73081
Train - Loss (one batch): 0.08086
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26652, 0.21672, 827.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.37s
- Epoch 058, ExpID 73081
Train - Loss (one batch): 0.07485
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07108, 0.07108, 0.26661, 0.21588, 826.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.93s
- Epoch 059, ExpID 73081
Train - Loss (one batch): 0.07399
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07114, 0.07114, 0.26672, 0.21677, 832.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 117.18s
- Epoch 060, ExpID 73081
Train - Loss (one batch): 0.06996
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07119, 0.07119, 0.26681, 0.21683, 835.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.28s
- Epoch 061, ExpID 73081
Train - Loss (one batch): 0.10100
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07112, 0.07112, 0.26668, 0.21681, 834.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.78s
- Epoch 062, ExpID 73081
Train - Loss (one batch): 0.07875
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21815, 851.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.68s
- Epoch 063, ExpID 73081
Train - Loss (one batch): 0.07986
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26658, 0.21743, 845.18%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 116.14s
- Epoch 064, ExpID 73081
Train - Loss (one batch): 0.07656
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26651, 0.21579, 828.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.91s
- Epoch 065, ExpID 73081
Train - Loss (one batch): 0.06816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21634, 832.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.07540, 0.07540, 0.27458, 0.22036, 823.14%
Time spent: 115.99s
- Epoch 066, ExpID 73081
Train - Loss (one batch): 0.07255
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07094, 0.07094, 0.26635, 0.21643, 831.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 129.60s
- Epoch 067, ExpID 73081
Train - Loss (one batch): 0.07563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07105, 0.07105, 0.26655, 0.21843, 853.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.08s
- Epoch 068, ExpID 73081
Train - Loss (one batch): 0.06944
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26648, 0.21728, 842.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.05s
- Epoch 069, ExpID 73081
Train - Loss (one batch): 0.07557
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26640, 0.21552, 821.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.26s
- Epoch 070, ExpID 73081
Train - Loss (one batch): 0.07989
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07099, 0.07099, 0.26644, 0.21746, 844.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.66s
- Epoch 071, ExpID 73081
Train - Loss (one batch): 0.06703
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26649, 0.21660, 836.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.87s
- Epoch 072, ExpID 73081
Train - Loss (one batch): 0.07638
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07116, 0.07116, 0.26676, 0.21926, 863.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.04s
- Epoch 073, ExpID 73081
Train - Loss (one batch): 0.07265
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07108, 0.07108, 0.26660, 0.21683, 837.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.13s
- Epoch 074, ExpID 73081
Train - Loss (one batch): 0.07512
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07103, 0.07103, 0.26652, 0.21839, 854.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.38s
- Epoch 075, ExpID 73081
Train - Loss (one batch): 0.07964
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26649, 0.21817, 850.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.60s
- Epoch 076, ExpID 73081
Train - Loss (one batch): 0.06050
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26649, 0.21647, 833.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.59s
- Epoch 077, ExpID 73081
Train - Loss (one batch): 0.07476
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07118, 0.07118, 0.26680, 0.21842, 858.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 115.93s
- Epoch 078, ExpID 73081
Train - Loss (one batch): 0.07897
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26648, 0.21707, 840.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 117.45s
- Epoch 079, ExpID 73081
Train - Loss (one batch): 0.07489
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07096, 0.07096, 0.26639, 0.21732, 840.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.14s
- Epoch 080, ExpID 73081
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07102, 0.07102, 0.26650, 0.21533, 821.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.71s
- Epoch 081, ExpID 73081
Train - Loss (one batch): 0.06949
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07101, 0.07101, 0.26649, 0.21864, 853.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.28s
- Epoch 082, ExpID 73081
Train - Loss (one batch): 0.07955
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07106, 0.07106, 0.26657, 0.21598, 831.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.11s
- Epoch 083, ExpID 73081
Train - Loss (one batch): 0.07740
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07098, 0.07098, 0.26641, 0.21623, 830.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.70s
- Epoch 084, ExpID 73081
Train - Loss (one batch): 0.08254
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21673, 835.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 66, 0.07545, 0.07545, 0.27468, 0.21893, 772.17%
Time spent: 116.29s
- Epoch 085, ExpID 73081
Train - Loss (one batch): 0.07830
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07090, 0.07090, 0.26628, 0.21671, 832.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 128.29s
- Epoch 086, ExpID 73081
Train - Loss (one batch): 0.06781
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21791, 846.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 119.50s
- Epoch 087, ExpID 73081
Train - Loss (one batch): 0.07804
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07093, 0.07093, 0.26633, 0.21604, 826.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 136.06s
- Epoch 088, ExpID 73081
Train - Loss (one batch): 0.06762
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21656, 835.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 137.13s
- Epoch 089, ExpID 73081
Train - Loss (one batch): 0.07889
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07097, 0.07097, 0.26640, 0.21654, 834.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 135.92s
- Epoch 090, ExpID 73081
Train - Loss (one batch): 0.07345
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07094, 0.07094, 0.26635, 0.21686, 836.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 135.24s
- Epoch 091, ExpID 73081
Train - Loss (one batch): 0.06858
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21851, 851.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 128.59s
- Epoch 092, ExpID 73081
Train - Loss (one batch): 0.06851
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07092, 0.07092, 0.26631, 0.21656, 832.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 114.66s
- Epoch 093, ExpID 73081
Train - Loss (one batch): 0.06977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07111, 0.07111, 0.26667, 0.21729, 843.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 118.04s
- Epoch 094, ExpID 73081
Train - Loss (one batch): 0.06666
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07109, 0.07109, 0.26662, 0.21607, 832.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.63s
- Epoch 095, ExpID 73081
Train - Loss (one batch): 0.07040
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07104, 0.07104, 0.26653, 0.21835, 853.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 116.02s
- Epoch 096, ExpID 73081
Train - Loss (one batch): 0.07244
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21693, 838.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.36s
- Epoch 097, ExpID 73081
Train - Loss (one batch): 0.07578
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21729, 843.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.90s
- Epoch 098, ExpID 73081
Train - Loss (one batch): 0.07122
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07100, 0.07100, 0.26646, 0.21788, 847.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 117.12s
- Epoch 099, ExpID 73081
Train - Loss (one batch): 0.07353
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07098, 0.07098, 0.26641, 0.21736, 841.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 85, 0.07540, 0.07540, 0.27459, 0.21919, 772.52%
Time spent: 115.17s
True
Couldn't import umap
PID, device: 1982474 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=280, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7245825
n_train_batches: 440
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 19:37:45
run_iTransformer.py --history 36 --model iTransformer --dataset mimic
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=464, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=2106113, pred_window=12, ndim=96, patch_layer=2, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 30162
Train - Loss (one batch): 0.07799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06866, 0.06866, 0.26202, 0.20672, 696.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 73.81s
- Epoch 001, ExpID 30162
Train - Loss (one batch): 0.07370
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21372, 778.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.69s
- Epoch 002, ExpID 30162
Train - Loss (one batch): 0.06950
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06964, 0.06964, 0.26389, 0.20873, 726.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 66.25s
- Epoch 003, ExpID 30162
Train - Loss (one batch): 0.05623
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07006, 0.07006, 0.26468, 0.21473, 787.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.16s
- Epoch 004, ExpID 30162
Train - Loss (one batch): 0.06341
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.20987, 735.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.46s
- Epoch 005, ExpID 30162
Train - Loss (one batch): 0.07468
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06994, 0.06994, 0.26446, 0.21282, 765.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.27s
- Epoch 006, ExpID 30162
Train - Loss (one batch): 0.06581
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06997, 0.06997, 0.26451, 0.20701, 701.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.12s
- Epoch 007, ExpID 30162
Train - Loss (one batch): 0.05903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06984, 0.06984, 0.26427, 0.21263, 766.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.90s
- Epoch 008, ExpID 30162
Train - Loss (one batch): 0.06444
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.21172, 756.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.38s
- Epoch 009, ExpID 30162
Train - Loss (one batch): 0.06315
Val - Loss, MSE, RMSE, MAE, MAPE: 0.07005, 0.07005, 0.26468, 0.21492, 790.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.62s
- Epoch 010, ExpID 30162
Train - Loss (one batch): 0.08195
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06999, 0.06999, 0.26456, 0.21424, 782.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.97s
- Epoch 011, ExpID 30162
Train - Loss (one batch): 0.05158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06978, 0.06978, 0.26416, 0.21083, 746.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.72s
- Epoch 012, ExpID 30162
Train - Loss (one batch): 0.06563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.20911, 727.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.22s
- Epoch 013, ExpID 30162
Train - Loss (one batch): 0.06887
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06979, 0.06979, 0.26418, 0.21009, 738.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.20s
- Epoch 014, ExpID 30162
Train - Loss (one batch): 0.07258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26421, 0.21127, 751.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.96s
- Epoch 015, ExpID 30162
Train - Loss (one batch): 0.06479
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06981, 0.06981, 0.26422, 0.21066, 744.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.62s
- Epoch 016, ExpID 30162
Train - Loss (one batch): 0.07007
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06984, 0.06984, 0.26426, 0.21135, 751.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.46s
- Epoch 017, ExpID 30162
Train - Loss (one batch): 0.06943
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21134, 751.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.18s
- Epoch 018, ExpID 30162
Train - Loss (one batch): 0.06164
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21098, 746.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.16s
- Epoch 019, ExpID 30162
Train - Loss (one batch): 0.06647
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26437, 0.20884, 722.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.36s
- Epoch 020, ExpID 30162
Train - Loss (one batch): 0.05979
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06985, 0.06985, 0.26429, 0.21031, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.31s
- Epoch 021, ExpID 30162
Train - Loss (one batch): 0.07134
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06997, 0.06997, 0.26451, 0.21318, 770.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.68s
- Epoch 022, ExpID 30162
Train - Loss (one batch): 0.06677
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26434, 0.20932, 728.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.95s
- Epoch 023, ExpID 30162
Train - Loss (one batch): 0.07364
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21159, 753.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.31s
- Epoch 024, ExpID 30162
Train - Loss (one batch): 0.06939
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21008, 736.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.03s
- Epoch 025, ExpID 30162
Train - Loss (one batch): 0.06135
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06993, 0.06993, 0.26444, 0.21263, 764.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.36s
- Epoch 026, ExpID 30162
Train - Loss (one batch): 0.06466
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21154, 752.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.17s
- Epoch 027, ExpID 30162
Train - Loss (one batch): 0.07300
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21182, 756.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.66s
- Epoch 028, ExpID 30162
Train - Loss (one batch): 0.07631
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21045, 741.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.28s
- Epoch 029, ExpID 30162
Train - Loss (one batch): 0.07008
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.20950, 730.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.22s
- Epoch 030, ExpID 30162
Train - Loss (one batch): 0.05081
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.20924, 727.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 66.04s
- Epoch 031, ExpID 30162
Train - Loss (one batch): 0.05875
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21065, 743.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.13s
- Epoch 032, ExpID 30162
Train - Loss (one batch): 0.06896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.20963, 731.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.48s
- Epoch 033, ExpID 30162
Train - Loss (one batch): 0.06411
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21159, 753.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.58s
- Epoch 034, ExpID 30162
Train - Loss (one batch): 0.05163
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26437, 0.21190, 756.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.29s
- Epoch 035, ExpID 30162
Train - Loss (one batch): 0.06800
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21012, 737.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.13s
- Epoch 036, ExpID 30162
Train - Loss (one batch): 0.06504
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21078, 744.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.29s
- Epoch 037, ExpID 30162
Train - Loss (one batch): 0.06461
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21039, 740.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.09s
- Epoch 038, ExpID 30162
Train - Loss (one batch): 0.06054
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20996, 735.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.36s
- Epoch 039, ExpID 30162
Train - Loss (one batch): 0.06059
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06998, 0.06998, 0.26454, 0.21342, 772.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.50s
- Epoch 040, ExpID 30162
Train - Loss (one batch): 0.05553
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21136, 751.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.25s
- Epoch 041, ExpID 30162
Train - Loss (one batch): 0.07474
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21117, 749.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.74s
- Epoch 042, ExpID 30162
Train - Loss (one batch): 0.06510
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21100, 747.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.28s
- Epoch 043, ExpID 30162
Train - Loss (one batch): 0.06436
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06996, 0.06996, 0.26451, 0.20777, 709.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 64.96s
- Epoch 044, ExpID 30162
Train - Loss (one batch): 0.06120
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21004, 736.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.46s
- Epoch 045, ExpID 30162
Train - Loss (one batch): 0.06275
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21172, 754.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.34s
- Epoch 046, ExpID 30162
Train - Loss (one batch): 0.06500
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21170, 754.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.27s
- Epoch 047, ExpID 30162
Train - Loss (one batch): 0.06222
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20972, 732.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 65.21s
- Epoch 048, ExpID 30162
Train - Loss (one batch): 0.05829
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21157, 753.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 75.16s
- Epoch 049, ExpID 30162
Train - Loss (one batch): 0.07969
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06990, 0.06990, 0.26438, 0.21205, 758.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 75.92s
- Epoch 050, ExpID 30162
Train - Loss (one batch): 0.07150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21161, 753.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 77.23s
- Epoch 051, ExpID 30162
Train - Loss (one batch): 0.06871
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21095, 746.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.05s
- Epoch 052, ExpID 30162
Train - Loss (one batch): 0.07350
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06990, 0.06990, 0.26438, 0.21206, 758.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.73s
- Epoch 053, ExpID 30162
Train - Loss (one batch): 0.05681
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21173, 755.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.01s
- Epoch 054, ExpID 30162
Train - Loss (one batch): 0.06849
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21144, 751.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.06s
- Epoch 055, ExpID 30162
Train - Loss (one batch): 0.05925
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21147, 752.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 75.81s
- Epoch 056, ExpID 30162
Train - Loss (one batch): 0.06648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21108, 748.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.84s
- Epoch 057, ExpID 30162
Train - Loss (one batch): 0.06768
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21030, 739.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 77.28s
- Epoch 058, ExpID 30162
Train - Loss (one batch): 0.07644
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21026, 738.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.51s
- Epoch 059, ExpID 30162
Train - Loss (one batch): 0.06258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20984, 734.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.66s
- Epoch 060, ExpID 30162
Train - Loss (one batch): 0.06767
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21225, 760.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.57s
- Epoch 061, ExpID 30162
Train - Loss (one batch): 0.07193
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21012, 737.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 90.01s
- Epoch 062, ExpID 30162
Train - Loss (one batch): 0.06308
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21147, 752.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.60s
- Epoch 063, ExpID 30162
Train - Loss (one batch): 0.06509
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.67s
- Epoch 064, ExpID 30162
Train - Loss (one batch): 0.07536
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21071, 743.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.98s
- Epoch 065, ExpID 30162
Train - Loss (one batch): 0.07077
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21069, 743.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.73s
- Epoch 066, ExpID 30162
Train - Loss (one batch): 0.07006
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20977, 733.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.30s
- Epoch 067, ExpID 30162
Train - Loss (one batch): 0.08869
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.20945, 729.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.59s
- Epoch 068, ExpID 30162
Train - Loss (one batch): 0.07095
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21121, 749.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.51s
- Epoch 069, ExpID 30162
Train - Loss (one batch): 0.06695
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06998, 0.06998, 0.26453, 0.21332, 771.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.20s
- Epoch 070, ExpID 30162
Train - Loss (one batch): 0.06173
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21173, 755.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.10s
- Epoch 071, ExpID 30162
Train - Loss (one batch): 0.06856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21029, 739.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.60s
- Epoch 072, ExpID 30162
Train - Loss (one batch): 0.06192
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20969, 732.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.16s
- Epoch 073, ExpID 30162
Train - Loss (one batch): 0.07430
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21050, 741.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.20s
- Epoch 074, ExpID 30162
Train - Loss (one batch): 0.07602
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.83s
- Epoch 075, ExpID 30162
Train - Loss (one batch): 0.06670
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21153, 752.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.15s
- Epoch 076, ExpID 30162
Train - Loss (one batch): 0.06597
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21032, 739.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.51s
- Epoch 077, ExpID 30162
Train - Loss (one batch): 0.06249
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21064, 743.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.03s
- Epoch 078, ExpID 30162
Train - Loss (one batch): 0.07370
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21223, 760.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.72s
- Epoch 079, ExpID 30162
Train - Loss (one batch): 0.06542
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21127, 750.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.17s
- Epoch 080, ExpID 30162
Train - Loss (one batch): 0.06014
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21070, 743.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.90s
- Epoch 081, ExpID 30162
Train - Loss (one batch): 0.06462
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.21097, 746.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.69s
- Epoch 082, ExpID 30162
Train - Loss (one batch): 0.06923
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.20993, 735.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.23s
- Epoch 083, ExpID 30162
Train - Loss (one batch): 0.06024
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26434, 0.21150, 752.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.59s
- Epoch 084, ExpID 30162
Train - Loss (one batch): 0.06473
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21079, 744.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 95.84s
- Epoch 085, ExpID 30162
Train - Loss (one batch): 0.06807
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06989, 0.06989, 0.26436, 0.21182, 756.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.32s
- Epoch 086, ExpID 30162
Train - Loss (one batch): 0.06324
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21136, 751.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.54s
- Epoch 087, ExpID 30162
Train - Loss (one batch): 0.06317
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06992, 0.06992, 0.26443, 0.21250, 763.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.62s
- Epoch 088, ExpID 30162
Train - Loss (one batch): 0.06389
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26436, 0.21172, 754.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.14s
- Epoch 089, ExpID 30162
Train - Loss (one batch): 0.07835
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21121, 749.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.08s
- Epoch 090, ExpID 30162
Train - Loss (one batch): 0.06422
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21049, 741.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 97.17s
- Epoch 091, ExpID 30162
Train - Loss (one batch): 0.07513
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26432, 0.20968, 732.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 96.84s
- Epoch 092, ExpID 30162
Train - Loss (one batch): 0.06535
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21063, 743.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 98.04s
- Epoch 093, ExpID 30162
Train - Loss (one batch): 0.06454
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06991, 0.06991, 0.26440, 0.21222, 760.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 77.95s
- Epoch 094, ExpID 30162
Train - Loss (one batch): 0.05238
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26433, 0.21119, 749.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.44s
- Epoch 095, ExpID 30162
Train - Loss (one batch): 0.06385
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06993, 0.06993, 0.26445, 0.21270, 765.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 77.09s
- Epoch 096, ExpID 30162
Train - Loss (one batch): 0.05735
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06986, 0.06986, 0.26431, 0.21035, 739.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 77.01s
- Epoch 097, ExpID 30162
Train - Loss (one batch): 0.05959
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21115, 748.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.62s
- Epoch 098, ExpID 30162
Train - Loss (one batch): 0.06648
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06988, 0.06988, 0.26435, 0.21155, 753.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 76.73s
- Epoch 099, ExpID 30162
Train - Loss (one batch): 0.06690
Val - Loss, MSE, RMSE, MAE, MAPE: 0.06987, 0.06987, 0.26432, 0.21112, 748.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.07077, 0.07077, 0.26602, 0.21111, 647.58%
Time spent: 75.80s
True
Couldn't import umap
PID, device: 2106113 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=464, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7340033
n_train_batches: 440
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 21:53:45
run_iTransformer.py --history 12 --model iTransformer --dataset mimic
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=133, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2185381, pred_window=36, ndim=96, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
True
Couldn't import umap
PID, device: 2185381 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=133, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7170561
n_train_batches: 440
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py", line 406, in <module>
    train_res["loss"].backward()
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.62 GiB. GPU 
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_iTransformer.py
2024-07-19 21:55:14
run_iTransformer.py --history 24 --model iTransformer --dataset ushcn
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='ushcn', quantization=0.0, model='iTransformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=205, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=2189286, n_months=48, pred_window=1, ndim=5, patch_layer=1, task_name='long_term_forecast', pred_len=720, output_attention=None, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=3, d_ff=512, e_layers=4, n_heads=1, activation='gelu')
- Epoch 000, ExpID 56753
Train - Loss (one batch): 0.83211
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14820, 1.14820, 1.07154, 0.52918, -98.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 76.05s
- Epoch 001, ExpID 56753
Train - Loss (one batch): 0.55090
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16086, 1.16086, 1.07743, 0.54474, -116.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 61.43s
- Epoch 002, ExpID 56753
Train - Loss (one batch): 0.48451
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15851, 1.15851, 1.07634, 0.52615, -97.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 60.69s
- Epoch 003, ExpID 56753
Train - Loss (one batch): 2.06049
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16868, 1.16868, 1.08105, 0.54518, -101.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.76610, 0.76610, 0.87527, 0.50373, -100.64%
Time spent: 64.36s
- Epoch 004, ExpID 56753
Train - Loss (one batch): 0.52008
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14504, 1.14504, 1.07006, 0.52319, -97.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.75642, 0.75642, 0.86973, 0.49328, -98.53%
Time spent: 75.52s
- Epoch 005, ExpID 56753
Train - Loss (one batch): 0.56293
Val - Loss, MSE, RMSE, MAE, MAPE: 1.14254, 1.14254, 1.06890, 0.51815, -91.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 74.60s
- Epoch 006, ExpID 56753
Train - Loss (one batch): 0.40378
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16982, 1.16982, 1.08158, 0.50860, -61.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.75s
- Epoch 007, ExpID 56753
Train - Loss (one batch): 0.31913
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16987, 1.16987, 1.08160, 0.51330, -68.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.44s
- Epoch 008, ExpID 56753
Train - Loss (one batch): 0.53086
Val - Loss, MSE, RMSE, MAE, MAPE: 1.18134, 1.18134, 1.08689, 0.51994, -63.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.82s
- Epoch 009, ExpID 56753
Train - Loss (one batch): 1.22436
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17922, 1.17922, 1.08592, 0.53023, -73.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.94s
- Epoch 010, ExpID 56753
Train - Loss (one batch): 0.66053
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20998, 1.20998, 1.09999, 0.54426, -58.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.21s
- Epoch 011, ExpID 56753
Train - Loss (one batch): 0.58991
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20787, 1.20787, 1.09903, 0.51965, -36.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.58s
- Epoch 012, ExpID 56753
Train - Loss (one batch): 0.59762
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20677, 1.20677, 1.09853, 0.54416, -62.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.12s
- Epoch 013, ExpID 56753
Train - Loss (one batch): 0.69837
Val - Loss, MSE, RMSE, MAE, MAPE: 1.21114, 1.21114, 1.10052, 0.52662, -41.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.34s
- Epoch 014, ExpID 56753
Train - Loss (one batch): 1.76396
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20537, 1.20537, 1.09789, 0.55906, -81.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.01s
- Epoch 015, ExpID 56753
Train - Loss (one batch): 0.46792
Val - Loss, MSE, RMSE, MAE, MAPE: 1.20246, 1.20246, 1.09657, 0.55506, -82.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.98s
- Epoch 016, ExpID 56753
Train - Loss (one batch): 1.11103
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17864, 1.17864, 1.08565, 0.53979, -77.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 66.23s
- Epoch 017, ExpID 56753
Train - Loss (one batch): 1.53562
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15223, 1.15223, 1.07342, 0.51908, -86.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.56s
- Epoch 018, ExpID 56753
Train - Loss (one batch): 0.42909
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16337, 1.16337, 1.07860, 0.53218, -95.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.87s
- Epoch 019, ExpID 56753
Train - Loss (one batch): 0.39305
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15561, 1.15561, 1.07499, 0.53179, -96.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.16s
- Epoch 020, ExpID 56753
Train - Loss (one batch): 1.37649
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15371, 1.15371, 1.07411, 0.51712, -84.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.98s
- Epoch 021, ExpID 56753
Train - Loss (one batch): 0.58898
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15810, 1.15810, 1.07615, 0.51669, -81.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 60.62s
- Epoch 022, ExpID 56753
Train - Loss (one batch): 1.25931
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15278, 1.15278, 1.07367, 0.52859, -97.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.88s
- Epoch 023, ExpID 56753
Train - Loss (one batch): 0.41614
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15941, 1.15941, 1.07676, 0.53215, -93.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.72s
- Epoch 024, ExpID 56753
Train - Loss (one batch): 0.55478
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15909, 1.15909, 1.07661, 0.53205, -95.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.37s
- Epoch 025, ExpID 56753
Train - Loss (one batch): 0.94325
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15222, 1.15222, 1.07342, 0.51493, -83.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.27s
- Epoch 026, ExpID 56753
Train - Loss (one batch): 0.88726
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15395, 1.15395, 1.07422, 0.52660, -91.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.49s
- Epoch 027, ExpID 56753
Train - Loss (one batch): 0.61737
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15420, 1.15420, 1.07434, 0.51693, -82.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.10s
- Epoch 028, ExpID 56753
Train - Loss (one batch): 0.36144
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15327, 1.15327, 1.07390, 0.51765, -86.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.08s
- Epoch 029, ExpID 56753
Train - Loss (one batch): 0.47134
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15433, 1.15433, 1.07440, 0.52649, -92.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.86s
- Epoch 030, ExpID 56753
Train - Loss (one batch): 2.25681
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15513, 1.15513, 1.07477, 0.52216, -89.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.56s
- Epoch 031, ExpID 56753
Train - Loss (one batch): 0.65219
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15533, 1.15533, 1.07486, 0.52841, -96.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.25s
- Epoch 032, ExpID 56753
Train - Loss (one batch): 1.24601
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15210, 1.15210, 1.07336, 0.51934, -86.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.64s
- Epoch 033, ExpID 56753
Train - Loss (one batch): 1.01426
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15147, 1.15147, 1.07307, 0.51948, -86.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 58.68s
- Epoch 034, ExpID 56753
Train - Loss (one batch): 0.47623
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15154, 1.15154, 1.07310, 0.52374, -92.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 59.23s
- Epoch 035, ExpID 56753
Train - Loss (one batch): 0.43605
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15445, 1.15445, 1.07445, 0.51351, -78.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 57.31s
- Epoch 036, ExpID 56753
Train - Loss (one batch): 1.14697
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16063, 1.16063, 1.07733, 0.53801, -103.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 59.68s
- Epoch 037, ExpID 56753
Train - Loss (one batch): 0.77069
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15349, 1.15349, 1.07401, 0.52692, -93.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.12s
- Epoch 038, ExpID 56753
Train - Loss (one batch): 0.60469
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15486, 1.15486, 1.07465, 0.53026, -97.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.36s
- Epoch 039, ExpID 56753
Train - Loss (one batch): 0.55241
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15483, 1.15483, 1.07463, 0.52374, -89.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.06s
- Epoch 040, ExpID 56753
Train - Loss (one batch): 0.37808
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15375, 1.15375, 1.07413, 0.52652, -93.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.03s
- Epoch 041, ExpID 56753
Train - Loss (one batch): 0.36023
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15212, 1.15212, 1.07337, 0.51675, -85.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.06s
- Epoch 042, ExpID 56753
Train - Loss (one batch): 0.42918
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15246, 1.15246, 1.07353, 0.51818, -84.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.25s
- Epoch 043, ExpID 56753
Train - Loss (one batch): 0.64691
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15359, 1.15359, 1.07405, 0.51395, -81.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.07s
- Epoch 044, ExpID 56753
Train - Loss (one batch): 0.40142
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15263, 1.15263, 1.07360, 0.51302, -81.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.54s
- Epoch 045, ExpID 56753
Train - Loss (one batch): 0.60986
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15607, 1.15607, 1.07521, 0.52999, -96.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.74s
- Epoch 046, ExpID 56753
Train - Loss (one batch): 1.54490
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15184, 1.15184, 1.07324, 0.51924, -87.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.36s
- Epoch 047, ExpID 56753
Train - Loss (one batch): 0.33049
Val - Loss, MSE, RMSE, MAE, MAPE: 1.17411, 1.17411, 1.08356, 0.55035, -108.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.49s
- Epoch 048, ExpID 56753
Train - Loss (one batch): 1.10149
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15209, 1.15209, 1.07335, 0.52456, -93.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.51s
- Epoch 049, ExpID 56753
Train - Loss (one batch): 0.42769
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15306, 1.15306, 1.07381, 0.52082, -89.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.56s
- Epoch 050, ExpID 56753
Train - Loss (one batch): 0.82024
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15133, 1.15133, 1.07300, 0.51736, -84.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.22s
- Epoch 051, ExpID 56753
Train - Loss (one batch): 0.34664
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16286, 1.16286, 1.07836, 0.54298, -107.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.72s
- Epoch 052, ExpID 56753
Train - Loss (one batch): 0.56038
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15342, 1.15342, 1.07397, 0.52530, -93.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.01s
- Epoch 053, ExpID 56753
Train - Loss (one batch): 0.70661
Val - Loss, MSE, RMSE, MAE, MAPE: 1.16605, 1.16605, 1.07984, 0.52575, -87.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.20s
- Epoch 054, ExpID 56753
Train - Loss (one batch): 0.47455
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15187, 1.15187, 1.07325, 0.52116, -89.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.94s
- Epoch 055, ExpID 56753
Train - Loss (one batch): 2.15308
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15338, 1.15338, 1.07396, 0.52759, -93.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.95s
- Epoch 056, ExpID 56753
Train - Loss (one batch): 0.65054
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15323, 1.15323, 1.07388, 0.52331, -90.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.88s
- Epoch 057, ExpID 56753
Train - Loss (one batch): 0.50470
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15736, 1.15736, 1.07581, 0.53014, -95.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.68s
- Epoch 058, ExpID 56753
Train - Loss (one batch): 2.06229
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15495, 1.15495, 1.07468, 0.53265, -99.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.65s
- Epoch 059, ExpID 56753
Train - Loss (one batch): 1.74771
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15389, 1.15389, 1.07419, 0.52923, -97.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.83s
- Epoch 060, ExpID 56753
Train - Loss (one batch): 0.99950
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15363, 1.15363, 1.07407, 0.52721, -95.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.31s
- Epoch 061, ExpID 56753
Train - Loss (one batch): 0.94571
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15223, 1.15223, 1.07342, 0.52514, -93.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.23s
- Epoch 062, ExpID 56753
Train - Loss (one batch): 0.82096
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15161, 1.15161, 1.07313, 0.51887, -88.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.63s
- Epoch 063, ExpID 56753
Train - Loss (one batch): 0.68655
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15441, 1.15441, 1.07443, 0.52920, -97.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 61.41s
- Epoch 064, ExpID 56753
Train - Loss (one batch): 0.64846
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15650, 1.15650, 1.07541, 0.52563, -94.65%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.81s
- Epoch 065, ExpID 56753
Train - Loss (one batch): 0.55562
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15483, 1.15483, 1.07463, 0.53014, -96.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.78s
- Epoch 066, ExpID 56753
Train - Loss (one batch): 0.79318
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15236, 1.15236, 1.07348, 0.51719, -85.07%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.46s
- Epoch 067, ExpID 56753
Train - Loss (one batch): 0.50406
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15109, 1.15109, 1.07289, 0.52104, -89.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.56s
- Epoch 068, ExpID 56753
Train - Loss (one batch): 0.68175
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15233, 1.15233, 1.07346, 0.51472, -83.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.49s
- Epoch 069, ExpID 56753
Train - Loss (one batch): 0.95467
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15162, 1.15162, 1.07314, 0.52248, -90.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.92s
- Epoch 070, ExpID 56753
Train - Loss (one batch): 0.37607
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15232, 1.15232, 1.07346, 0.51567, -84.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.56s
- Epoch 071, ExpID 56753
Train - Loss (one batch): 0.42334
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15542, 1.15542, 1.07490, 0.52225, -90.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.17s
- Epoch 072, ExpID 56753
Train - Loss (one batch): 0.55590
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15316, 1.15316, 1.07385, 0.51252, -79.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.03s
- Epoch 073, ExpID 56753
Train - Loss (one batch): 0.72268
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15172, 1.15172, 1.07318, 0.52163, -90.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.35s
- Epoch 074, ExpID 56753
Train - Loss (one batch): 0.51229
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15151, 1.15151, 1.07309, 0.51920, -86.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.22s
- Epoch 075, ExpID 56753
Train - Loss (one batch): 0.66453
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15287, 1.15287, 1.07372, 0.51404, -82.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.95s
- Epoch 076, ExpID 56753
Train - Loss (one batch): 0.57236
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15077, 1.15077, 1.07274, 0.51549, -83.99%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.73s
- Epoch 077, ExpID 56753
Train - Loss (one batch): 0.52558
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15821, 1.15821, 1.07620, 0.52257, -89.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.61s
- Epoch 078, ExpID 56753
Train - Loss (one batch): 0.41785
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15581, 1.15581, 1.07509, 0.51915, -85.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.46s
- Epoch 079, ExpID 56753
Train - Loss (one batch): 0.40002
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15460, 1.15460, 1.07453, 0.53159, -97.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.07s
- Epoch 080, ExpID 56753
Train - Loss (one batch): 0.32391
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15089, 1.15089, 1.07279, 0.51697, -85.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.15s
- Epoch 081, ExpID 56753
Train - Loss (one batch): 0.56063
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15632, 1.15632, 1.07532, 0.52322, -90.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.69s
- Epoch 082, ExpID 56753
Train - Loss (one batch): 1.62874
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15113, 1.15113, 1.07291, 0.52152, -89.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.37s
- Epoch 083, ExpID 56753
Train - Loss (one batch): 0.44640
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15439, 1.15439, 1.07442, 0.52573, -92.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 57.28s
- Epoch 084, ExpID 56753
Train - Loss (one batch): 1.87010
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15281, 1.15281, 1.07369, 0.51975, -88.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 58.50s
- Epoch 085, ExpID 56753
Train - Loss (one batch): 1.74642
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15377, 1.15377, 1.07414, 0.51806, -85.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 62.99s
- Epoch 086, ExpID 56753
Train - Loss (one batch): 1.34664
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15207, 1.15207, 1.07334, 0.51506, -83.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 58.05s
- Epoch 087, ExpID 56753
Train - Loss (one batch): 0.37898
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15112, 1.15112, 1.07290, 0.51548, -85.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.14s
- Epoch 088, ExpID 56753
Train - Loss (one batch): 0.38694
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15173, 1.15173, 1.07319, 0.51724, -86.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.29s
- Epoch 089, ExpID 56753
Train - Loss (one batch): 1.30190
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15290, 1.15290, 1.07373, 0.52437, -93.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.57s
- Epoch 090, ExpID 56753
Train - Loss (one batch): 0.49529
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15119, 1.15119, 1.07294, 0.52046, -89.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.01s
- Epoch 091, ExpID 56753
Train - Loss (one batch): 0.68505
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15312, 1.15312, 1.07384, 0.51565, -84.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 67.04s
- Epoch 092, ExpID 56753
Train - Loss (one batch): 0.52596
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15320, 1.15320, 1.07387, 0.51655, -83.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.87s
- Epoch 093, ExpID 56753
Train - Loss (one batch): 1.31327
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15201, 1.15201, 1.07332, 0.51862, -85.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 67.60s
- Epoch 094, ExpID 56753
Train - Loss (one batch): 1.48162
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15281, 1.15281, 1.07369, 0.52727, -93.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 63.66s
- Epoch 095, ExpID 56753
Train - Loss (one batch): 1.03094
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15692, 1.15692, 1.07560, 0.53546, -102.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 67.50s
- Epoch 096, ExpID 56753
Train - Loss (one batch): 0.27677
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15148, 1.15148, 1.07307, 0.52400, -91.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 66.96s
- Epoch 097, ExpID 56753
Train - Loss (one batch): 1.37166
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15311, 1.15311, 1.07383, 0.52808, -96.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 64.71s
- Epoch 098, ExpID 56753
Train - Loss (one batch): 0.46769
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15275, 1.15275, 1.07366, 0.51409, -81.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 66.89s
- Epoch 099, ExpID 56753
Train - Loss (one batch): 0.47322
Val - Loss, MSE, RMSE, MAE, MAPE: 1.15290, 1.15290, 1.07373, 0.50902, -76.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.74997, 0.74997, 0.86601, 0.48771, -92.32%
Time spent: 65.45s
True
Couldn't import umap
PID, device: 2189286 cuda:0
Total records: 1114
Dataset n_samples: 1114 668 223 223
Test record ids (first 20): [886, 101, 1120, 730, 291, 875, 958, 260, 128, 1043, 620, 88, 1005, 872, 617, 538, 508, 44, 273, 817]
Test record ids (last 20): [982, 275, 991, 997, 66, 801, 67, 942, 12, 361, 555, 710, 333, 1017, 851, 184, 882, 509, 726, 587]
data_max: tensor([37.2551, 36.6691, 38.2520,  3.0401,  2.9104], device='cuda:0')
data_min: tensor([-0.3057, -0.1280, -0.1738, -3.8821, -4.1854], device='cuda:0')
time_max: tensor(48., device='cuda:0')
Dataset n_samples after time split: 26736 16032 5352 5352
model Model(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=205, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-3): 4 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=511, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=512, out_features=1, bias=True)
  )
)
parameters: 7207425
n_train_batches: 501
./scripts/run_baselines_iTransformer.sh: 17: y: not found
