nohup: ignoring input
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 11:11:19
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 32 --lr 1e-3 --patch_size 3 --stride 3 --nhead 1 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 1 --gpu 0 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='0', npatch=8, device=device(type='cuda', index=0), PID=91691, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 89224
Train - Loss (one batch): 0.02803
Val - Loss, MSE, RMSE, MAE, MAPE: 0.03114, 0.03114, 0.17646, 0.12296, 449.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.03468, 0.03468, 0.18623, 0.12456, 389.94%
Time spent: 98.54s
- Epoch 001, ExpID 89224
Train - Loss (one batch): 0.01286
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02094, 0.02094, 0.14472, 0.09037, 260.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02428, 0.02428, 0.15582, 0.09263, 215.74%
Time spent: 97.95s
- Epoch 002, ExpID 89224
Train - Loss (one batch): 0.01447
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01973, 0.01973, 0.14046, 0.08556, 216.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02258, 0.02258, 0.15025, 0.08769, 174.81%
Time spent: 96.88s
- Epoch 003, ExpID 89224
Train - Loss (one batch): 0.02465
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01904, 0.01904, 0.13797, 0.08346, 203.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02185, 0.02185, 0.14783, 0.08546, 165.38%
Time spent: 98.13s
- Epoch 004, ExpID 89224
Train - Loss (one batch): 0.01085
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01856, 0.01856, 0.13622, 0.07986, 172.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.02108, 0.02108, 0.14520, 0.08156, 138.66%
Time spent: 97.90s
- Epoch 005, ExpID 89224
Train - Loss (one batch): 0.01328
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01822, 0.01822, 0.13500, 0.07798, 153.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.02063, 0.02063, 0.14362, 0.07957, 123.45%
Time spent: 96.71s
- Epoch 006, ExpID 89224
Train - Loss (one batch): 0.01459
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01847, 0.01847, 0.13589, 0.08272, 179.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.02063, 0.02063, 0.14362, 0.07957, 123.45%
Time spent: 82.38s
- Epoch 007, ExpID 89224
Train - Loss (one batch): 0.01430
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01784, 0.01784, 0.13358, 0.07725, 150.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.02019, 0.02019, 0.14210, 0.07912, 125.04%
Time spent: 97.91s
- Epoch 008, ExpID 89224
Train - Loss (one batch): 0.01402
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01760, 0.01760, 0.13266, 0.07438, 131.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02001, 0.02001, 0.14146, 0.07639, 112.24%
Time spent: 97.54s
- Epoch 009, ExpID 89224
Train - Loss (one batch): 0.01452
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01785, 0.01785, 0.13361, 0.07775, 137.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02001, 0.02001, 0.14146, 0.07639, 112.24%
Time spent: 81.70s
- Epoch 010, ExpID 89224
Train - Loss (one batch): 0.02070
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01735, 0.01735, 0.13172, 0.07459, 133.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01954, 0.01954, 0.13977, 0.07648, 114.11%
Time spent: 114.78s
- Epoch 011, ExpID 89224
Train - Loss (one batch): 0.01595
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01718, 0.01718, 0.13109, 0.07418, 144.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01941, 0.01941, 0.13933, 0.07628, 126.77%
Time spent: 99.13s
- Epoch 012, ExpID 89224
Train - Loss (one batch): 0.01102
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01710, 0.01710, 0.13075, 0.07370, 131.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.01927, 0.01927, 0.13880, 0.07563, 113.95%
Time spent: 117.14s
- Epoch 013, ExpID 89224
Train - Loss (one batch): 0.01911
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01700, 0.01700, 0.13039, 0.07290, 127.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01915, 0.01915, 0.13838, 0.07491, 111.44%
Time spent: 98.10s
- Epoch 014, ExpID 89224
Train - Loss (one batch): 0.00722
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01727, 0.01727, 0.13141, 0.07315, 118.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01915, 0.01915, 0.13838, 0.07491, 111.44%
Time spent: 82.13s
- Epoch 015, ExpID 89224
Train - Loss (one batch): 0.01079
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01702, 0.01702, 0.13046, 0.07318, 119.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01915, 0.01915, 0.13838, 0.07491, 111.44%
Time spent: 81.58s
- Epoch 016, ExpID 89224
Train - Loss (one batch): 0.00849
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01693, 0.01693, 0.13010, 0.07291, 115.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01919, 0.01919, 0.13852, 0.07496, 106.32%
Time spent: 98.17s
- Epoch 017, ExpID 89224
Train - Loss (one batch): 0.01350
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01714, 0.01714, 0.13090, 0.07346, 106.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01919, 0.01919, 0.13852, 0.07496, 106.32%
Time spent: 81.80s
- Epoch 018, ExpID 89224
Train - Loss (one batch): 0.01967
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01685, 0.01685, 0.12983, 0.07242, 115.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01899, 0.01899, 0.13781, 0.07438, 105.41%
Time spent: 97.83s
- Epoch 019, ExpID 89224
Train - Loss (one batch): 0.01077
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01709, 0.01709, 0.13074, 0.07395, 116.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01899, 0.01899, 0.13781, 0.07438, 105.41%
Time spent: 82.42s
- Epoch 020, ExpID 89224
Train - Loss (one batch): 0.01104
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01689, 0.01689, 0.12996, 0.07121, 109.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01899, 0.01899, 0.13781, 0.07438, 105.41%
Time spent: 82.25s
- Epoch 021, ExpID 89224
Train - Loss (one batch): 0.01210
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01674, 0.01674, 0.12937, 0.07236, 127.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 98.27s
- Epoch 022, ExpID 89224
Train - Loss (one batch): 0.00942
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01690, 0.01690, 0.12999, 0.07143, 100.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 81.86s
- Epoch 023, ExpID 89224
Train - Loss (one batch): 0.01217
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01688, 0.01688, 0.12991, 0.07197, 115.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 82.08s
- Epoch 024, ExpID 89224
Train - Loss (one batch): 0.00941
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01684, 0.01684, 0.12978, 0.07270, 125.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 80.90s
- Epoch 025, ExpID 89224
Train - Loss (one batch): 0.01011
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01690, 0.01690, 0.13001, 0.07257, 112.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 81.30s
- Epoch 026, ExpID 89224
Train - Loss (one batch): 0.01566
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01695, 0.01695, 0.13019, 0.07124, 97.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 81.46s
- Epoch 027, ExpID 89224
Train - Loss (one batch): 0.01186
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01697, 0.01697, 0.13026, 0.07384, 129.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 80.99s
- Epoch 028, ExpID 89224
Train - Loss (one batch): 0.00915
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12965, 0.07072, 109.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 76.46s
- Epoch 029, ExpID 89224
Train - Loss (one batch): 0.01558
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01691, 0.01691, 0.13003, 0.07286, 130.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 76.51s
- Epoch 030, ExpID 89224
Train - Loss (one batch): 0.00922
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01707, 0.01707, 0.13065, 0.07434, 120.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 79.33s
- Epoch 031, ExpID 89224
Train - Loss (one batch): 0.01027
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01680, 0.01680, 0.12962, 0.07133, 109.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01895, 0.01895, 0.13766, 0.07425, 118.27%
Time spent: 81.09s
Couldn't import umap
PID, device: 91691 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 440
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 12:02:25
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 32 --lr 1e-3 --patch_size 3 --stride 3 --nhead 1 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 2 --gpu 0 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=2, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='0', npatch=8, device=device(type='cuda', index=0), PID=127629, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 52157
Train - Loss (one batch): 0.03483
Val - Loss, MSE, RMSE, MAE, MAPE: 0.03426, 0.03426, 0.18509, 0.12423, 411.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.03763, 0.03763, 0.19399, 0.12837, 365.24%
Time spent: 98.37s
- Epoch 001, ExpID 52157
Train - Loss (one batch): 0.02667
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02417, 0.02417, 0.15548, 0.09653, 274.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02675, 0.02675, 0.16357, 0.09970, 243.67%
Time spent: 99.85s
- Epoch 002, ExpID 52157
Train - Loss (one batch): 0.01278
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02011, 0.02011, 0.14180, 0.08467, 183.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02273, 0.02273, 0.15076, 0.08736, 162.97%
Time spent: 99.41s
- Epoch 003, ExpID 52157
Train - Loss (one batch): 0.02130
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01805, 0.01805, 0.13435, 0.08077, 180.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02043, 0.02043, 0.14292, 0.08362, 160.67%
Time spent: 98.21s
- Epoch 004, ExpID 52157
Train - Loss (one batch): 0.01277
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01708, 0.01708, 0.13067, 0.07787, 151.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.01972, 0.01972, 0.14042, 0.08138, 134.87%
Time spent: 97.55s
- Epoch 005, ExpID 52157
Train - Loss (one batch): 0.01436
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01627, 0.01627, 0.12755, 0.07680, 171.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01889, 0.01889, 0.13745, 0.08015, 152.34%
Time spent: 97.86s
- Epoch 006, ExpID 52157
Train - Loss (one batch): 0.01394
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01629, 0.01629, 0.12763, 0.07582, 150.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01889, 0.01889, 0.13745, 0.08015, 152.34%
Time spent: 86.73s
- Epoch 007, ExpID 52157
Train - Loss (one batch): 0.01948
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01578, 0.01578, 0.12562, 0.07468, 166.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.01813, 0.01813, 0.13464, 0.07761, 147.34%
Time spent: 133.94s
- Epoch 008, ExpID 52157
Train - Loss (one batch): 0.01217
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01565, 0.01565, 0.12511, 0.07288, 135.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 133.56s
- Epoch 009, ExpID 52157
Train - Loss (one batch): 0.01051
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01601, 0.01601, 0.12655, 0.07545, 152.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 111.16s
- Epoch 010, ExpID 52157
Train - Loss (one batch): 0.01513
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01579, 0.01579, 0.12567, 0.07292, 129.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 111.40s
- Epoch 011, ExpID 52157
Train - Loss (one batch): 0.01303
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01591, 0.01591, 0.12614, 0.07279, 130.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 84.36s
- Epoch 012, ExpID 52157
Train - Loss (one batch): 0.01863
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01592, 0.01592, 0.12618, 0.07436, 146.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 82.87s
- Epoch 013, ExpID 52157
Train - Loss (one batch): 0.01544
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01604, 0.01604, 0.12666, 0.07459, 135.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 82.71s
- Epoch 014, ExpID 52157
Train - Loss (one batch): 0.00696
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01600, 0.01600, 0.12649, 0.07308, 135.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 83.63s
- Epoch 015, ExpID 52157
Train - Loss (one batch): 0.01264
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01612, 0.01612, 0.12696, 0.07181, 125.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 83.77s
- Epoch 016, ExpID 52157
Train - Loss (one batch): 0.01041
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01627, 0.01627, 0.12757, 0.07180, 123.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 82.71s
- Epoch 017, ExpID 52157
Train - Loss (one batch): 0.01422
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01648, 0.01648, 0.12839, 0.07422, 137.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 83.65s
- Epoch 018, ExpID 52157
Train - Loss (one batch): 0.01570
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01668, 0.01668, 0.12915, 0.07378, 141.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01809, 0.01809, 0.13451, 0.07606, 118.55%
Time spent: 83.41s
Couldn't import umap
PID, device: 127629 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 440
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 12:36:03
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 32 --lr 1e-3 --patch_size 3 --stride 3 --nhead 1 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 3 --gpu 0 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=3, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='0', npatch=8, device=device(type='cuda', index=0), PID=148893, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 30506
Train - Loss (one batch): 0.03058
Val - Loss, MSE, RMSE, MAE, MAPE: 0.04311, 0.04311, 0.20762, 0.14432, 415.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.04908, 0.04908, 0.22154, 0.14936, 395.00%
Time spent: 97.37s
- Epoch 001, ExpID 30506
Train - Loss (one batch): 0.01781
Val - Loss, MSE, RMSE, MAE, MAPE: 0.03153, 0.03153, 0.17757, 0.10997, 269.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.03603, 0.03603, 0.18982, 0.11365, 236.65%
Time spent: 97.58s
- Epoch 002, ExpID 30506
Train - Loss (one batch): 0.02000
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02690, 0.02690, 0.16402, 0.09862, 223.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.03131, 0.03131, 0.17694, 0.10259, 193.62%
Time spent: 97.71s
- Epoch 003, ExpID 30506
Train - Loss (one batch): 0.01198
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02363, 0.02363, 0.15372, 0.09074, 191.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02757, 0.02757, 0.16604, 0.09460, 166.65%
Time spent: 97.06s
- Epoch 004, ExpID 30506
Train - Loss (one batch): 0.01850
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02132, 0.02132, 0.14600, 0.08772, 188.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.02499, 0.02499, 0.15807, 0.09121, 166.06%
Time spent: 97.52s
- Epoch 005, ExpID 30506
Train - Loss (one batch): 0.00932
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02037, 0.02037, 0.14271, 0.08346, 169.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.02373, 0.02373, 0.15403, 0.08689, 147.21%
Time spent: 98.04s
- Epoch 006, ExpID 30506
Train - Loss (one batch): 0.01676
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01971, 0.01971, 0.14040, 0.08240, 156.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.02266, 0.02266, 0.15055, 0.08521, 136.74%
Time spent: 98.38s
- Epoch 007, ExpID 30506
Train - Loss (one batch): 0.02840
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01878, 0.01878, 0.13705, 0.08197, 177.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.02131, 0.02131, 0.14597, 0.08429, 157.53%
Time spent: 90.80s
- Epoch 008, ExpID 30506
Train - Loss (one batch): 0.01233
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01814, 0.01814, 0.13470, 0.07880, 161.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02053, 0.02053, 0.14327, 0.08084, 143.32%
Time spent: 91.16s
- Epoch 009, ExpID 30506
Train - Loss (one batch): 0.01040
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01815, 0.01815, 0.13471, 0.07925, 155.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02053, 0.02053, 0.14327, 0.08084, 143.32%
Time spent: 105.29s
- Epoch 010, ExpID 30506
Train - Loss (one batch): 0.01217
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01769, 0.01769, 0.13299, 0.07572, 132.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01999, 0.01999, 0.14140, 0.07757, 121.21%
Time spent: 133.40s
- Epoch 011, ExpID 30506
Train - Loss (one batch): 0.01419
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01761, 0.01761, 0.13270, 0.07901, 158.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01985, 0.01985, 0.14088, 0.08116, 143.83%
Time spent: 133.12s
- Epoch 012, ExpID 30506
Train - Loss (one batch): 0.00891
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01741, 0.01741, 0.13195, 0.07506, 121.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.01951, 0.01951, 0.13967, 0.07689, 111.91%
Time spent: 134.04s
- Epoch 013, ExpID 30506
Train - Loss (one batch): 0.01469
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01723, 0.01723, 0.13126, 0.07469, 135.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01931, 0.01931, 0.13895, 0.07652, 122.90%
Time spent: 103.10s
- Epoch 014, ExpID 30506
Train - Loss (one batch): 0.01008
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01728, 0.01728, 0.13146, 0.07408, 131.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01931, 0.01931, 0.13895, 0.07652, 122.90%
Time spent: 83.33s
- Epoch 015, ExpID 30506
Train - Loss (one batch): 0.01867
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01762, 0.01762, 0.13273, 0.07759, 145.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01931, 0.01931, 0.13895, 0.07652, 122.90%
Time spent: 82.76s
- Epoch 016, ExpID 30506
Train - Loss (one batch): 0.01415
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01714, 0.01714, 0.13093, 0.07423, 148.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01916, 0.01916, 0.13843, 0.07617, 135.86%
Time spent: 98.57s
- Epoch 017, ExpID 30506
Train - Loss (one batch): 0.00781
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01735, 0.01735, 0.13171, 0.07509, 123.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01916, 0.01916, 0.13843, 0.07617, 135.86%
Time spent: 81.93s
- Epoch 018, ExpID 30506
Train - Loss (one batch): 0.01723
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01733, 0.01733, 0.13164, 0.07599, 143.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01916, 0.01916, 0.13843, 0.07617, 135.86%
Time spent: 83.10s
- Epoch 019, ExpID 30506
Train - Loss (one batch): 0.01525
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01722, 0.01722, 0.13123, 0.07495, 143.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01916, 0.01916, 0.13843, 0.07617, 135.86%
Time spent: 82.49s
- Epoch 020, ExpID 30506
Train - Loss (one batch): 0.01619
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01713, 0.01713, 0.13087, 0.07363, 136.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.01922, 0.01922, 0.13863, 0.07552, 128.25%
Time spent: 99.69s
- Epoch 021, ExpID 30506
Train - Loss (one batch): 0.01168
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01706, 0.01706, 0.13062, 0.07383, 137.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01911, 0.01911, 0.13824, 0.07583, 129.02%
Time spent: 133.78s
- Epoch 022, ExpID 30506
Train - Loss (one batch): 0.01303
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01702, 0.01702, 0.13045, 0.07296, 123.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01905, 0.01905, 0.13801, 0.07509, 116.19%
Time spent: 137.47s
- Epoch 023, ExpID 30506
Train - Loss (one batch): 0.01184
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01712, 0.01712, 0.13085, 0.07453, 132.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01905, 0.01905, 0.13801, 0.07509, 116.19%
Time spent: 112.85s
- Epoch 024, ExpID 30506
Train - Loss (one batch): 0.01094
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01701, 0.01701, 0.13043, 0.07492, 134.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01912, 0.01912, 0.13827, 0.07717, 127.50%
Time spent: 132.24s
- Epoch 025, ExpID 30506
Train - Loss (one batch): 0.01795
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01693, 0.01693, 0.13012, 0.07256, 129.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01897, 0.01897, 0.13774, 0.07462, 120.56%
Time spent: 102.81s
- Epoch 026, ExpID 30506
Train - Loss (one batch): 0.01047
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01694, 0.01694, 0.13017, 0.07324, 142.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01897, 0.01897, 0.13774, 0.07462, 120.56%
Time spent: 110.98s
- Epoch 027, ExpID 30506
Train - Loss (one batch): 0.00877
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01685, 0.01685, 0.12981, 0.07407, 136.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.01902, 0.01902, 0.13790, 0.07613, 127.44%
Time spent: 132.85s
- Epoch 028, ExpID 30506
Train - Loss (one batch): 0.00991
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01675, 0.01675, 0.12944, 0.07348, 156.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01896, 0.01896, 0.13769, 0.07579, 145.93%
Time spent: 133.72s
- Epoch 029, ExpID 30506
Train - Loss (one batch): 0.00977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01687, 0.01687, 0.12988, 0.07272, 128.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01896, 0.01896, 0.13769, 0.07579, 145.93%
Time spent: 110.46s
- Epoch 030, ExpID 30506
Train - Loss (one batch): 0.00862
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01685, 0.01685, 0.12981, 0.07370, 145.95%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01896, 0.01896, 0.13769, 0.07579, 145.93%
Time spent: 108.60s
- Epoch 031, ExpID 30506
Train - Loss (one batch): 0.01354
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01684, 0.01684, 0.12976, 0.07297, 131.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01896, 0.01896, 0.13769, 0.07579, 145.93%
Time spent: 110.25s
- Epoch 032, ExpID 30506
Train - Loss (one batch): 0.01567
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01664, 0.01664, 0.12901, 0.07257, 144.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 134.64s
- Epoch 033, ExpID 30506
Train - Loss (one batch): 0.01144
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01688, 0.01688, 0.12991, 0.07493, 139.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 113.35s
- Epoch 034, ExpID 30506
Train - Loss (one batch): 0.01046
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01680, 0.01680, 0.12960, 0.07352, 134.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 112.48s
- Epoch 035, ExpID 30506
Train - Loss (one batch): 0.01263
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01684, 0.01684, 0.12976, 0.07221, 119.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 110.82s
- Epoch 036, ExpID 30506
Train - Loss (one batch): 0.01078
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01693, 0.01693, 0.13012, 0.07260, 117.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 112.16s
- Epoch 037, ExpID 30506
Train - Loss (one batch): 0.01409
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01674, 0.01674, 0.12938, 0.07292, 135.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 112.45s
- Epoch 038, ExpID 30506
Train - Loss (one batch): 0.01507
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01685, 0.01685, 0.12982, 0.07110, 114.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 108.20s
- Epoch 039, ExpID 30506
Train - Loss (one batch): 0.00889
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01727, 0.01727, 0.13142, 0.07830, 148.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 110.96s
- Epoch 040, ExpID 30506
Train - Loss (one batch): 0.01223
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01686, 0.01686, 0.12983, 0.07297, 125.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 136.38s
- Epoch 041, ExpID 30506
Train - Loss (one batch): 0.01170
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01671, 0.01671, 0.12928, 0.07171, 117.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 136.42s
- Epoch 042, ExpID 30506
Train - Loss (one batch): 0.00800
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01680, 0.01680, 0.12961, 0.07237, 114.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.01889, 0.01889, 0.13745, 0.07476, 134.03%
Time spent: 136.27s
Couldn't import umap
PID, device: 148893 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 440
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 14:01:05
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 32 --lr 1e-3 --patch_size 3 --stride 3 --nhead 1 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 4 --gpu 0 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=4, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='0', npatch=8, device=device(type='cuda', index=0), PID=201404, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 30207
Train - Loss (one batch): 0.01023
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02164, 0.02164, 0.14710, 0.09531, 231.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.02585, 0.02585, 0.16077, 0.09648, 212.63%
Time spent: 149.56s
- Epoch 001, ExpID 30207
Train - Loss (one batch): 0.01790
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01881, 0.01881, 0.13716, 0.08409, 190.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02283, 0.02283, 0.15110, 0.08492, 169.57%
Time spent: 148.24s
- Epoch 002, ExpID 30207
Train - Loss (one batch): 0.01224
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01859, 0.01859, 0.13633, 0.08084, 153.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02228, 0.02228, 0.14926, 0.08120, 135.89%
Time spent: 148.46s
- Epoch 003, ExpID 30207
Train - Loss (one batch): 0.01483
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01844, 0.01844, 0.13579, 0.08360, 172.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02172, 0.02172, 0.14738, 0.08387, 153.82%
Time spent: 147.81s
- Epoch 004, ExpID 30207
Train - Loss (one batch): 0.00839
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01804, 0.01804, 0.13432, 0.07923, 149.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.02120, 0.02120, 0.14561, 0.07921, 131.53%
Time spent: 147.74s
- Epoch 005, ExpID 30207
Train - Loss (one batch): 0.01581
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01804, 0.01804, 0.13431, 0.07738, 133.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.02073, 0.02073, 0.14399, 0.07724, 115.19%
Time spent: 148.26s
- Epoch 006, ExpID 30207
Train - Loss (one batch): 0.01600
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01782, 0.01782, 0.13349, 0.07748, 137.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.02009, 0.02009, 0.14175, 0.07742, 119.05%
Time spent: 135.99s
- Epoch 007, ExpID 30207
Train - Loss (one batch): 0.01257
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01786, 0.01786, 0.13366, 0.07554, 122.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.02009, 0.02009, 0.14175, 0.07742, 119.05%
Time spent: 113.69s
- Epoch 008, ExpID 30207
Train - Loss (one batch): 0.01950
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01774, 0.01774, 0.13319, 0.07670, 139.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01944, 0.01944, 0.13942, 0.07660, 119.05%
Time spent: 136.08s
- Epoch 009, ExpID 30207
Train - Loss (one batch): 0.01319
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01768, 0.01768, 0.13298, 0.07661, 135.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.01938, 0.01938, 0.13920, 0.07638, 120.69%
Time spent: 132.04s
- Epoch 010, ExpID 30207
Train - Loss (one batch): 0.01181
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01779, 0.01779, 0.13337, 0.07687, 138.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.01938, 0.01938, 0.13920, 0.07638, 120.69%
Time spent: 112.01s
- Epoch 011, ExpID 30207
Train - Loss (one batch): 0.01120
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01751, 0.01751, 0.13233, 0.07454, 138.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01904, 0.01904, 0.13800, 0.07445, 122.16%
Time spent: 134.15s
- Epoch 012, ExpID 30207
Train - Loss (one batch): 0.01056
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01745, 0.01745, 0.13210, 0.07462, 133.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.01899, 0.01899, 0.13780, 0.07463, 119.92%
Time spent: 132.94s
- Epoch 013, ExpID 30207
Train - Loss (one batch): 0.01303
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01741, 0.01741, 0.13196, 0.07538, 137.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01885, 0.01885, 0.13731, 0.07548, 126.75%
Time spent: 135.54s
- Epoch 014, ExpID 30207
Train - Loss (one batch): 0.01677
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01721, 0.01721, 0.13119, 0.07323, 126.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 14, 0.01866, 0.01866, 0.13661, 0.07345, 112.60%
Time spent: 134.71s
- Epoch 015, ExpID 30207
Train - Loss (one batch): 0.01004
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01731, 0.01731, 0.13158, 0.07453, 120.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 14, 0.01866, 0.01866, 0.13661, 0.07345, 112.60%
Time spent: 130.90s
- Epoch 016, ExpID 30207
Train - Loss (one batch): 0.01175
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01729, 0.01729, 0.13151, 0.07395, 113.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 14, 0.01866, 0.01866, 0.13661, 0.07345, 112.60%
Time spent: 216.45s
- Epoch 017, ExpID 30207
Train - Loss (one batch): 0.01085
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01821, 0.01821, 0.13493, 0.08052, 158.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 14, 0.01866, 0.01866, 0.13661, 0.07345, 112.60%
Time spent: 253.98s
- Epoch 018, ExpID 30207
Train - Loss (one batch): 0.00812
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01718, 0.01718, 0.13106, 0.07408, 127.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01861, 0.01861, 0.13642, 0.07473, 118.43%
Time spent: 308.65s
- Epoch 019, ExpID 30207
Train - Loss (one batch): 0.01062
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01723, 0.01723, 0.13125, 0.07475, 139.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01861, 0.01861, 0.13642, 0.07473, 118.43%
Time spent: 250.64s
- Epoch 020, ExpID 30207
Train - Loss (one batch): 0.00716
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01742, 0.01742, 0.13199, 0.07759, 151.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01861, 0.01861, 0.13642, 0.07473, 118.43%
Time spent: 247.28s
- Epoch 021, ExpID 30207
Train - Loss (one batch): 0.00811
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01715, 0.01715, 0.13097, 0.07557, 141.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01848, 0.01848, 0.13594, 0.07624, 132.63%
Time spent: 332.48s
- Epoch 022, ExpID 30207
Train - Loss (one batch): 0.01445
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01696, 0.01696, 0.13024, 0.07346, 127.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01813, 0.01813, 0.13466, 0.07380, 118.77%
Time spent: 285.82s
- Epoch 023, ExpID 30207
Train - Loss (one batch): 0.01799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01737, 0.01737, 0.13179, 0.07835, 163.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01813, 0.01813, 0.13466, 0.07380, 118.77%
Time spent: 194.33s
- Epoch 024, ExpID 30207
Train - Loss (one batch): 0.00637
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01705, 0.01705, 0.13057, 0.07369, 129.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01813, 0.01813, 0.13466, 0.07380, 118.77%
Time spent: 138.28s
- Epoch 025, ExpID 30207
Train - Loss (one batch): 0.01350
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01718, 0.01718, 0.13106, 0.07407, 131.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01813, 0.01813, 0.13466, 0.07380, 118.77%
Time spent: 226.02s
- Epoch 026, ExpID 30207
Train - Loss (one batch): 0.01715
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01692, 0.01692, 0.13009, 0.07246, 118.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.01801, 0.01801, 0.13419, 0.07309, 111.10%
Time spent: 325.26s
- Epoch 027, ExpID 30207
Train - Loss (one batch): 0.01508
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01686, 0.01686, 0.12983, 0.07361, 133.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.01809, 0.01809, 0.13450, 0.07437, 126.83%
Time spent: 318.68s
- Epoch 028, ExpID 30207
Train - Loss (one batch): 0.01983
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12964, 0.07296, 118.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01829, 0.01829, 0.13524, 0.07406, 112.00%
Time spent: 302.60s
- Epoch 029, ExpID 30207
Train - Loss (one batch): 0.01247
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01695, 0.01695, 0.13017, 0.07237, 114.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01829, 0.01829, 0.13524, 0.07406, 112.00%
Time spent: 241.29s
- Epoch 030, ExpID 30207
Train - Loss (one batch): 0.01411
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01705, 0.01705, 0.13059, 0.07330, 111.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01829, 0.01829, 0.13524, 0.07406, 112.00%
Time spent: 211.42s
- Epoch 031, ExpID 30207
Train - Loss (one batch): 0.01150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01688, 0.01688, 0.12992, 0.07360, 140.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01829, 0.01829, 0.13524, 0.07406, 112.00%
Time spent: 229.29s
- Epoch 032, ExpID 30207
Train - Loss (one batch): 0.00903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01697, 0.01697, 0.13028, 0.07324, 121.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 28, 0.01829, 0.01829, 0.13524, 0.07406, 112.00%
Time spent: 265.25s
- Epoch 033, ExpID 30207
Train - Loss (one batch): 0.01331
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01677, 0.01677, 0.12948, 0.07206, 118.04%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01806, 0.01806, 0.13439, 0.07315, 118.23%
Time spent: 329.03s
- Epoch 034, ExpID 30207
Train - Loss (one batch): 0.00751
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01705, 0.01705, 0.13057, 0.07307, 115.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01806, 0.01806, 0.13439, 0.07315, 118.23%
Time spent: 232.36s
- Epoch 035, ExpID 30207
Train - Loss (one batch): 0.00804
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01693, 0.01693, 0.13011, 0.07244, 118.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01806, 0.01806, 0.13439, 0.07315, 118.23%
Time spent: 267.84s
- Epoch 036, ExpID 30207
Train - Loss (one batch): 0.00962
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01682, 0.01682, 0.12968, 0.07278, 126.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01806, 0.01806, 0.13439, 0.07315, 118.23%
Time spent: 249.05s
- Epoch 037, ExpID 30207
Train - Loss (one batch): 0.01520
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01672, 0.01672, 0.12932, 0.07136, 103.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 37, 0.01798, 0.01798, 0.13408, 0.07263, 100.23%
Time spent: 293.11s
- Epoch 038, ExpID 30207
Train - Loss (one batch): 0.01355
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01712, 0.01712, 0.13086, 0.07213, 109.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 37, 0.01798, 0.01798, 0.13408, 0.07263, 100.23%
Time spent: 251.15s
- Epoch 039, ExpID 30207
Train - Loss (one batch): 0.01008
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01683, 0.01683, 0.12975, 0.07343, 130.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 37, 0.01798, 0.01798, 0.13408, 0.07263, 100.23%
Time spent: 251.54s
- Epoch 040, ExpID 30207
Train - Loss (one batch): 0.00799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01682, 0.01682, 0.12967, 0.07227, 111.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 37, 0.01798, 0.01798, 0.13408, 0.07263, 100.23%
Time spent: 266.12s
- Epoch 041, ExpID 30207
Train - Loss (one batch): 0.00996
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01679, 0.01679, 0.12957, 0.07276, 118.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 37, 0.01798, 0.01798, 0.13408, 0.07263, 100.23%
Time spent: 208.49s
- Epoch 042, ExpID 30207
Train - Loss (one batch): 0.01204
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01672, 0.01672, 0.12931, 0.07242, 114.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 42, 0.01811, 0.01811, 0.13459, 0.07390, 110.96%
Time spent: 257.12s
- Epoch 043, ExpID 30207
Train - Loss (one batch): 0.01143
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01687, 0.01687, 0.12989, 0.07327, 136.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 42, 0.01811, 0.01811, 0.13459, 0.07390, 110.96%
Time spent: 111.92s
- Epoch 044, ExpID 30207
Train - Loss (one batch): 0.01956
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01692, 0.01692, 0.13006, 0.07211, 107.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 42, 0.01811, 0.01811, 0.13459, 0.07390, 110.96%
Time spent: 121.01s
- Epoch 045, ExpID 30207
Train - Loss (one batch): 0.00712
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01671, 0.01671, 0.12927, 0.07194, 111.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 148.16s
- Epoch 046, ExpID 30207
Train - Loss (one batch): 0.01431
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01683, 0.01683, 0.12973, 0.07215, 105.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 131.29s
- Epoch 047, ExpID 30207
Train - Loss (one batch): 0.01119
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01690, 0.01690, 0.13000, 0.07174, 106.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 135.30s
- Epoch 048, ExpID 30207
Train - Loss (one batch): 0.01063
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01702, 0.01702, 0.13047, 0.07414, 130.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 122.77s
- Epoch 049, ExpID 30207
Train - Loss (one batch): 0.01296
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01689, 0.01689, 0.12995, 0.07163, 112.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 123.76s
- Epoch 050, ExpID 30207
Train - Loss (one batch): 0.01342
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01706, 0.01706, 0.13060, 0.07278, 124.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 117.89s
- Epoch 051, ExpID 30207
Train - Loss (one batch): 0.01591
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01675, 0.01675, 0.12941, 0.07289, 119.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 138.45s
- Epoch 052, ExpID 30207
Train - Loss (one batch): 0.01009
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01680, 0.01680, 0.12961, 0.07173, 118.40%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 152.20s
- Epoch 053, ExpID 30207
Train - Loss (one batch): 0.00601
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01675, 0.01675, 0.12942, 0.07166, 109.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 45, 0.01827, 0.01827, 0.13518, 0.07345, 104.23%
Time spent: 131.89s
- Epoch 054, ExpID 30207
Train - Loss (one batch): 0.01037
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01668, 0.01668, 0.12917, 0.07241, 113.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 54, 0.01827, 0.01827, 0.13518, 0.07372, 103.55%
Time spent: 262.68s
- Epoch 055, ExpID 30207
Train - Loss (one batch): 0.01143
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01685, 0.01685, 0.12980, 0.07105, 113.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 54, 0.01827, 0.01827, 0.13518, 0.07372, 103.55%
Time spent: 311.50s
- Epoch 056, ExpID 30207
Train - Loss (one batch): 0.00964
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01668, 0.01668, 0.12916, 0.07300, 130.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 403.16s
- Epoch 057, ExpID 30207
Train - Loss (one batch): 0.01071
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01712, 0.01712, 0.13085, 0.07427, 128.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 354.45s
- Epoch 058, ExpID 30207
Train - Loss (one batch): 0.01399
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01690, 0.01690, 0.13001, 0.07262, 116.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 302.47s
- Epoch 059, ExpID 30207
Train - Loss (one batch): 0.01428
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01711, 0.01711, 0.13082, 0.07593, 145.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 405.25s
- Epoch 060, ExpID 30207
Train - Loss (one batch): 0.01221
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01676, 0.01676, 0.12946, 0.07175, 106.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 413.30s
- Epoch 061, ExpID 30207
Train - Loss (one batch): 0.01457
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01709, 0.01709, 0.13072, 0.07366, 126.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 56, 0.01843, 0.01843, 0.13576, 0.07475, 118.34%
Time spent: 419.59s
- Epoch 062, ExpID 30207
Train - Loss (one batch): 0.01453
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01657, 0.01657, 0.12872, 0.07184, 120.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 62, 0.01818, 0.01818, 0.13483, 0.07356, 108.98%
Time spent: 422.17s
- Epoch 063, ExpID 30207
Train - Loss (one batch): 0.01011
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01671, 0.01671, 0.12926, 0.07232, 124.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 62, 0.01818, 0.01818, 0.13483, 0.07356, 108.98%
Time spent: 247.37s
- Epoch 064, ExpID 30207
Train - Loss (one batch): 0.01107
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01667, 0.01667, 0.12912, 0.07070, 114.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 62, 0.01818, 0.01818, 0.13483, 0.07356, 108.98%
Time spent: 163.00s
- Epoch 065, ExpID 30207
Train - Loss (one batch): 0.01244
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01689, 0.01689, 0.12995, 0.07448, 137.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 62, 0.01818, 0.01818, 0.13483, 0.07356, 108.98%
Time spent: 159.42s
- Epoch 066, ExpID 30207
Train - Loss (one batch): 0.01073
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01673, 0.01673, 0.12935, 0.07141, 116.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 62, 0.01818, 0.01818, 0.13483, 0.07356, 108.98%
Time spent: 154.87s
- Epoch 067, ExpID 30207
Train - Loss (one batch): 0.01109
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01657, 0.01657, 0.12872, 0.07054, 104.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 67, 0.01820, 0.01820, 0.13490, 0.07250, 97.60%
Time spent: 198.44s
- Epoch 068, ExpID 30207
Train - Loss (one batch): 0.01324
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01666, 0.01666, 0.12907, 0.07238, 122.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 67, 0.01820, 0.01820, 0.13490, 0.07250, 97.60%
Time spent: 154.71s
- Epoch 069, ExpID 30207
Train - Loss (one batch): 0.01155
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01654, 0.01654, 0.12860, 0.07211, 127.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 193.64s
- Epoch 070, ExpID 30207
Train - Loss (one batch): 0.02060
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01699, 0.01699, 0.13035, 0.07519, 142.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 162.25s
- Epoch 071, ExpID 30207
Train - Loss (one batch): 0.01265
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01683, 0.01683, 0.12973, 0.07173, 102.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 122.62s
- Epoch 072, ExpID 30207
Train - Loss (one batch): 0.00934
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01700, 0.01700, 0.13038, 0.07382, 130.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 106.22s
- Epoch 073, ExpID 30207
Train - Loss (one batch): 0.01436
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01667, 0.01667, 0.12909, 0.07172, 117.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 151.00s
- Epoch 074, ExpID 30207
Train - Loss (one batch): 0.01416
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01662, 0.01662, 0.12893, 0.07179, 121.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 165.98s
- Epoch 075, ExpID 30207
Train - Loss (one batch): 0.00876
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01666, 0.01666, 0.12909, 0.07253, 127.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 154.19s
- Epoch 076, ExpID 30207
Train - Loss (one batch): 0.00903
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01668, 0.01668, 0.12915, 0.07231, 120.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 162.64s
- Epoch 077, ExpID 30207
Train - Loss (one batch): 0.01460
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01675, 0.01675, 0.12943, 0.07193, 124.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 164.30s
- Epoch 078, ExpID 30207
Train - Loss (one batch): 0.01530
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01677, 0.01677, 0.12948, 0.07288, 127.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 157.74s
- Epoch 079, ExpID 30207
Train - Loss (one batch): 0.01035
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01660, 0.01660, 0.12883, 0.07069, 103.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 69, 0.01827, 0.01827, 0.13517, 0.07383, 117.52%
Time spent: 169.32s
Couldn't import umap
PID, device: 201404 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 440
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 18:42:14
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 32 --lr 1e-3 --patch_size 3 --stride 3 --nhead 1 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 5 --gpu 0 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=5, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='0', npatch=8, device=device(type='cuda', index=0), PID=714961, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 92457
Train - Loss (one batch): 0.01770
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02809, 0.02809, 0.16761, 0.11440, 320.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.03098, 0.03098, 0.17601, 0.11711, 290.31%
Time spent: 193.30s
- Epoch 001, ExpID 92457
Train - Loss (one batch): 0.01257
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02021, 0.02021, 0.14217, 0.08631, 148.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02348, 0.02348, 0.15322, 0.08897, 145.44%
Time spent: 200.69s
- Epoch 002, ExpID 92457
Train - Loss (one batch): 0.01075
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01869, 0.01869, 0.13672, 0.08240, 171.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02194, 0.02194, 0.14812, 0.08501, 157.10%
Time spent: 196.04s
- Epoch 003, ExpID 92457
Train - Loss (one batch): 0.01656
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01854, 0.01854, 0.13618, 0.08112, 162.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02115, 0.02115, 0.14542, 0.08278, 142.78%
Time spent: 197.24s
- Epoch 004, ExpID 92457
Train - Loss (one batch): 0.01277
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01874, 0.01874, 0.13690, 0.07886, 134.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02115, 0.02115, 0.14542, 0.08278, 142.78%
Time spent: 193.64s
- Epoch 005, ExpID 92457
Train - Loss (one batch): 0.01894
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01834, 0.01834, 0.13542, 0.07747, 123.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01972, 0.01972, 0.14042, 0.07869, 114.20%
Time spent: 561.14s
- Epoch 006, ExpID 92457
Train - Loss (one batch): 0.01328
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01811, 0.01811, 0.13456, 0.07591, 123.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.01955, 0.01955, 0.13983, 0.07715, 117.64%
Time spent: 540.48s
- Epoch 007, ExpID 92457
Train - Loss (one batch): 0.01253
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01819, 0.01819, 0.13487, 0.07894, 164.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.01955, 0.01955, 0.13983, 0.07715, 117.64%
Time spent: 156.39s
- Epoch 008, ExpID 92457
Train - Loss (one batch): 0.01385
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01803, 0.01803, 0.13428, 0.07676, 121.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01945, 0.01945, 0.13948, 0.07827, 120.21%
Time spent: 189.50s
- Epoch 009, ExpID 92457
Train - Loss (one batch): 0.01294
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01803, 0.01803, 0.13427, 0.07439, 122.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.01915, 0.01915, 0.13839, 0.07544, 120.91%
Time spent: 194.64s
- Epoch 010, ExpID 92457
Train - Loss (one batch): 0.01626
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01782, 0.01782, 0.13350, 0.07837, 145.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01960, 0.01960, 0.14000, 0.07998, 142.91%
Time spent: 190.09s
- Epoch 011, ExpID 92457
Train - Loss (one batch): 0.00858
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01780, 0.01780, 0.13340, 0.07507, 119.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01927, 0.01927, 0.13882, 0.07642, 116.09%
Time spent: 225.08s
- Epoch 012, ExpID 92457
Train - Loss (one batch): 0.01517
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01775, 0.01775, 0.13324, 0.07562, 130.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.01957, 0.01957, 0.13989, 0.07724, 123.22%
Time spent: 209.71s
- Epoch 013, ExpID 92457
Train - Loss (one batch): 0.01382
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01761, 0.01761, 0.13269, 0.07388, 122.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01909, 0.01909, 0.13816, 0.07521, 118.63%
Time spent: 200.98s
- Epoch 014, ExpID 92457
Train - Loss (one batch): 0.01128
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01790, 0.01790, 0.13380, 0.07644, 141.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01909, 0.01909, 0.13816, 0.07521, 118.63%
Time spent: 154.39s
- Epoch 015, ExpID 92457
Train - Loss (one batch): 0.00895
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01758, 0.01758, 0.13259, 0.07643, 171.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 15, 0.01954, 0.01954, 0.13979, 0.07816, 159.37%
Time spent: 170.57s
- Epoch 016, ExpID 92457
Train - Loss (one batch): 0.01537
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01754, 0.01754, 0.13243, 0.07370, 115.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01917, 0.01917, 0.13847, 0.07508, 111.48%
Time spent: 132.54s
- Epoch 017, ExpID 92457
Train - Loss (one batch): 0.01611
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01764, 0.01764, 0.13282, 0.07603, 147.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01917, 0.01917, 0.13847, 0.07508, 111.48%
Time spent: 138.80s
- Epoch 018, ExpID 92457
Train - Loss (one batch): 0.00679
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01771, 0.01771, 0.13309, 0.07459, 133.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01917, 0.01917, 0.13847, 0.07508, 111.48%
Time spent: 155.68s
- Epoch 019, ExpID 92457
Train - Loss (one batch): 0.01150
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01757, 0.01757, 0.13256, 0.07421, 123.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01917, 0.01917, 0.13847, 0.07508, 111.48%
Time spent: 108.83s
- Epoch 020, ExpID 92457
Train - Loss (one batch): 0.01225
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01745, 0.01745, 0.13211, 0.07565, 140.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.01923, 0.01923, 0.13868, 0.07729, 135.30%
Time spent: 154.68s
- Epoch 021, ExpID 92457
Train - Loss (one batch): 0.00628
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01729, 0.01729, 0.13148, 0.07384, 134.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01898, 0.01898, 0.13778, 0.07512, 131.85%
Time spent: 199.09s
- Epoch 022, ExpID 92457
Train - Loss (one batch): 0.00893
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01771, 0.01771, 0.13309, 0.07724, 151.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01898, 0.01898, 0.13778, 0.07512, 131.85%
Time spent: 160.18s
- Epoch 023, ExpID 92457
Train - Loss (one batch): 0.01491
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01740, 0.01740, 0.13190, 0.07477, 123.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01898, 0.01898, 0.13778, 0.07512, 131.85%
Time spent: 166.28s
- Epoch 024, ExpID 92457
Train - Loss (one batch): 0.00825
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01714, 0.01714, 0.13091, 0.07421, 131.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 205.37s
- Epoch 025, ExpID 92457
Train - Loss (one batch): 0.01029
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01754, 0.01754, 0.13245, 0.07621, 125.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 517.49s
- Epoch 026, ExpID 92457
Train - Loss (one batch): 0.01032
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01759, 0.01759, 0.13263, 0.07392, 119.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 517.59s
- Epoch 027, ExpID 92457
Train - Loss (one batch): 0.01154
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01733, 0.01733, 0.13163, 0.07437, 129.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 556.64s
- Epoch 028, ExpID 92457
Train - Loss (one batch): 0.01208
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01736, 0.01736, 0.13175, 0.07295, 113.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 617.54s
- Epoch 029, ExpID 92457
Train - Loss (one batch): 0.00919
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01746, 0.01746, 0.13214, 0.07666, 154.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 24, 0.01888, 0.01888, 0.13741, 0.07543, 127.05%
Time spent: 567.29s
