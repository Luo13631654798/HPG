nohup: ignoring input
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 14:37:19
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 16 --lr 1e-3 --patch_size 3 --stride 3 --nhead 2 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 1 --gpu 2 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=16, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=2, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='2', npatch=8, device=device(type='cuda', index=0), PID=228549, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 43469
Train - Loss (one batch): 0.01057
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02331, 0.02331, 0.15267, 0.09443, 227.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.02559, 0.02559, 0.15998, 0.09632, 190.18%
Time spent: 488.05s
- Epoch 001, ExpID 43469
Train - Loss (one batch): 0.00940
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01994, 0.01994, 0.14123, 0.08149, 183.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02219, 0.02219, 0.14896, 0.08350, 153.50%
Time spent: 480.07s
- Epoch 002, ExpID 43469
Train - Loss (one batch): 0.01593
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01911, 0.01911, 0.13826, 0.07841, 164.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02109, 0.02109, 0.14521, 0.08030, 137.16%
Time spent: 489.77s
- Epoch 003, ExpID 43469
Train - Loss (one batch): 0.01729
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01866, 0.01866, 0.13659, 0.07885, 189.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02063, 0.02063, 0.14364, 0.08094, 163.82%
Time spent: 509.81s
- Epoch 004, ExpID 43469
Train - Loss (one batch): 0.01347
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01814, 0.01814, 0.13467, 0.07840, 175.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.02013, 0.02013, 0.14187, 0.08017, 157.72%
Time spent: 307.51s
- Epoch 005, ExpID 43469
Train - Loss (one batch): 0.00879
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01783, 0.01783, 0.13354, 0.07502, 145.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01954, 0.01954, 0.13980, 0.07667, 130.26%
Time spent: 491.72s
- Epoch 006, ExpID 43469
Train - Loss (one batch): 0.00837
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01786, 0.01786, 0.13364, 0.07625, 146.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01954, 0.01954, 0.13980, 0.07667, 130.26%
Time spent: 413.23s
- Epoch 007, ExpID 43469
Train - Loss (one batch): 0.01922
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01744, 0.01744, 0.13205, 0.07418, 146.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.01896, 0.01896, 0.13769, 0.07576, 131.71%
Time spent: 487.01s
- Epoch 008, ExpID 43469
Train - Loss (one batch): 0.01105
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01735, 0.01735, 0.13170, 0.07354, 130.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.01888, 0.01888, 0.13739, 0.07515, 118.61%
Time spent: 492.79s
- Epoch 009, ExpID 43469
Train - Loss (one batch): 0.02627
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01714, 0.01714, 0.13092, 0.07332, 132.06%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.01872, 0.01872, 0.13681, 0.07504, 122.09%
Time spent: 494.68s
- Epoch 010, ExpID 43469
Train - Loss (one batch): 0.00221
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01700, 0.01700, 0.13037, 0.07357, 129.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01868, 0.01868, 0.13669, 0.07549, 118.60%
Time spent: 485.09s
- Epoch 011, ExpID 43469
Train - Loss (one batch): 0.00756
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01698, 0.01698, 0.13031, 0.07210, 118.18%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01880, 0.01880, 0.13711, 0.07399, 110.55%
Time spent: 480.22s
- Epoch 012, ExpID 43469
Train - Loss (one batch): 0.01296
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01709, 0.01709, 0.13072, 0.07300, 123.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.01880, 0.01880, 0.13711, 0.07399, 110.55%
Time spent: 391.15s
- Epoch 013, ExpID 43469
Train - Loss (one batch): 0.02080
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01688, 0.01688, 0.12991, 0.07292, 116.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01866, 0.01866, 0.13662, 0.07476, 108.20%
Time spent: 482.84s
- Epoch 014, ExpID 43469
Train - Loss (one batch): 0.00459
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01709, 0.01709, 0.13074, 0.07337, 119.60%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01866, 0.01866, 0.13662, 0.07476, 108.20%
Time spent: 398.60s
- Epoch 015, ExpID 43469
Train - Loss (one batch): 0.01692
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01689, 0.01689, 0.12997, 0.07217, 108.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01866, 0.01866, 0.13662, 0.07476, 108.20%
Time spent: 244.47s
- Epoch 016, ExpID 43469
Train - Loss (one batch): 0.00569
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01689, 0.01689, 0.12995, 0.07363, 131.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01866, 0.01866, 0.13662, 0.07476, 108.20%
Time spent: 225.05s
- Epoch 017, ExpID 43469
Train - Loss (one batch): 0.00828
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12964, 0.07086, 95.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 17, 0.01902, 0.01902, 0.13792, 0.07309, 92.82%
Time spent: 250.66s
- Epoch 018, ExpID 43469
Train - Loss (one batch): 0.02563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01699, 0.01699, 0.13033, 0.07496, 127.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 17, 0.01902, 0.01902, 0.13792, 0.07309, 92.82%
Time spent: 222.02s
- Epoch 019, ExpID 43469
Train - Loss (one batch): 0.01391
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01677, 0.01677, 0.12950, 0.07143, 105.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 19, 0.01889, 0.01889, 0.13744, 0.07364, 100.05%
Time spent: 266.26s
- Epoch 020, ExpID 43469
Train - Loss (one batch): 0.01361
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01672, 0.01672, 0.12930, 0.07157, 112.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.01887, 0.01887, 0.13736, 0.07378, 108.02%
Time spent: 250.07s
- Epoch 021, ExpID 43469
Train - Loss (one batch): 0.00680
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01669, 0.01669, 0.12921, 0.07240, 123.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 431.98s
- Epoch 022, ExpID 43469
Train - Loss (one batch): 0.00994
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01709, 0.01709, 0.13073, 0.07368, 107.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 443.00s
- Epoch 023, ExpID 43469
Train - Loss (one batch): 0.00920
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12965, 0.07173, 105.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 470.95s
- Epoch 024, ExpID 43469
Train - Loss (one batch): 0.00986
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01690, 0.01690, 0.13000, 0.07308, 108.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 546.59s
- Epoch 025, ExpID 43469
Train - Loss (one batch): 0.00958
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01671, 0.01671, 0.12927, 0.07213, 118.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 679.26s
- Epoch 026, ExpID 43469
Train - Loss (one batch): 0.00727
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01676, 0.01676, 0.12946, 0.07156, 103.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 662.41s
- Epoch 027, ExpID 43469
Train - Loss (one batch): 0.01392
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12964, 0.07192, 103.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 411.98s
- Epoch 028, ExpID 43469
Train - Loss (one batch): 0.02114
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01674, 0.01674, 0.12938, 0.07216, 110.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 290.20s
- Epoch 029, ExpID 43469
Train - Loss (one batch): 0.00905
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01673, 0.01673, 0.12934, 0.07323, 125.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 293.17s
- Epoch 030, ExpID 43469
Train - Loss (one batch): 0.00929
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01681, 0.01681, 0.12966, 0.07116, 96.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 286.53s
- Epoch 031, ExpID 43469
Train - Loss (one batch): 0.01600
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01712, 0.01712, 0.13086, 0.07473, 113.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 21, 0.01875, 0.01875, 0.13691, 0.07452, 120.05%
Time spent: 299.65s
Couldn't import umap
PID, device: 228549 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 880
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 18:20:17
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 16 --lr 1e-3 --patch_size 3 --stride 3 --nhead 2 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 2 --gpu 2 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=16, viz=False, save='experiments/', load=None, seed=2, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=2, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='2', npatch=8, device=device(type='cuda', index=0), PID=701322, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 20620
Train - Loss (one batch): 0.01932
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02610, 0.02610, 0.16157, 0.10141, 272.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.03001, 0.03001, 0.17322, 0.10559, 244.97%
Time spent: 346.42s
- Epoch 001, ExpID 20620
Train - Loss (one batch): 0.01796
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01914, 0.01914, 0.13834, 0.08740, 192.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.02293, 0.02293, 0.15144, 0.09256, 173.81%
Time spent: 344.28s
- Epoch 002, ExpID 20620
Train - Loss (one batch): 0.01775
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01738, 0.01738, 0.13183, 0.08128, 139.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02105, 0.02105, 0.14508, 0.08621, 131.72%
Time spent: 336.62s
- Epoch 003, ExpID 20620
Train - Loss (one batch): 0.01518
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01678, 0.01678, 0.12953, 0.08057, 137.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02010, 0.02010, 0.14177, 0.08509, 129.70%
Time spent: 245.43s
- Epoch 004, ExpID 20620
Train - Loss (one batch): 0.01034
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01632, 0.01632, 0.12776, 0.07515, 116.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.01948, 0.01948, 0.13957, 0.07921, 111.05%
Time spent: 330.90s
- Epoch 005, ExpID 20620
Train - Loss (one batch): 0.00977
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01627, 0.01627, 0.12755, 0.07469, 105.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 350.27s
- Epoch 006, ExpID 20620
Train - Loss (one batch): 0.01599
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01675, 0.01675, 0.12940, 0.07559, 130.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 290.29s
- Epoch 007, ExpID 20620
Train - Loss (one batch): 0.02563
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01632, 0.01632, 0.12774, 0.07398, 120.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 339.97s
- Epoch 008, ExpID 20620
Train - Loss (one batch): 0.01308
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01646, 0.01646, 0.12829, 0.07391, 119.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 371.49s
- Epoch 009, ExpID 20620
Train - Loss (one batch): 0.00607
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01652, 0.01652, 0.12852, 0.07613, 135.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 335.65s
- Epoch 010, ExpID 20620
Train - Loss (one batch): 0.01369
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01707, 0.01707, 0.13065, 0.07632, 112.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 300.88s
- Epoch 011, ExpID 20620
Train - Loss (one batch): 0.00777
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01704, 0.01704, 0.13054, 0.07165, 100.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 277.85s
- Epoch 012, ExpID 20620
Train - Loss (one batch): 0.01503
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01693, 0.01693, 0.13013, 0.07310, 110.28%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 283.91s
- Epoch 013, ExpID 20620
Train - Loss (one batch): 0.00907
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01687, 0.01687, 0.12990, 0.07168, 104.46%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 319.93s
- Epoch 014, ExpID 20620
Train - Loss (one batch): 0.00989
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01670, 0.01670, 0.12923, 0.07152, 103.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 299.99s
- Epoch 015, ExpID 20620
Train - Loss (one batch): 0.00338
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01649, 0.01649, 0.12843, 0.07444, 141.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.01924, 0.01924, 0.13869, 0.07853, 100.23%
Time spent: 269.73s
Couldn't import umap
PID, device: 701322 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model BaselineHPG_v2(
  (gcs): ModuleList(
    (0): BaselineGTrans
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=7, bias=True)
  (obs_enc): Linear(in_features=1, out_features=8, bias=True)
  (relu): ReLU()
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=8, out_features=8, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=8, out_features=1, bias=True)
  )
)
parameters: 3817
n_train_batches: 880
Exp has been early stopped!
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_models.py
2024-07-22 19:47:52
run_models.py --dataset mimic --state def --history 24 --patience 10 --batch_size 16 --lr 1e-3 --patch_size 3 --stride 3 --nhead 2 --tf_layer 1 --nlayer 1 --hid_dim 8 --outlayer Linear --seed 3 --gpu 2 --alpha 1
Namespace(state='def', n=100000000, epoch=1000, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=16, viz=False, save='experiments/', load=None, seed=3, dataset='mimic', quantization=0.0, model='tPatchGNN', outlayer='Linear', hop=1, nhead=2, tf_layer=1, nlayer=1, patch_size=3.0, stride=3.0, hid_dim=8, te_dim=10, node_dim=10, alpha=1.0, res=1, gpu='2', npatch=8, device=device(type='cuda', index=0), PID=785403, pred_window=24, ndim=96, patch_layer=4, scale_patch_size=0.0625)
- Epoch 000, ExpID 20050
Train - Loss (one batch): 0.02478
Val - Loss, MSE, RMSE, MAE, MAPE: 0.03370, 0.03370, 0.18358, 0.11457, 268.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.03868, 0.03868, 0.19667, 0.11937, 236.92%
Time spent: 308.05s
- Epoch 001, ExpID 20050
Train - Loss (one batch): 0.00908
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02790, 0.02790, 0.16702, 0.09694, 199.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.03267, 0.03267, 0.18075, 0.10123, 171.34%
Time spent: 273.26s
- Epoch 002, ExpID 20050
Train - Loss (one batch): 0.01546
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02522, 0.02522, 0.15882, 0.09409, 208.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.02940, 0.02940, 0.17148, 0.09821, 180.08%
Time spent: 354.06s
- Epoch 003, ExpID 20050
Train - Loss (one batch): 0.00852
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02297, 0.02297, 0.15157, 0.08698, 165.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.02661, 0.02661, 0.16312, 0.09043, 139.12%
Time spent: 350.28s
- Epoch 004, ExpID 20050
Train - Loss (one batch): 0.00724
Val - Loss, MSE, RMSE, MAE, MAPE: 0.02031, 0.02031, 0.14253, 0.08345, 160.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.02331, 0.02331, 0.15268, 0.08660, 148.18%
Time spent: 390.13s
- Epoch 005, ExpID 20050
Train - Loss (one batch): 0.00932
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01951, 0.01951, 0.13966, 0.08106, 162.27%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.02230, 0.02230, 0.14935, 0.08387, 137.24%
Time spent: 470.78s
- Epoch 006, ExpID 20050
Train - Loss (one batch): 0.00506
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01873, 0.01873, 0.13684, 0.07939, 152.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.02161, 0.02161, 0.14699, 0.08255, 134.12%
Time spent: 412.83s
- Epoch 007, ExpID 20050
Train - Loss (one batch): 0.02797
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01839, 0.01839, 0.13560, 0.08078, 165.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.02105, 0.02105, 0.14508, 0.08348, 141.41%
Time spent: 448.23s
- Epoch 008, ExpID 20050
Train - Loss (one batch): 0.01337
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01805, 0.01805, 0.13435, 0.07611, 155.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02050, 0.02050, 0.14319, 0.07827, 126.85%
Time spent: 425.05s
- Epoch 009, ExpID 20050
Train - Loss (one batch): 0.01282
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01808, 0.01808, 0.13447, 0.08270, 181.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.02050, 0.02050, 0.14319, 0.07827, 126.85%
Time spent: 476.43s
