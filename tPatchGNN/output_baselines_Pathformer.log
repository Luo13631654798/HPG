nohup: ignoring input
True
Couldn't import umap
PID, device: 196977 cuda:0
Total records: 25
Dataset n_samples: 25 15 5 5
Test record ids (first 20): ['B04', 'D02', 'A01', 'E04', 'C02']
Test record ids (last 20): ['B04', 'D02', 'A01', 'E04', 'C02']
data_max: tensor([5.6217, 3.9067, 2.4465, 5.3456, 3.9414, 2.2163, 5.7582, 3.9781, 2.6061,
        5.3913, 3.9654, 2.6011], device='cuda:0')
data_min: tensor([ 0.1852, -0.1228, -1.4615,  0.1606, -0.4760, -2.5436, -0.2787, -0.4944,
        -2.0269,  0.0198, -0.4789, -1.9588], device='cuda:0')
time_max: tensor(304861., device='cuda:0')
manual set time_max: tensor(4000)
Dataset n_samples after time split: 5400 3318 1007 1075
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=12, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-1): 2 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=2, out_features=98, bias=True)
          (emb_linear): Linear(in_features=784, out_features=784, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=784, out_features=784, bias=True)
            (W_K): Linear(in_features=784, out_features=784, bias=True)
            (W_V): Linear(in_features=784, out_features=784, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=784, out_features=784, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-6): 7 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=7, out_features=98, bias=True)
          (emb_linear): Linear(in_features=224, out_features=224, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=224, out_features=224, bias=True)
            (W_K): Linear(in_features=224, out_features=224, bias=True)
            (W_V): Linear(in_features=224, out_features=224, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=224, out_features=224, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (2): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-13): 14 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=14, out_features=98, bias=True)
          (emb_linear): Linear(in_features=112, out_features=112, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=112, out_features=112, bias=True)
            (W_K): Linear(in_features=112, out_features=112, bias=True)
            (W_V): Linear(in_features=112, out_features=112, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=112, out_features=112, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (3): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-48): 49 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=49, out_features=98, bias=True)
          (emb_linear): Linear(in_features=32, out_features=32, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=32, out_features=32, bias=True)
            (W_K): Linear(in_features=32, out_features=32, bias=True)
            (W_V): Linear(in_features=32, out_features=32, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(98, 98, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 13:47:45
run_baselines.py --history 3000 --model Pathformer --dataset activity --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=3000, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='activity', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=98, top_k=5, num_kernels=64, npatch=125, device=device(type='cuda', index=0), PID=196977, pred_window=1000, ndim=12, patch_layer=8, patch_size_list=[49, 14, 7, 2], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=12, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
- Epoch 000, ExpID 96303
Train - Loss (one batch): 0.01683
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01582, 0.01582, 0.12579, 0.09592, 25.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.01489, 0.01489, 0.12201, 0.09371, 22.56%
Time spent: 145.28s
- Epoch 001, ExpID 96303
Train - Loss (one batch): 0.01436
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01467, 0.01467, 0.12114, 0.09211, 24.11%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.01406, 0.01406, 0.11859, 0.09088, 21.25%
Time spent: 145.30s
- Epoch 002, ExpID 96303
Train - Loss (one batch): 0.01517
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01410, 0.01410, 0.11874, 0.08943, 23.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.01333, 0.01333, 0.11545, 0.08799, 20.54%
Time spent: 144.20s

parameters: 10589252
n_train_batches: 104
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 103, in forward
    gates, load = self.noisy_top_k_gating(new_x, self.training)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 94, in noisy_top_k_gating
    load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 61, in _prob_in_top_k
    prob_if_in = normal.cdf((clean_values - threshold_if_in) / noise_stddev)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/normal.py", line 93, in cdf
    self._validate_sample(value)
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py", line 312, in _validate_sample
    raise ValueError(
ValueError: Expected value argument (Tensor of shape (32, 4)) to be within the support (Real()) of the distribution Normal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')), but found invalid values:
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 13:56:06
run_baselines.py --history 2000 --model Pathformer --dataset activity --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=2000, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='activity', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=65, top_k=5, num_kernels=64, npatch=84, device=device(type='cuda', index=0), PID=202347, pred_window=2000, ndim=12, patch_layer=8, patch_size_list=[13, 5], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=12, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
True
Couldn't import umap
PID, device: 202347 cuda:0
Total records: 25
Dataset n_samples: 25 15 5 5
Test record ids (first 20): ['B04', 'D02', 'A01', 'E04', 'C02']
Test record ids (last 20): ['B04', 'D02', 'A01', 'E04', 'C02']
data_max: tensor([5.6217, 3.9067, 2.4465, 5.3456, 3.9414, 2.2163, 5.7582, 3.9781, 2.6061,
        5.3913, 3.9654, 2.6011], device='cuda:0')
data_min: tensor([ 0.1852, -0.1228, -1.4615,  0.1606, -0.4760, -2.5436, -0.2787, -0.4944,
        -2.0269,  0.0198, -0.4789, -1.9588], device='cuda:0')
time_max: tensor(304861., device='cuda:0')
manual set time_max: tensor(4000)
Dataset n_samples after time split: 2719 1671 507 541
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=12, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-4): 5 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=5, out_features=65, bias=True)
          (emb_linear): Linear(in_features=208, out_features=208, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=208, out_features=208, bias=True)
            (W_K): Linear(in_features=208, out_features=208, bias=True)
            (W_V): Linear(in_features=208, out_features=208, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=208, out_features=208, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-12): 13 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=13, out_features=65, bias=True)
          (emb_linear): Linear(in_features=80, out_features=80, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=80, out_features=80, bias=True)
            (W_K): Linear(in_features=80, out_features=80, bias=True)
            (W_V): Linear(in_features=80, out_features=80, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=80, out_features=80, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(65, 65, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)
parameters: 910940
n_train_batches: 53
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 2 is out of range
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 13:56:11
run_baselines.py --history 1000 --model Pathformer --dataset activity --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=1000, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='activity', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=34, top_k=5, num_kernels=64, npatch=42, device=device(type='cuda', index=0), PID=202598, pred_window=3000, ndim=12, patch_layer=7, patch_size_list=[17, 2], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=12, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
True
Couldn't import umap
PID, device: 202598 cuda:0
Total records: 25
Dataset n_samples: 25 15 5 5
Test record ids (first 20): ['B04', 'D02', 'A01', 'E04', 'C02']
Test record ids (last 20): ['B04', 'D02', 'A01', 'E04', 'C02']
data_max: tensor([5.6217, 3.9067, 2.4465, 5.3456, 3.9414, 2.2163, 5.7582, 3.9781, 2.6061,
        5.3913, 3.9654, 2.6011], device='cuda:0')
data_min: tensor([ 0.1852, -0.1228, -1.4615,  0.1606, -0.4760, -2.5436, -0.2787, -0.4944,
        -2.0269,  0.0198, -0.4789, -1.9588], device='cuda:0')
time_max: tensor(304861., device='cuda:0')
manual set time_max: tensor(4000)
Dataset n_samples after time split: 1826 1121 341 364
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=12, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-1): 2 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=2, out_features=34, bias=True)
          (emb_linear): Linear(in_features=272, out_features=272, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=272, out_features=272, bias=True)
            (W_K): Linear(in_features=272, out_features=272, bias=True)
            (W_V): Linear(in_features=272, out_features=272, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=272, out_features=272, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-16): 17 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=17, out_features=34, bias=True)
          (emb_linear): Linear(in_features=32, out_features=32, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=32, out_features=32, bias=True)
            (W_K): Linear(in_features=32, out_features=32, bias=True)
            (W_V): Linear(in_features=32, out_features=32, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(34, 34, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)
parameters: 1277954
n_train_batches: 36
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 2 is out of range
True
Couldn't import umap
PID, device: 202845 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=41, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-1): 2 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=2, out_features=128, bias=True)
          (emb_linear): Linear(in_features=1024, out_features=1024, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=1024, out_features=1024, bias=True)
            (W_K): Linear(in_features=1024, out_features=1024, bias=True)
            (W_V): Linear(in_features=1024, out_features=1024, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=1024, out_features=1024, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-3): 4 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=4, out_features=128, bias=True)
          (emb_linear): Linear(in_features=512, out_features=512, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=512, out_features=512, bias=True)
            (W_K): Linear(in_features=512, out_features=512, bias=True)
            (W_V): Linear(in_features=512, out_features=512, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (2): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-7): 8 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=8, out_features=128, bias=True)
          (emb_linear): Linear(in_features=256, out_features=256, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=256, out_features=256, bias=True)
            (W_K): Linear(in_features=256, out_features=256, bias=True)
            (W_V): Linear(in_features=256, out_features=256, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (3): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-15): 16 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=16, out_features=128, bias=True)
          (emb_linear): Linear(in_features=128, out_features=128, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (4): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-31): 32 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=32, out_features=128, bias=True)
          (emb_linear): Linear(in_features=64, out_features=64, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=64, out_features=64, bias=True)
            (W_K): Linear(in_features=64, out_features=64, bias=True)
            (W_V): Linear(in_features=64, out_features=64, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (5): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-63): 64 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=64, out_features=128, bias=True)
          (emb_linear): Linear(in_features=32, out_features=32, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=32, out_features=32, bias=True)
            (W_K): Linear(in_features=32, out_features=32, bias=True)
            (W_V): Linear(in_features=32, out_features=32, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 13:57:39
run_baselines.py --history 24 --model Pathformer --dataset physionet --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=128, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=202845, pred_window=24, ndim=41, patch_layer=1, patch_size_list=[64, 32, 16, 8, 4, 2], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=41, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])

parameters: 21830249
n_train_batches: 225
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 103, in forward
    gates, load = self.noisy_top_k_gating(new_x, self.training)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 94, in noisy_top_k_gating
    load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 61, in _prob_in_top_k
    prob_if_in = normal.cdf((clean_values - threshold_if_in) / noise_stddev)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/normal.py", line 93, in cdf
    self._validate_sample(value)
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py", line 312, in _validate_sample
    raise ValueError(
ValueError: Expected value argument (Tensor of shape (32, 4)) to be within the support (Real()) of the distribution Normal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')), but found invalid values:
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)
True
Couldn't import umap
PID, device: 203910 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=41, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-4): 5 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=5, out_features=175, bias=True)
          (emb_linear): Linear(in_features=560, out_features=560, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=560, out_features=560, bias=True)
            (W_K): Linear(in_features=560, out_features=560, bias=True)
            (W_V): Linear(in_features=560, out_features=560, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=560, out_features=560, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-6): 7 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=7, out_features=175, bias=True)
          (emb_linear): Linear(in_features=400, out_features=400, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=400, out_features=400, bias=True)
            (W_K): Linear(in_features=400, out_features=400, bias=True)
            (W_V): Linear(in_features=400, out_features=400, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=400, out_features=400, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (2): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-24): 25 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=25, out_features=175, bias=True)
          (emb_linear): Linear(in_features=112, out_features=112, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=112, out_features=112, bias=True)
            (W_K): Linear(in_features=112, out_features=112, bias=True)
            (W_V): Linear(in_features=112, out_features=112, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=112, out_features=112, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (3): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-34): 35 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=35, out_features=175, bias=True)
          (emb_linear): Linear(in_features=80, out_features=80, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=80, out_features=80, bias=True)
            (W_K): Linear(in_features=80, out_features=80, bias=True)
            (W_V): Linear(in_features=80, out_features=80, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=80, out_features=80, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(175, 175, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 13:59:12
run_baselines.py --history 36 --model Pathformer --dataset physionet --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=175, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=203910, pred_window=12, ndim=41, patch_layer=2, patch_size_list=[35, 25, 7, 5], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=41, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
- Epoch 000, ExpID 40749
Train - Loss (one batch): 0.00831
Val - Loss, MSE, RMSE, MAE, MAPE: 0.00915, 0.00915, 0.09564, 0.05625, 217.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.00976, 0.00976, 0.09877, 0.05553, 226.65%
Time spent: 318.76s
- Epoch 001, ExpID 40749
Train - Loss (one batch): 0.00695
Val - Loss, MSE, RMSE, MAE, MAPE: 0.00899, 0.00899, 0.09480, 0.05671, 205.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.00949, 0.00949, 0.09742, 0.05616, 211.75%
Time spent: 330.71s
- Epoch 002, ExpID 40749
Train - Loss (one batch): 0.00894
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01001, 0.01001, 0.10006, 0.05956, 222.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.00949, 0.00949, 0.09742, 0.05616, 211.75%
Time spent: 295.73s

parameters: 8004773
n_train_batches: 225
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 103, in forward
    gates, load = self.noisy_top_k_gating(new_x, self.training)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 94, in noisy_top_k_gating
    load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 61, in _prob_in_top_k
    prob_if_in = normal.cdf((clean_values - threshold_if_in) / noise_stddev)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/normal.py", line 93, in cdf
    self._validate_sample(value)
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py", line 312, in _validate_sample
    raise ValueError(
ValueError: Expected value argument (Tensor of shape (32, 4)) to be within the support (Real()) of the distribution Normal(loc: tensor([0.], device='cuda:0'), scale: tensor([1.], device='cuda:0')), but found invalid values:
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 14:17:50
run_baselines.py --history 12 --model Pathformer --dataset physionet --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='physionet', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=83, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=215058, pred_window=36, ndim=41, patch_layer=1, patch_size_list=[83, 1], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=41, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[10, 10, 10])
True
Couldn't import umap
PID, device: 215058 cuda:0
Total records: 12000
Dataset n_samples: 12000 7200 2400 2400
Test record ids (first 20): ['141035', '147605', '135375', '161896', '139505', '162915', '146059', '161471', '137723', '146625', '137197', '133811', '142577', '138349', '145848', '156467', '139420', '145445', '141972', '132617']
Test record ids (last 20): ['151541', '135141', '148664', '151895', '137025', '143410', '132799', '160417', '143342', '157915', '143290', '141428', '147807', '145320', '146007', '136354', '153098', '148397', '138869', '145810']
data_max: tensor([9.0000e+01, 1.0000e+00, 4.6230e+02, 4.0000e+00, 3.0000e+02, 5.3000e+00,
        4.6950e+03, 1.6920e+04, 2.3950e+04, 6.2100e+01, 2.0900e+02, 3.6000e+02,
        2.2000e+01, 2.8300e+02, 1.0000e+00, 1.5000e+01, 1.5910e+03, 5.2000e+01,
        6.1800e+01, 3.0000e+02, 1.9600e+01, 3.1000e+01, 3.7500e+01, 3.0000e+02,
        1.0000e+00, 1.8000e+02, 2.1100e+02, 2.2800e+02, 3.0000e+02, 1.0000e+02,
        5.0000e+02, 7.3300e+02, 2.2920e+03, 1.0000e+02, 1.0000e+02, 2.9500e+02,
        4.2200e+01, 4.9600e+01, 2.9910e+01, 1.1000e+04, 6.2519e+03],
       device='cuda:0')
data_min: tensor([ 1.5000e+01, -1.0000e+00, -1.0000e+00,  1.0000e+00, -1.0000e+00,
         1.0000e+00,  8.0000e+00,  1.0000e+00,  4.0000e+00,  0.0000e+00,
         0.0000e+00,  2.8000e+01,  1.0000e-01, -1.0000e+00,  2.1000e-01,
         3.0000e+00,  8.0000e+00,  5.0000e+00,  5.5000e+00,  0.0000e+00,
         1.5000e+00,  3.0000e-01,  6.0000e-01,  0.0000e+00,  1.0000e+00,
         9.9000e+01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.7800e+01,  1.0000e-01,  1.0000e-02,  0.0000e+00,
         1.0000e-01], device='cuda:0')
time_max: tensor(48., device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=41, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0): Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=1, out_features=83, bias=True)
          (emb_linear): Linear(in_features=1328, out_features=1328, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=1328, out_features=1328, bias=True)
            (W_K): Linear(in_features=1328, out_features=1328, bias=True)
            (W_V): Linear(in_features=1328, out_features=1328, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=1328, out_features=1328, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-82): 83 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=83, out_features=83, bias=True)
          (emb_linear): Linear(in_features=16, out_features=16, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=16, out_features=16, bias=True)
            (W_K): Linear(in_features=16, out_features=16, bias=True)
            (W_V): Linear(in_features=16, out_features=16, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(83, 83, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)
parameters: 26881799
n_train_batches: 225
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 2 is out of range
True
Couldn't import umap
PID, device: 216503 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=96, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-1): 2 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=2, out_features=280, bias=True)
          (emb_linear): Linear(in_features=2240, out_features=2240, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=2240, out_features=2240, bias=True)
            (W_K): Linear(in_features=2240, out_features=2240, bias=True)
            (W_V): Linear(in_features=2240, out_features=2240, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=2240, out_features=2240, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-3): 4 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=4, out_features=280, bias=True)
          (emb_linear): Linear(in_features=1120, out_features=1120, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=1120, out_features=1120, bias=True)
            (W_K): Linear(in_features=1120, out_features=1120, bias=True)
            (W_V): Linear(in_features=1120, out_features=1120, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=1120, out_features=1120, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (2): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-4): 5 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=5, out_features=280, bias=True)
          (emb_linear): Linear(in_features=896, out_features=896, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=896, out_features=896, bias=True)
            (W_K): Linear(in_features=896, out_features=896, bias=True)
            (W_V): Linear(in_features=896, out_features=896, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=896, out_features=896, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (3): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-6): 7 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=7, out_features=280, bias=True)
          (emb_linear): Linear(in_features=640, out_features=640, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=640, out_features=640, bias=True)
            (W_K): Linear(in_features=640, out_features=640, bias=True)
            (W_V): Linear(in_features=640, out_features=640, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (4): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-7): 8 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=8, out_features=280, bias=True)
          (emb_linear): Linear(in_features=560, out_features=560, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=560, out_features=560, bias=True)
            (W_K): Linear(in_features=560, out_features=560, bias=True)
            (W_V): Linear(in_features=560, out_features=560, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=560, out_features=560, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (5): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-9): 10 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=10, out_features=280, bias=True)
          (emb_linear): Linear(in_features=448, out_features=448, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=448, out_features=448, bias=True)
            (W_K): Linear(in_features=448, out_features=448, bias=True)
            (W_V): Linear(in_features=448, out_features=448, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=448, out_features=448, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (6): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-27): 28 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=28, out_features=280, bias=True)
          (emb_linear): Linear(in_features=160, out_features=160, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=160, out_features=160, bias=True)
            (W_K): Linear(in_features=160, out_features=160, bias=True)
            (W_V): Linear(in_features=160, out_features=160, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=160, out_features=160, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (7): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-34): 35 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=35, out_features=280, bias=True)
          (emb_linear): Linear(in_features=128, out_features=128, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (8): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-39): 40 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=40, out_features=280, bias=True)
          (emb_linear): Linear(in_features=112, out_features=112, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=112, out_features=112, bias=True)
            (W_K): Linear(in_features=112, out_features=112, bias=True)
            (W_V): Linear(in_features=112, out_features=112, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=112, out_features=112, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (9): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-55): 56 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=56, out_features=280, bias=True)
          (emb_linear): Linear(in_features=80, out_features=80, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=80, out_features=80, bias=True)
            (W_K): Linear(in_features=80, out_features=80, bias=True)
            (W_V): Linear(in_features=80, out_features=80, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=80, out_features=80, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (10): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-69): 70 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=70, out_features=280, bias=True)
          (emb_linear): Linear(in_features=64, out_features=64, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=64, out_features=64, bias=True)
            (W_K): Linear(in_features=64, out_features=64, bias=True)
            (W_V): Linear(in_features=64, out_features=64, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (11): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-139): 140 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=140, out_features=280, bias=True)
          (emb_linear): Linear(in_features=32, out_features=32, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=32, out_features=32, bias=True)
            (W_K): Linear(in_features=32, out_features=32, bias=True)
            (W_V): Linear(in_features=32, out_features=32, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(280, 280, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 14:24:01
run_baselines.py --history 24 --model Pathformer --dataset mimic --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=280, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=216503, pred_window=24, ndim=96, patch_layer=1, patch_size_list=[140, 70, 56, 40, 35, 28, 10, 8, 7, 5, 4, 2], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=96, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[10000, 10000, 100000])

parameters: 191929190
n_train_batches: 440
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 12 is out of range
True
Couldn't import umap
PID, device: 221878 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=96, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-1): 2 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=2, out_features=464, bias=True)
          (emb_linear): Linear(in_features=3712, out_features=3712, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=3712, out_features=3712, bias=True)
            (W_K): Linear(in_features=3712, out_features=3712, bias=True)
            (W_V): Linear(in_features=3712, out_features=3712, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=3712, out_features=3712, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-3): 4 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=4, out_features=464, bias=True)
          (emb_linear): Linear(in_features=1856, out_features=1856, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=1856, out_features=1856, bias=True)
            (W_K): Linear(in_features=1856, out_features=1856, bias=True)
            (W_V): Linear(in_features=1856, out_features=1856, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=1856, out_features=1856, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (2): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-7): 8 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=8, out_features=464, bias=True)
          (emb_linear): Linear(in_features=928, out_features=928, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=928, out_features=928, bias=True)
            (W_K): Linear(in_features=928, out_features=928, bias=True)
            (W_V): Linear(in_features=928, out_features=928, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=928, out_features=928, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (3): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-57): 58 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=58, out_features=464, bias=True)
          (emb_linear): Linear(in_features=128, out_features=128, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (4): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-15): 16 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=16, out_features=464, bias=True)
          (emb_linear): Linear(in_features=464, out_features=464, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=464, out_features=464, bias=True)
            (W_K): Linear(in_features=464, out_features=464, bias=True)
            (W_V): Linear(in_features=464, out_features=464, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=464, out_features=464, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (5): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-28): 29 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=29, out_features=464, bias=True)
          (emb_linear): Linear(in_features=256, out_features=256, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=256, out_features=256, bias=True)
            (W_K): Linear(in_features=256, out_features=256, bias=True)
            (W_V): Linear(in_features=256, out_features=256, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (6): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-115): 116 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=116, out_features=464, bias=True)
          (emb_linear): Linear(in_features=64, out_features=64, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=64, out_features=64, bias=True)
            (W_K): Linear(in_features=64, out_features=64, bias=True)
            (W_V): Linear(in_features=64, out_features=64, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (7): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-231): 232 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=232, out_features=464, bias=True)
          (emb_linear): Linear(in_features=32, out_features=32, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=32, out_features=32, bias=True)
            (W_K): Linear(in_features=32, out_features=32, bias=True)
            (W_V): Linear(in_features=32, out_features=32, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(464, 464, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 14:30:09
run_baselines.py --history 36 --model Pathformer --dataset mimic --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=36, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=464, top_k=5, num_kernels=64, npatch=2, device=device(type='cuda', index=0), PID=221878, pred_window=12, ndim=96, patch_layer=2, patch_size_list=[232, 116, 58, 8, 29, 16, 4, 2], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=96, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])

parameters: 280414598
n_train_batches: 440
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/Layer.py", line 104, in forward
    x = self.emb_linear(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 14:36:10
run_baselines.py --history 12 --model Pathformer --dataset mimic --gpu 3
Namespace(state='def', n=100000000, epoch=100, patience=10, history=12, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='mimic', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='3', seq_len=133, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=225526, pred_window=36, ndim=96, patch_layer=1, patch_size_list=[19, 7], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=96, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
True
Couldn't import umap
PID, device: 225526 cuda:0
Total records: 23457
Dataset n_samples: 23457 14073 4692 4692
Test record ids (first 20): [20539, 3274, 10159, 3501, 7505, 21021, 12964, 12250, 9303, 13430, 247, 8922, 3778, 6354, 9236, 17756, 21529, 17436, 10901, 19808]
Test record ids (last 20): [5183, 5610, 19849, 20653, 16941, 20849, 22721, 18864, 6285, 23347, 42, 14912, 3570, 22992, 17801, 4049, 5466, 795, 396, 18120]
data_max: tensor([5.6000e+01, 5.0000e+01, 2.3300e+01, 1.5500e+02, 2.7800e+01, 2.3400e+03,
        3.7500e+01, 3.2800e+01, 1.5600e+01, 1.8200e+02, 4.0240e+03, 3.6400e+04,
        8.2800e+01, 2.8000e+02, 4.0000e+01, 5.1000e+01, 7.0600e+01, 2.2300e+01,
        1.0000e+02, 4.8000e+01, 3.9800e+01, 1.3900e+02, 1.0000e+02, 9.9000e+01,
        2.2920e+03, 2.9700e+01, 8.4400e+00, 6.0020e+02, 1.6260e+02, 3.1000e+01,
        1.5000e+02, 1.5030e+04, 9.0000e+00, 1.0800e+00, 1.0000e+01, 5.0000e+00,
        2.0000e+01, 2.0000e+00, 1.6667e+01, 1.1000e+04, 4.2000e+01, 2.5000e+03,
        9.2967e+01, 9.0000e+02, 1.8750e+02, 5.3333e+01, 5.0000e+01, 1.5571e+01,
        2.9000e+01, 6.5000e+01, 1.7400e+02, 7.7500e+02, 6.3000e+00, 3.1000e+03,
        2.7000e+02, 6.5000e+02, 9.0000e+01, 1.0000e+02, 3.7500e+02, 1.6390e+01,
        2.5000e+00, 2.5000e+01, 2.9000e+01, 1.0000e+02, 4.0000e+01, 5.2000e+02,
        6.0000e+01, 1.2500e+01, 1.5000e+02, 1.0000e+03, 4.8000e+03, 6.2500e+01,
        4.5000e+02, 3.0000e+02, 2.6000e+01, 2.5000e+02, 3.7000e+03, 1.0000e+02,
        4.0000e+01, 8.0000e+00, 1.0000e+03, 1.5000e+03, 3.0000e+01, 5.0000e+03,
        2.8000e+02, 2.7041e+03, 1.0500e+03, 5.0000e+00, 2.0640e+00, 1.4286e+02,
        5.0000e+02, 7.0000e+02, 1.0200e+03, 6.0000e+02, 1.5000e+03, 2.5000e+02],
       device='cuda:0')
data_min: tensor([2.0000e+00, 5.0000e+00, 1.8000e+00, 3.9000e+01, 0.0000e+00, 0.0000e+00,
        2.0000e-01, 0.0000e+00, 8.0000e-01, 8.2000e+01, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        5.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-01, 1.2500e+01, 1.0000e-01,
        8.2000e+00, 1.0000e+00, 1.0000e+00, 1.5000e-02, 3.0000e+00, 1.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3333e-01, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1000e+01, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 8.1000e+01, 3.0000e+00, 1.0000e+02, 0.0000e+00, 0.0000e+00,
        3.3333e-02, 0.0000e+00, 8.3333e-01, 1.0000e+00, 5.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+01, 0.0000e+00, 3.3333e-01,
        0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5000e+00,
        8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e-02, 0.0000e+00, 0.0000e+00,
        4.1667e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
       device='cuda:0')
time_max: tensor(47.9833, device='cuda:0')
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=96, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-6): 7 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=7, out_features=133, bias=True)
          (emb_linear): Linear(in_features=304, out_features=304, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=304, out_features=304, bias=True)
            (W_K): Linear(in_features=304, out_features=304, bias=True)
            (W_V): Linear(in_features=304, out_features=304, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=304, out_features=304, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-18): 19 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=19, out_features=133, bias=True)
          (emb_linear): Linear(in_features=112, out_features=112, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=112, out_features=112, bias=True)
            (W_K): Linear(in_features=112, out_features=112, bias=True)
            (W_V): Linear(in_features=112, out_features=112, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=112, out_features=112, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(133, 133, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)
parameters: 1922612
n_train_batches: 440
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 2 is out of range
/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py
2024-07-22 14:36:27
run_baselines.py --history 24 --model Pathformer --dataset ushcn
Namespace(state='def', n=100000000, epoch=100, patience=10, history=24, logmode='a', lr=0.001, w_decay=0.0, batch_size=32, viz=False, save='experiments/', load=None, seed=1, dataset='ushcn', quantization=0.0, model='Pathformer', outlayer='Linear', patch_ts=False, hop=1, nhead=1, tf_layer=1, nlayer=1, patch_size=24, stride=24, hid_dim=64, te_dim=10, node_dim=10, alpha=0.9, res=1, gpu='0', seq_len=205, top_k=5, num_kernels=64, npatch=1, device=device(type='cuda', index=0), PID=230765, n_months=48, pred_window=1, ndim=5, patch_layer=1, patch_size_list=[41, 5], task_name='long_term_forecast', layer_nums=3, pred_len=98, num_nodes=5, pre_len=98, k=2, d_model=16, d_ff=64, residual_connection=1, revin=1, num_experts_list=[4, 4, 4])
True
Couldn't import umap
PID, device: 230765 cuda:0
Total records: 1114
Dataset n_samples: 1114 668 223 223
Test record ids (first 20): [886, 101, 1120, 730, 291, 875, 958, 260, 128, 1043, 620, 88, 1005, 872, 617, 538, 508, 44, 273, 817]
Test record ids (last 20): [982, 275, 991, 997, 66, 801, 67, 942, 12, 361, 555, 710, 333, 1017, 851, 184, 882, 509, 726, 587]
data_max: tensor([37.2551, 36.6691, 38.2520,  3.0401,  2.9104], device='cuda:0')
data_min: tensor([-0.3057, -0.1280, -0.1738, -3.8821, -4.1854], device='cuda:0')
time_max: tensor(48., device='cuda:0')
Dataset n_samples after time split: 26736 16032 5352 5352
model Model(
  (revin_layer): RevIN()
  (start_fc): Linear(in_features=1, out_features=16, bias=True)
  (AMS_lists): ModuleList(
    (0-2): 3 x AMS(
      (start_linear): Linear(in_features=5, out_features=1, bias=True)
      (seasonality_model): FourierLayer()
      (trend_model): series_decomp_multi(
        (layer): Linear(in_features=1, out_features=3, bias=True)
      )
      (experts): ModuleList(
        (0): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-4): 5 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=5, out_features=205, bias=True)
          (emb_linear): Linear(in_features=656, out_features=656, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=656, out_features=656, bias=True)
            (W_K): Linear(in_features=656, out_features=656, bias=True)
            (W_V): Linear(in_features=656, out_features=656, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=656, out_features=656, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
        (1): Transformer_Layer(
          (embeddings_generator): ModuleList(
            (0-40): 41 x Sequential(
              (0): Linear(in_features=16, out_features=16, bias=True)
            )
          )
          (intra_patch_attention): Intra_Patch_Attention(
            (custom_linear): CustomLinear()
          )
          (weights_generator_distinct): WeightGenerator(
            (generator): Sequential(
              (0): Linear(in_features=16, out_features=64, bias=True)
              (1): Tanh()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Tanh()
              (4): Linear(in_features=64, out_features=100, bias=True)
            )
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x10 (cuda:0)]
            )
            (Q): ParameterList(
                (0): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 10x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 100x16 (cuda:0)]
            )
          )
          (weights_generator_shared): WeightGenerator(
            (P): ParameterList(
                (0): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 16x16 (cuda:0)]
            )
            (B): ParameterList(
                (0): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 1x16 (cuda:0)]
            )
          )
          (intra_Linear): Linear(in_features=41, out_features=205, bias=True)
          (emb_linear): Linear(in_features=80, out_features=80, bias=True)
          (inter_patch_attention): Inter_Patch_Attention(
            (W_Q): Linear(in_features=80, out_features=80, bias=True)
            (W_K): Linear(in_features=80, out_features=80, bias=True)
            (W_V): Linear(in_features=80, out_features=80, bias=True)
            (sdp_attn): ScaledDotProductAttention(
              (attn_dropout): Dropout(p=0, inplace=False)
            )
            (to_out): Sequential(
              (0): Linear(in_features=80, out_features=80, bias=True)
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (norm_attn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (norm_ffn): Sequential(
            (0): Transpose()
            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Transpose()
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (ff): Sequential(
            (0): Linear(in_features=16, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.2, inplace=False)
            (3): Linear(in_features=64, out_features=16, bias=True)
          )
        )
      )
      (MLPs): ModuleList()
      (end_MLP): MLP(
        (fc): Conv2d(205, 205, kernel_size=(1, 1), stride=(1, 1))
      )
      (softplus): Softplus(beta=1.0, threshold=20.0)
      (softmax): Softmax(dim=1)
    )
  )
  (te_scale): Linear(in_features=1, out_features=1, bias=True)
  (te_periodic): Linear(in_features=1, out_features=15, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=16, out_features=1, bias=True)
  )
)
parameters: 6902807
n_train_batches: 501
Traceback (most recent call last):
  File "/home/jiaxihu/bowenzhang/HPG/tPatchGNN/run_baselines.py", line 428, in <module>
    train_res = compute_all_losses(model, batch_dict)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/lib/evaluation.py", line 331, in compute_all_losses
    pred_y = model.forecasting(batch_dict["tp_to_predict"],
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/models/Pathformer.py", line 87, in forecasting
    out, aux_loss = layer(out)
                    ^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/bowenzhang/HPG/baselines/layers/AMS.py", line 110, in forward
    expert_outputs = [self.experts[i](expert_inputs[i])[0] for i in range(self.num_experts)]
                      ~~~~~~~~~~~~^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 295, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jiaxihu/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 285, in _get_abs_string_index
    raise IndexError(f'index {idx} is out of range')
IndexError: index 2 is out of range
